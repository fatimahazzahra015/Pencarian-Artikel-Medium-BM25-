url,title,authors,timestamp,text,image_url,updatedTags
https://medium.com/s/story/big-data-dengan-hadoop-install-virtual-box-cloudera-distribution-part-3-1050927f594f,Big Data dengan Hadoop (Install Virtual Box & Cloudera Distribution) — Part #3,Farhan,2018-05-23T13:59:43.717Z,"Halo Kamerad, dicatatan sebelumnyamengenai Hadoop saya menjelaskan tentang HadoopEcosystemtentang elemen apa saja yang ada pada HadoopEcosystem.Pada catatan kali ini saya akan menjelaskan pengalaman saya meng-installHadoop dariClouderadistribution pada PC saya dengan bantuanVirtualBox.
Tanpa bertele-tele lagi berikut saya jelaskan bagaimana langkah-langkahnya:
Sekian tutorial install dan setup Hadoop dengan cloudera distribution, di catatan selanjutnya akan saya praktekan bagaimana cara menggunakannya untuk memahami konsep HDFS.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:1200/1*XBMlw-8m20Ydefkp7V88fg.png,"Big Data, Hadoop, Virtual Machine, Cloudera, Data Science"
https://medium.com/s/story/visualisasi-data-tingkat-inflasi-jakarta-2006-2012-dengan-python-10f4e5b14e1b,Visualisasi Data Tingkat Inflasi Jakarta 2006–2012 dengan Python,Muhamad Bayu Wilanda,2018-07-16T15:16:35.081Z,"Pada kesempatan kali ini saya akan mempraktikan visualisasi data dengan menggunakanAnaconda. Anaconda merupakan sebuah tools yang dapat mempercepat dan mempermudah pengolahan data dengan bahasa Python dan R, Anaconda tersedia untuk Windows, Linux dan Mac OS X.
Sebelum memulai, silakan unduh dan install terlebih dahulu Anacondadisini. Jika sudah, mari kita mulai.
Buka Anaconda Navigator dengan mengklik-ganda ikonnya.
Bagi pengguna Linux silahkan buka terminal dan ketikan
Setelah Anaconda Navigator terbuka, klikLaunchpada kotakJupyter notebook,maka akan terbuka jendela web browser yang akan anda gunakan, tampilannya akan seperti gambar di bawah ini.
Setelah itu buat beberapa folder diDesktophingga susunannya seperti ini /home/…/Desktop/data visualization/inflasi jakarta/
Setelah itu unduh datasetTingkat Inflasi Jakarta dan Nasionaldan simpan di/home/…/Desktop/data visualization/inflasi jakarta/inflasi-jakarta-nasional.csv. Dalam file tersebut terdapat:
Kembali ke Jupyter, buka Desktop>data visualization>inflasi jakarta, lalu klik new>klik python 2/3.
Setelah itu maka akan terbuka tab baru, silahkan ubah judul dengan mengklik-ganda “Untitled” menjadi “Visualisasi DataTingkat Inflasi Jakarta 2006–2012” seperti gambar di bawah ini.
Setelah itu pada kolom pertama anda ketikan kode seperti dibawah ini.
Jika sudah diketikan, maka tekan tombolrunyang terdapat pada baris atas menu navigasi. Kode diatas digunakan untuk mengimport modul yang akan diperlukan pada tahap-tahap selanjutnya. Selanjutnya adalah mengimport data berekstensi.csvyang sudah anda unduh tadi dengan mengetikan kode di bawah.
Tekan tombolrun,jika sudah maka anda bisa melihat isi file tersebut dengan mengetikkan.
Jika sudah, maka tekanrun,maka tampilannya akan seperti berikut.
Karena kita hanya akan menggunakan datatahun dan inflasi_jakarta,makainflasi_nasionalakan dihapus dengan mengetikan kode di bawah ini dan menampilkannya.
Setelah di-run, maka hasilnya akan seperti ini.
Jika sudah seperti itu, maka langkah selanjutnya adalah kita akan memvisualisasikan datanya dalam bentuk grafik batang, dan menyimpan hasilnya dengan namatingkat-inflasi-jakarta.png.
Maka hasilnya akan seperti ini:
Setelah melihat hasil visualisasinya, data mentah dalam bentuk .csv lebih mudah untuk dibaca. Anda bisa menyimpulkan hasilnya dengan bahasa anda sendiri. Hehehe.
Mudah bukan? Tentu, karena ini hanya tahap awal-awalnya saja. Sampai jumpa di praktek visualisasi data yang lebih rumit berikutnya.
GNU/Linux User | #Gooner",https://miro.medium.com/v2/resize:fit:1200/1*Y69lexAvXXZEGY3oqjXh5g.png,"Python, Data Science, Data Visualization"
https://medium.com/s/story/asymmetry-variability-dan-relationship-data-116992ca79e3,"Asymmetry , Variability dan Relationship Data",Rangga Rizky A,2018-05-27T22:35:14.488Z,"tahap yang paling awal dalam analisa data adalah mengukur Central tendency, Asymmetry dan Variability. Explolatory data analisis merupakan cara untuk mendeskripsikan suatu data. dimana kita melakukan eksplorasi data untuk mengetahu karaterisktik dan sifat-sifat data tersebut. dengan lebih mengenal data yang akan di analisa maka kita dapat menentukan langkah-langkah selanjutnya yang dapat kita lakukan.
Central Tendency dan Asymmetry
Mungkin kita lebih mengenal central tendency dengan Mean , Median dan Modus, Saya rasa semua yang membaca tulisan ini sudah paham dengan 3 pengukuran diatas. jadi saya akan bahas apa yang bisa didapatkan dari pengukuran diatas. salah satunya adalah Skewness. dengan mengetahui skewnessnya kita akan tahu dimana data terkonsentrasi dan dimana letak outlier pada data tersebut. untuk lebih jelasnya dapat perhatikan gambar dibawah ini.
jika mean lebih kecil dari median maka data tersebut memiliki negative skewed, yang artinya data terkonsentrasi di atas-rata-rata dan outlier terletak pada data dibawah rata-rata. jika mean,median dan mode nilainya sama maka memiliki normal skew sehingga akan membentuk simetrikal distibution. untuk positif skew kebalikan dari negatif skew.
cara yang paling umum untuk mengetahui variability suatu data dapat menggunakan variance , standard deviasi dan covariance. pada umumnya rumus dibagi menjadi sample dan populasi. kita akan cenderung membahas pada sample data. variance sendiri digunakan untuk megukur penyebaran data point dari nilai rata-rata. untuk menghitungnya kamu dapat mebuka artikel inihttps://id.wikihow.com/Menghitung-Variasi.
jika nilai variansnya rendah berarti data menyebar di sekitar rata-rata jika snilainya tinggi maka sebaliknya. karena unit dari variance adalah kuadrat maka agar benar-benar bermanfaat maka dicari standard deviasi. rumus standar deviasi sendiri adalah hanya melakukan akar pada nilai varians, sehingga memiliki skala yang sama dengan dataset. lalu bagaimana dengan coefficient of variance, rumus dari coefficient of variance adalah standard deviasi dibagi dengan mean. mengapa coefficient of variance? coefficient of variance digunakan untuk membandingkan 2 / lebih dataset sedangkan standard deviasi hanya untuk 1 dataset. untuk lebih lanjut dapat perhatikan dataset dibawah
dari 2 dataset tersebut memiliki standard deviasi yang berbeda tetapi mempunyai coefficient of variance yang sama. dapat diperhatikan bahwa pola penyebaran kedua data sama walaupun memiliki nilai yang berbeda. coefficient of variance sendiri tidak memiliki unit measurement sehingga cocok untuk perbandingan. kita juga dapat menghitung co-variance dari 2 data dengan rumus
bayangkan kita ingin mengetahui dari data penjualan rumah. apakah luas rumah mempengaruhi dari harga sebuah rumah atau sebaliknya. nah dengan covariance kita dapat menjawab pertanyaan tersebut. rumus covariance sendiri adalah
nilai dari covariance akan membantu kita mendapatkan sense of direction dari data. dimana jika nilainya > 0 maka kedua variabel bergerak bersama, jika nilainya < 0 maka bergerak berlawanan jika nilainya 0 maka kedua variabel bersifat independen.
agar dapat lebih mudah dalam mengambil instuisi maka dapat dicari nilai koefision korelasi. yang didapatkan dari nilai covariance(x,y) / std(x) * std(y) . secara matetmatik nilai dari koefisien korelasi adalah -1 hingga 1 . semakin mendekati 1 berarti variabel x searah dengan variabel y ilustrasinya adalah jika nilai x naik maka nila y naik, jika nilai x turun maka nila y turun. jika nilai koefisien korelasi mendekati 0 maka kedua variabel tersebut semakin tidak saling berhubungan . jika nilainya antara -1 dan 0 berarti kedua variabel berlawanan arah. jika nilai x naik maka nilai y turun dan sebaliknya
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:1200/0*XIYg9f8QD1ps-WDO.,"Data Science, Statistics"
https://medium.com/s/story/automatic-license-plate-recognition-1189b2e594e5,Automatic License Plate Recognition,yohanes,2018-08-03T03:01:48.548Z,"Artikel ini merupakan publikasi pertama dari tim Mahapatih. Pada tulisan pertama ini dikupas sedikit mengenai bagaimana teknologi yang kami kembangkan melakukan implementasi ALPR untuk mengenali plat nomor kendaraan di Indonesia.
Saat ini sistem pengenalan plat nomor secara otomatis (ALPR,automatic license plate recognition) sudah mulai digunakan di berbagai negara. Sistem ini sangat berguna untuk banyak hal mulai dari meningkatkan ketertiban lalu-lintas, sampai membantu pencarian pelaku penculikan yang melarikan diri menggunakan mobil dengan plat nomor yang dikenal. Tulisan ini membahas mengenai teknik LPR secara umum dan solusi LPR dari produk Machine Vision Mahapatih (codename: Mata Dewa) yang dioptimasi untuk penggunaan di Indonesia.
Mungkin pertanyaan pertama yang muncul adalah: mengapa tidak memakai sistem open source yang sudah ada? Saat ini sistem open source yang sudah terkenal adalahOpenALPRdan akurasinya cukup baik untuk plat nomor negara tertentu.
Alasan paling utama adalah lisensi, OpenALPR memiliki lisensiAffero GPL. LisensiAfferoini mewajibkan kita memberikan source code kepada siapapun yang memakai software, meskipun dari internet, ini berbeda dari GPL biasa yang hanya mewajibkan kita membagi source code ke orang yang menjalankan software tersebut. Artinya segala macam perubahan yang kita lakukan terhadap software OpenALPR harus dibuka ke publik.
Akurasi OpenALPR juga bukan yang terbaik, sistem pengenalan digitnya memakai Tesseract OCR. Tesseract OCR versi stabil (versi 3) memakai algoritma klasik yang kalah jauh dari metode dengan deep learning. Baru pada tahun lalu Tesseract OCR 4 mulai memakai deep learning (sampai saat ini versi 4 tersebut masih beta).
Saat ini development OpenALPR ini juga cukup lambat (ketika artikel ini ditulis,committerakhir adalah 2 bulan yang lalu), sementara ada banyak pertanyaan yang tidak terjawab di bagian issues (misalnya bagaimana mengadaptasi OpenALPR untuk plat nomor yang terdiri dari dua baris di Singapura). Perlambatan ini bisa dimaklumi karena fokus mereka saat ini adalah layanan berbasis cloud yang sifatnya komersial (tidak gratis).
Sebenarnya langkah pembacaan otomatis plat nomor cukup sederhana, tapi detail implementasinya yang rumit. Langkah pertama adalah mencari plat nomor di dalam gambar (biasanya gambar ini merupakan satu frame video), membersihkan gambar, lalu mengenali plat yang ada pada gambar tersebut.
Pada bagian berikutnya asumsinya kita bekerja pada satu gambar statik saja. Jika inputnya adalah video maka bisa dilakukan object tracking agar tidak mengulangi semua langkah.
Bentuk, warna, ukuran, dan font plat nomor berbeda di tiap negara dan sebuah aplikasi yang baik harus diadaptasi khusus untuk sistem di sebuah negara. Kadang sistem ini juga harus mengenali sistem dari negara lain, contohnya di berbagai negara Eropa yang berdekatan berbagai mobil dari negara tetangga dapat dengan mudah melintasi batas sebuah negara. Untuk penggunaan di Indonesia yang merupakan negara kepulauan (jumlah lalu lintas mobil antar pulau terbatas), sistem bisa dioptimasi untuk mengenali plat yang umum di wilayah atau pulau tertentu.
Saat ini framework deteksi objek yang cepat dan terbukti cukup akurat adalahViola-Jones. Untuk bisa mengenali objek kita perlu melakukan training terhadap ribuan plat nomor dengan berbagai kondisi pencahayaan dan berbagai kondisi cuaca.
Input algoritma ini adalah gambar dalam formatgrayscale(komponenLuminancesaja). Setelah kita melakukan training maka didapatkan datacascade. Dengan data ini kita bisa menggunakan berbagai library yang dapat melakukan deteksi dengan cepat, misalnya OpenCV memiliki fungsicvHaarDetectObjects.
Meskipun akurasi metode ini cukup tinggi, tapi deteksi objek ini juga tidak 100% akurat, kadang ada wilayah yang terdeteksi sebagai plat nomor tapi ternyata bukan. Kasus ini bisa difilter dengan kembali ke gambar asli yang berwarna dan memeriksa warna untuk memastikan warna dominan merupakan warna plat nomor. Filter berikutnya adalah dengan mencari apakah wilayah yang dideteksi tersebut memilikicontouryang polanya seperti huruf atau angka.
Optimasi juga bisa dilakukan tergantung dari lokasi deployment sistem. Untuk sistem di mana lokasi kamera dan kendaraan relatif tetap (misalnya untuk mendeteksi truk masuk gerbang) sistem bisa mendeteksi hanya area tertentu di gerbang dan akurasinya akan lebih tinggi.
Deteksi objek hanya dapat memberikan wilayah persegi yang merupakan plat nomor. Berikutnya area ini perlu diproses agar pembacaan teks plat nomor oleh algoritma berikutnya bisa lebih akurat. Gambar yang miring harus diluruskan, gambar yang perspektifnya salah harus dikoreksi dan secara umum gambar perlu dipertajam. Proses ini mungkin terlihat yang paling sederhana, tapi sebenarnya justru yang sangat sulit.
Tidak ada satu algoritma yang dapat memberikan output terbaik. Berbagai sistem ALPR memakai heuristic yang berbeda untuk memperkirakanhomography matrixuntuk mengoreksi perspektif. Cara yang dipakai pada sistem Mata Dewa adalah dengan menggunakan komputasihistogramuntuk mendapatkan alignment terbaik.
Berdasarkan histogram per baris dan kolom, gambar dapat diluruskan dan siap diproses oleh algoritma berikutnya.
Setelah mendapatkan gambar yang bersih, lokasi setiap huruf dan angka perlu dideteksi lagi menggunakan pendekatanViola-Jones. Dengan ini bisa didapatkan persegi yang menjadi kandidat satu huruf. Strategiheuristicdigunakan untuk membuang kemungkinan yang salah (seperti stiker yang ditempel yang posisinya tidak sejajar dengan digit lain).
Setiap kandidat huruf yang ditemui diberikan ke sebuahneural networkyang memiliki akurasi cukup tinggi dalam mengenali huruf dan angka. Pada sistem Mata Dewa training dilakukan dengan plat nomor yang ada di Indonesia.
Karena sistem dirancang khusus untuk Indonesia, maka pembobotan ekstra dapat diberikan agar memiliki akurasi lebih tinggi. Contohnya untuk sistem yang di-deploy di wilayah jakarta kemungkinan karakter pertamanya adalah huruf B dan bukan angka 8.
Sebuah plat nomor dalam pencahayaan sempurna kadang tetap sulit dibaca karena kotor, sudah kusam, penyok, atau ditempeli stiker. Penanganan kasus ini tergantung pada deploymentnya. Misalnya di jalan raya dapat dipasang beberapa kamera yang dapat melihat dari berbagai sudut, termasuk juga dari arah belakang mobil yang mungkin lebih jelas plat nomornya.
Sistem LPR hanya merupakan satu bagian dari sistem menyeluruh berbasis Machine Vision yang sedang dikembangkan oleh Mata Dewa.Berbagai teknologi yang dikembangkan ini diadaptasi khusus untuk penggunaan di Indonesia.
Contoh salah satu teknologi machine vision yang bisa digabungkan dengan LPR adalah:illegal parking detection.Sebuah kamera yang jangkauannya lebar (menggunakanwide-angle lens) dapat digunakan untuk mendeteksi parkir ilegal dan LPR dengan kamera lain dapat digunakan untuk mendeteksi plat nomor kendaraan yang parkir illegal tersebut. Tentunya definisi parkir illegal ini sesuai dengan aturan yang berlaku di Indonesia.
Sistem LPR ini juga bisa diintegrasikan denganElectronic Law Enforcement.Contohnya adalah untuk memonitor sistem ganjil genap sehingga dapat membantu pihak kepolisian mengawasi ruas-ruas jalan di Jakarta secara otomatis. Sistem elektronik juga bisa melakukan pengecekan otomatis kendaraan denganplat nomor palsu(registration identification) berdasarkan database kepolisian sehingga dapat mendeteksi aksi-aksi kreatif dari masyarakat seperti pada video dibawah ini :)
Sistem yang dikembangkan oleh Mata Dewa dikembangkan didalam negeri dan mudah diintegrasikan dengan berbagai sistem yang sudah ada. Karena menyangkut masalah keamanan, produk dalam negeri memiliki kelebihan karena source code dapat diaudit bersama dan tidak perlu perlu khawatir adanya backdoor.
PT Mahapatih Sibernusa Teknologiis a company focusing on digital transformation & cyber security technology. We are developing product such as Machine Vision and Content Disarm & Reconstruction (CDR).
We are also offering services such as cyber security consulting & managed security service provider.
A programmer and a hacker. A husband and a father. From Web to Kernel.",https://miro.medium.com/v2/resize:fit:1200/1*Nnh_Ll43NirRd-rIxgWVvA.jpeg,"Matadewa, Machine Learning, Computer Vision"
https://medium.com/s/story/artifcial-intelligence-ai-dalam-pandangan-professor-mit-12862927e79c,Artifcial Intelligence (AI) dalam Pandangan Professor MIT,Tristia Riskawati,2017-12-11T07:44:15.066Z,"Forming tech based society
“Mungkin kita kebanyakan nonton film Terminator, jadinya takut AI.”
Begitu ujar Luis Perez-Breva, professor yang mendalami Artificial Intelligence di Massachusetts Institute of Technology (MIT). Padahal menurutnya, pengembangan AI tidak seperti di film Terminator amat.
Dalam video kuliah singkatnyadi situs BigThink, Perez-Breva menyatakan, secara garis besar AI adalah ‘robot’ yang lebih pandai dari kita dalam beberapa hal. Orang-orang kuatir jika AI ini akan menggeser manusia di beberapa bidang.
Justru, Perez-Breva tidak berpikir seperti itu mengenai peran AI dalam masyarakat.
Perez-Breva berpendapat, ketika perusahaan mulai mengotomatisasikan sistemnya — maka biasanya ada lapangan pekerjaan yang terhapus. Menurutnya, sayang sekali jika perusahaan tidak memikirkan untuk mengkreasi lapangan pekerjaan baru.
“Mereka biasanya berpikir soal penghematan biaya. Padahal dengan diciptakannya lapangan pekerjaan yang baru, perusahaan tersebut bisa mengembangkan pasar mereka.”
Bagi Perez-Breva, ini dikarenakan karena kurangnya imajinasi dari perusahaan tersebut. Padahal, dengan sistem yang sudah terotomatisasi, seharusnya ada lebih banyak waktu untuk memikirkan inovasi yang lebih kreatif sertamarketyang lebih luas.
Lebih lanjut lagi, Perez-Breva menyatakan, AI adalah sarana agar membuat komputer menjadi rekan kerja kita.
Ia mencontohkan bagaimana Google memanfaatkanadvance machine learninguntuk menyediakan rekan kerja terbaik bagi kita. Ya, betapa kita dimudahkan oleh mesin pencari, email, google maps, drive, dan lain sebagainya!
Ini seperti Google yang memudahkan kita untuk melakukan hal-hal yang dulu sulit dilakukan.
Seperti, kini, kamu tinggal mengetikkan apa yang ingin kamu tahu soal topik tugas akhirmu di mesin pencari — ketimbang harus pergi ke perpustakaan dulu. Atau kamu dapat dengan mudah menerjemahkan sebuah teks ke bahasa lainnya lewat Google Translate.
Bagaimana, masih ngeri dengan AI? Atau malah semakin antusias ‘bersahabat’ dengannya?
Forming tech based society
Intuitive writer. Nerd wifey-momma.",https://miro.medium.com/v2/resize:fit:620/0*Wh5nPnYtN2LLb1Ht.jpg,"Indonesia, Artificial Intelligence, AI, Technology"
https://medium.com/s/story/beberapa-cara-untuk-preprocessing-data-dalam-machine-learning-13cef4294614,Beberapa Cara untuk Preprocessing Data dalam Machine Learning,AC,2018-04-02T15:49:14.400Z,"Founded in 2017, Warung Pintar has officially concluded its operations, marking the close of a meaningful chapter. Thank you for reading our stories along the way.
Biasanya data yang kita dapatkan dari flat files atau database berupa data mentah. Algoritma machine learning classification bekerja dengan data yang akan diformat dengan cara tertentu sebelum mereka memulai proses training. Untuk menyiapkan data untuk konsumsi oleh algoritma machine learning, kita harus memrosesnya terlebih dahulu dan mengonversinya menjadi format yang tepat.
dalam penerapannya kita bisa menggunakan scikit-learn untuk preprocessing data, dalam library scikit-learn ada banyak fungsi-fungsi yang tersedia.
prosesbinarizationadalah ketika kita ingin mengubah variabel numerik kedalam nilai boolean (0 dan 1)
Mean removal adalah cara umum dalam teknik preprocessing yang digunakan dalam machine learning, menghilangkan rata-rata biasanya sangat berguna dari variabel, jadi variabel berada ditengah tengah pada angka 0. kita melakukannya untuk menghilangkan bias dari variabel
biasanya dalam dataset yang masih mentah, beberapa variabel memiliki nilai yang sangat bervariasi dan random, jadi sangat penting untuk di scale feature feature tersebut,in my prespectivefeature-feature tersebut nilainya sangat besar atau kecil hanya karenathe nature of measurements
Biasanya dalam preprocessing, proses normalisasi untuk memodifikasi nilai dalam varabel sehingga kita dapat mengukurnya dalam skala umum. Dalam machine learning, kita menggunakan berbagai bentuk normalisasi. Beberapa bentuk normalisasi yang paling umum bertujuan untuk mengubah nilai-nilai sehingga jumlahnya menjadi 1. Normalisasi L1 (di library scikit-learn), yang mengacu pada Penyimpangan Absolut Terkecil, bekerja dengan memastikan bahwa jumlah nilai absolut adalah 1 dalam setiap baris. Normalisasi L2, yang mengacu pada kuadrat terkecil, bekerja dengan memastikan bahwa jumlah kuadrat adalah 1. Secara umum, teknik normalisasi L1 dianggap lebih kuat daripada teknik normalisasi L2. Teknik normalisasi L1 kuat karena tahan terhadap outlier dalam data. Seringkali, data cenderung mengandung outlier dan kita tidak bisa berbuat apa-apa. Kami ingin menggunakan teknik yang dapat dengan aman dan efektif mengabaikannya selama perhitungan. Jika kita memecahkan masalah di mana outlier itu penting, maka mungkin normalisasi L2 menjadi pilihan yang lebih baik
Ketika kita melakukan klasifikasi, biasanya kita berurusan dengan banyak label. Label-label ini bisa dalam bentuk kata-kata, angka, atau sesuatu yang lain. Fungsi pembelajaran mesin dalam sklearn mengharapkan mereka menjadi angka. Jadi jika mereka sudah menjadi nomor, maka kita dapat menggunakannya secara langsung untuk memulai pelatihan. Tetapi ini tidak biasanya terjadi.Di dunia nyata, label dibuat dalam bentuk kata-kata, karena kata-kata dapat dibaca manusia. kita melabeli data training dengan kata-kata sehingga pemetaan dapat dilacak. Untuk mengonversi label kata menjadi angka, kita perlu menggunakan pembuat label encoding. label encoding mengacu pada proses transformasi label kata menjadi bentuk numerik. dalam hal regresi jika memuat varibel kategori dan nilainya tidak bisa di faktorisasi dalam bentuk tingkatan, dilakukan proses dummy, setiap nilai didalam variabel itu menjadi variabel lain.
Founded in 2017, Warung Pintar has officially concluded its operations, marking the close of a meaningful chapter. Thank you for reading our stories along the way.",https://miro.medium.com/v2/resize:fit:864/1*5Ohzn2z6VqpVQlI-EUcTOw.jpeg,"Preprocessing, Classification, Machine Learning, Data"
https://medium.com/s/story/1-4-masalah-etika-revolusi-mesin-pintar-146257093a08,(1/4) Masalah Etika Revolusi Mesin Pintar,Machine Learning Indonesia (ML ID),2018-05-02T07:11:35.198Z,"Machine Learning Indonesia
Kecerdasan buatan (articifical intelligenceatau disingkat AI) adalah istilah untuk kemampuan intelijen atau kecerdasan yang dimiliki oleh perangkat lunak yang berjalan pada mesin komputer. Tujuan utama dari penelitian kecerdasan buatan meliputi penalaran, pengetahuan, perencanaan, pembelajaran, pengamatan, persepsi, pengolahan bahasa alami, pengenalan pola dan banyak lagi. Saat ini, ada banyak fakta tentang kemajuan dramatis dalam AI seperti komputer yang berhasil mengalahkan juara dunia catur, mobil tanpa supir dari Google, robot pemain bulu tangkis, komputer memainkan video game, dan lain-lain. Hal tersebut cenderung menghasilkan prediksi spekulatif bahwa AI akan mencapai kecerdasan tingkat manusia di masa depan. Ray Kurzweil, pemikir terkemuka dalam prediksi teknologi, memperkirakan ini akan terjadi pada tahun 2029. Perusahaan teknologi dunia memulai taruhan besar dengan menempatkan investasi besar untuk profitabilitas AI. Untuk contoh, raksasa teknologi AS Google, Microsoft, Amazon, Twitter dan Facebook, serta Baidu dari China, baru-baru ini memacu departemen penelitian AI mereka dengan mempekerjakan peneliti AI terkenal dari kalangan akademisi. Disisi lain, nama-nama besar dalam teknologi seperti Stephen Hawking, Bill Gates dan Elon Musk telah memperingatkan tentang bahaya yang mungkin diakibatkan oleh mesin pintar dengan kecerdasan setara manusia.
Kemajuan AI dimulai oleh cabang ilmu komputer yang disebut pembelajaran mesin (machine learning), yang merupakan bidang studi AI untuk memberikan komputer kemampuan belajar dari data dan menghasilkan keputusan tanpa eksplisit diprogram. Pembelajaran mesin memungkinkan komputer untuk belajar dari pengalaman dan teknologi tersebut telah ada di mana-mana saat ini. Pembelaran mesin membuat pencarian Web yang lebih relevan, tes darah lebih akurat dan layanan kencan untuk menemukan calon pasangan sesuai kriteria. Secara sederhana, algoritma pembelajaran mesin mengambil kumpulan data yang ada, menyisir untuk mempelajari pola, kemudian menggunakan pola-pola ini untuk menghasilkan prediksi tentang data baru di masa depan. Terkesan sederhana, namun kemajuan dalam pembelajaran mesin selama dekade terakhir telah banyak berubah. Contohnya, AlphaGo, sistem komputer Google yang menggunakan pembelajaran mesin telah mengalahkan pemain Catur-Go yang terbaik di dunia. Atau robot dengan kamera dan kemampuan pengamatan visual dari pembelajaran mesin telah mampu bermain bulu tangkis dengan pemain professional.
Kompetisi mesin pintar tidak hanya melibatkan mesin. Beberapa tahun yang lalu Netflix perusahaan rental film online ingin membantu pelanggan menemukan film yang mereka senangi, terutama film-film yang tidak begitu terkenal dan bukan “rilis terbaru” yang sebagian besar diabaikan dalam katalog mereka. Perusahaan sudah memiliki sistem rekomendasi film, tapi masih jauh dari sempurna. Jadi perusahaan meluncurkan kompetisi untuk memperbaiki sistem yang ada dengan aturan sederhana: pemenang pertama untuk yang mengalahkan kinerja sistem terdahulu sebesar 10 persen akan mendapatkan hadiah $1-juta. Puluhan ribu peneliti dan insinyur dari seluruh dunia mendaftar. Bagi mereka, kompetisi ini seperti mimpi dan bukan hanya untuk hadiah uangnya. Komponen yang paling penting dari sistem mesin pintar adalah data dan Netflix menyediakan 100 juta data real yang siap untuk digunakan. Saat ini tidak hanya NetFlix, semua raksasa teknologi seperti Facebook, Google, Microsoft, Amazon, Twitter, Samsung telah berkolaborasi untuk mempercepat kemajuan mesin pintar dengan berbagi pengalaman, teknologi dan yang lebih penting lagi banyak data secara gratis.
Kemajuan akan semakin dipercepat. Mesin pintar bukanlah konsep sci-fi yang masih jauh, tetapi kita sudah menggunakannya setiap hari tanpa menyadarinya. Ketika anda mengetik di software pengolah kata, banyak algoritma pembelajaran mesin yang membantu untuk mengetik lebih baik dengan memprediksi kesalahan ketik dan memberi perbaikan. Atau ketika anda membuka browser untuk mencari sesuatu di internet. Kita tidak tahu seberapa jauh kemajuan teknologi mesin pintar ini akan berkembang untuk mengubah cara hidup kita dengan disadari maupun tidak.
Ada beberapa jenis atau bentuk mesin pintar karena kecerdasan buatan merupakan konsep yang sangat luas. Saya menggunakan tiga kategori utama dalam tulisan ini:
ANI adalah mesin pintar yang sama atau melebihi kecerdasan atau efisiensi manusia pada hal tertentu. Beberapa contoh seperti mobil tanpa supir Google, pengenalan suara di Smartphone (Siri, Cortana, S-Voice), filter spam email, penerjemah dan pencarian Google, Facebook face recognition dan lain-lain. Sistem ANI seperti sekarang, tidak sangat berbahaya. Tapi sementara ANI tidak memiliki kemampuan untuk menyebabkan ancaman eksistensial, kita harus melihat ini dari sisi ekosistem yang semakin besar dan kompleks, karena meskipun relatif tidak berbahaya ANI akan mengubah dunia. Selanjutnya para ilmuwan dan filsuf percaya bahwa AI adalah suatu revolusi, mengambil jalan dari ANI, melalui AGI, untuk menuju ke ASI yang tentu saja akan membuat dampak besar bagi kemanusiaan. Mereka percaya setiap inovasi ANI secara tidak sadar akan menambahkan batu bata ke jalan menuju AGI dan kemudian ASI. Selama proses tersebut isu-isu etika yang terkait dengan penciptaan masa depan, kemungkinan adanya mesin dengan kemampuan AGI yang jauh melampaui manusia, akan menimbulkan masalah etika yang sangat berbeda. ASI bukan hanya pengembangan teknologi tetapi akan menjadi penemuan paling penting yang pernah dibuat manusia sehingga akan menyebabkan ledakan kemajuan di segala bidang ilmiah dan teknologi. Mesin pintar ASI bukan hanya super intelijen, tapi mampu terus berkembang memodifikasi dan memperbaiki kecerdasannya.
Sulit untuk membuat mesin sepintar manusia karena otak manusia adalah obyek paling kompleks di alam semesta. Google saat ini menghabiskan miliaran dolar mencoba untuk melakukannya. Hal-hal sulit untuk otak manusia seperti kalkulus, strategi pasar keuangan, dan menterjemahkan bahasa, saat ini menjadi sangat mudah dengan komputer, sementara hal-hal mudah pada manusia seperti pengamatan gerak, dan persepsi, sangat sulit dilakukan oleh komputer. Ilmuwan komputer Donald Knuth katakan, “AI telah sekarang telah berhasil melakukan segala sesuatu yang membutuhkan pemikiran, tetapi gagal untuk melakukan apa yang orang dan hewan lakukan tanpa berpikir”.
Kita tidak akan membahas kompleksitas teknis untuk mencapai AGI atau ASI dalam blog ini. Sebaliknya, kita akan menyimak dan mendiskusikan beberapa masalah etika sebagai akibat dari revolusi mesin pintar yang saat ini terjadi. Kami percaya kemajuan teknologi mesin pintar dapat menimbulkan konflik besar terhadap aspek kemanusiaan dalam konteks kualitas pemikiran moral dan etika saat ini. Namun diharapkan pesan itu akan sampai ke arsitek mesin pintar untuk menentukan motivasi yang lebih baik bagi kemanusiaan. Mesin pintar dapat menjadi tak terkendali karena keunggulan intelektual dan teknologinya yang bisa berkembang sendiri. Oleh sebab itu sangatlah penting untuk memberikan motivasi ramah kemanusiaan kepada mesin pintar atau insinyur yang nanti akan membuat mesin pintar.
Baca selanjutnya mengenaiANI,AGIdanASI. Semoga bermanfaat!
TSMRA, Jakarta, 2016.
Disadur darihttp://deepbrains.com/2016/06/1-masalah-etika-revolusi-mesin-pintar/seijinPenulis
Machine Learning Indonesia
Machine Learning Indonesian Community",,"Ethics, Artificial Intelligence, Machine Learning, Robots"
https://medium.com/s/story/memproses-category-data-1515245c83e7,Memproses Category Data,R. Kukuh,2018-02-13T03:54:29.544Z,"Dalam memproses suatu dataset, kita harus memperhatikan jenis-jenis data agar tidak terjadi kesalahan proses.
Salah satu jenis data yang akan dibahas dalam artikel kali ini adalah: Categorical Data.
Mari kita lihat dataset dariproject sebelumnyaberikut ini:
Terlihat jelas bahwa Age dan Salary adalah jenis Numberical Data, sedangkan Country dan Purchased termasuk Categorical Data.
Buka Anaconda Navigator, lalu buka Spyder
Lanjutkan project sebelumnya tentangData Preprocessing 03
Tambahkan kode berikut ini:
Eksekusi kode diatas, sekaligus cek nilai X terbarunya seperti ini:
Dibandingkan dengan nilai X sebelumnya:
Perhatikan bahwa sekarang value dari kolom Country telah berubah:
Tentu, angka 0, 1, dan 2 ini seharusnya tidak membuat Germany lebih baik dari Spain, dan Spain lebih baik dari France bukan?
Itu sebabnya kita masih harus melakukan proses selanjutnya: Dummy Encoding / One Hot Encoding
Tambahkan kode berikut:
Eksekusi kode tersebut, lalu lihat nilai X terbaru:
Agar lebih enak dilihat, lihatlah nilai X dari variable explorer
Jika dibandingkan dengan value awal kolom Country, adalah sebagai berikut:
Seperti yang sudah dijelaskan diatas, ada 2 kolom yang jenisnya categorical data, yaitu Country dan Purchased.
Tambahkan kode berikut untuk meng-encode kolom Purchased:
Eksekusi kode tersebut, lalu cek nilai Y terbaru:
Kita pun bisa memeriksanya lewat variable explorer:
Catatan: Hasil encode kolom Purchased ini tidak perlu diproses lagi dengan OneHot Encoder, karena tidak ada kerancuan antara label “Yes” dan “No”
Lanjutkan project sebelumnya tentangData Preprocessing 03
Tambahkan kode berikut ini:
Eksekusi kode tersebut, lalu lihat isi dataset terbaru
Lakukan hal yang sama untuk kolom Purchased
Eksekusi kode tersebut, lalu lihat isi dataset terbaru
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*hlKU0svjVvcNecjErPaouw.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/4-4-masalah-etika-revolusi-mesin-pintar-15551b38c8f5,(4/4) Masalah Etika Revolusi Mesin Pintar,Machine Learning Indonesia (ML ID),2018-05-02T07:10:38.610Z,"Machine Learning Indonesia
Dari AGI dengan tingkat kecerdasan yang sama dengan manusia dan pesatnya pertumbuhan kemampuan komputer, mesin pintar diprediksi mampu melebihi manusia karena beberapa hal berikut ini:
Sisi Perangkat keras:
Mesin yang kemungkinan akan mencapai AGI dengan kemampuan meningkatkan kemampuan dirinya sendiri, tidak akan melihat “intelijen tingkat manusia” secara relevan dari sudut pandang manusia dan tidak akan memiliki alasan untuk “berhenti” di tingkat manusia. Dan mengingat keunggulannya dibandingkan manusia, sudah cukup jelas bahwa dalam waktu cepat mesin pintar akan berpacu ke ranah Superintelligence (ASI). Ini mungkin akan mengejutkan manusia ketika itu terjadi. Alasannya adalah bahwa dari sudut pandang manusia, kecerdasan berbagai jenis hewan bervariasi, manusia menyadari bahwa intelijen hewan jauh lebih rendah dan melihat manusia cerdas lebih pintar dari manusia paling bodoh.
Artificial Superintelligence (ASI) adalah mesin sangat cerdas untuk memahami desain sendiri sehingga bisa mendesain ulang sendiri atau menciptakan sistem penerus, lebih cerdas, yang kemudian bisa mendesain ulang sendiri lagi untuk menjadi lebih cerdas, dan seterusnya dalam siklus umpan balik positif. Ini adalah “ledakan intelijen.” Skenario rekursif yang tidak terbatas terjadi pada mesin pintar. Jika manusia memiliki mesin yang meningkat IQ nya, ledakan intelijen pasti terjadi kepada, setelah manusia menjadi cukup pintar, akan terus mencoba untuk merancang versi yang lebih pintar.
ASI juga mungkin dicapai dengan meningkatkan kecepatan pemrosesan, neuron diamati tercepat beruulang 1000 kali per detik; serat akson perilaku sinyal tercepatnya pada 150 m/s, setengah-juta kecepatan cahaya. Tampaknya sangat mungkin secara fisik untuk membangun otak yang dapat menghitung satu juta kali lebih cepat dari otak manusia, tanpa menyusut ukurannya dan bisa menulis ulang perangkat lunak. Jika pikiran manusia dipercepat, tahun subjektif dari pemikiran akan dicapai untuk setiap 31 detik fisik di dunia luar, dan milenium tercapai di delapan setengah jam. Ini disebut pikiran melesat naik seperti “superintelligence”: pikiran yang berpikir seperti manusia, tetapi jauh lebih cepat.
Yudkowsky mengusulkan tiga kategori metafora untuk memvisualisasikan kemampuan ASI:
Bahkan jika kita membatasi diri untuk metafora sejarah, jelas bahwa ASI menyediakan tantangan etika yang secara harfiah belum pernah terjadi sebelumnya. Pada titik ini taruhannya tidak lagi dalam skala individu (misalnya, hipotek tidak adil disetujui, orang dianiaya) tapi pada skala global atau kosmis (misalnya, manusia dimusnahkan). Atau jika ASI dapat dibentuk untuk menjadi bermanfaat, kemudian tergantung pada kemampuan teknologi, mungkin menyelesaikan banyak masalah masa kini yang telah terbukti sulit untuk intelijen tingkat manusia.
ASI adalah salah satu dari beberapa “risiko eksistensial” seperti yang didefinisikan oleh Bostrom. Dimana bisa berisiko memusnahkan kehidupan cerdas di bumi secara permanen atau sebaliknya, ASI bisa secara positif melestarikan kehidupan cerdas dibumi dan memenuhi potensinya. Penting untuk ditekankan bahwa ASI membawa potensi keuntungan besar serta risiko tinggi secara bersamaan.
Misalkan intuisi kita tentang skenario masa depan yang “masuk akal dan realistis” dibentuk oleh apa terlihat di TV dan di film atau novel. Sebagian besar dari wacana tentang masa depan adalah dalam bentuk fiksi dan konteks rekreasi lainnya. Ketika berpikir kritis kita menduga intuisi seakan-akan bergerak ke arah nyata karena skenario tersebut tampak jauh lebih akrab. Ilusi bisa menjadi cukup kuat. Kapan terakhir kali anda melihat sebuah film tentang kepunahan manusia secara tiba-tiba tanpa peringatan dan tanpa diganti oleh peradaban baru. Padahal skenario ini jauh lebih mungkin daripada skenario di mana pahlawan manusia berhasil mengusir invasi alien atau prajurit robot, yang tentu saja tidak akan menyenangkan untuk ditonton. Akan menjadi suatu kesalahan besar untuk menganggap mesin pintar ASI sebagai spesies dengan karakteristik statik dan terus bertanya, “apakah mereka baik atau jahat?”. Istilah “Artificial Intelligence” mengacu pada desain ruang yang luas, mungkin jauh lebih besar dari ruang pikiran manusia karena semua manusia berbagi arsitektur otak yang sama dan terbatas.
Apakah mesin pintar pada level ASI dapat dikontrol efeknya terhadap kehidupan manusia? Kurzweil menyatakan bahwa “kecerdasan secara inheren tidak mungkin dikontrol,” dan meskipun manusia dapat mengambil tindakan pencegahan, “mesin pintar akan dengan mudah mengatasi hambatan tersebut.” ASI tidak hanya super pintar, tapi memiliki kemampuan meningkatkan kecerdasan sendiri, memiliki akses tanpa hambatan ke kode sumber sendiri sehingga dapat menulis ulang sendiri apa pun yang dirinya inginkan.
Sejauh ini dalam pengembangan AI, apakah ada cara yang bisa dilakukan untuk mengarahkan AI menjadi baik melalui penelitian modern? Tampak terlalu dini untuk berspekulasi, tetapi bukan tidak mungkin beberapa paradigma AI lebih unggul daripada yang lain sehingga akhirnya membuktikan penciptaan mesin pintar yang mampu memodifikasi kecerdasannya sendiri. Misalnya AI berbasis Bayesian yang terinspirasi oleh sistem matematika koheren seperti teori probabilitas yang diharapkan dapat memaksimalkan kemampuannya. Bayesian tampaknya lebih dekat ke masalah modifikasi diri dibanding pemrograman evolusioner dan algoritma genetika lainnya. Seperti terlalu kontroversial, tetapi ini menggambarkan bahwa jika kita berpikir tentang tantangan Superintelligence, seharusnya kita bisa mengarahkan penelitian AI menjadi lebih terkontrol. Tetapi bagaimana kalau mesin pintar ASI tercipta karena ketidaksengajaan di laboratorium?
Seandainya kita menentukan tujuan sistem AI untuk suatu saat mampu secara terus menerus memodifikasi dan memperbaiki diri, ini akan mulai menyentuh masalah etika inti mengenai Superintelligence. Manusia memiliki kecerdasan umum pertama di bumi yang telah digunakan secara substansial membentuk kembali pegunungan, membangun gedung pencakar langit, membuat pertanian di gurun, bahkan mengakibatkan perubahan iklim yang tidak diinginkan. Sebuah kecerdasan yang lebih kuat bisa memiliki konsekuensi yang jauh lebih besar.
Pertimbangkan lagi metafora historis untuk ASI — mirip dengan perbedaan antara masa lalu dan peradaban sekarang. Peradaban sekarang ini tidak lepas dari pengaruh Yunani kuno dan terus berubah karena peningkatan ilmu pengetahuan dan kemampuan teknologi. Ada perbedaan perspektif etis seperti Yunani Kuno berpikir perbudakan diterima; saat ini kita berpikir sebaliknya. Bahkan antara abad 19 ke 20 ada perbedaan pendapat etis substansial mengenai hak perempuan. Atau hak orang kulit hitam. Mungkin bahwa etika orang saat ini tidak terlihat sebagai etis sempurna di peradaban masa depan, bukan hanya karena kegagalan untuk memecahkan masalah etika saat ini diakui, seperti kemiskinan dan ketimpangan, tetapi juga untuk kegagalan dalam mengenali masalah etika tertentu. Mungkin suatu hari nanti memaksa anak ke sekolah akan dianggap pelecehan atau sebaliknya, membiarkan anak-anak untuk meninggalkan sekolah pada usia 18 dipandang sebagai pelecehan anak. Kita tidak tahu. Mengingat sejarah etika peradaban manusia selama berabad-abad, kita dapat memprediksi bahwa mungkin terjadi tragedi yang sangat besar. Bagaimana jika ternyata Archimedes dari Syracuse telah mampu menciptakan kecerdasan buatan tahan lama dengan versi moral Yunani Kuno? Kadang ide baru yang baik dalam etika datang bersama atau datang sebagai kejutan; tetapi perubahan etika yang dihasilkan paling acak akan menyerang kita sebagai kebodohan atau omong kosong.
Ini memberi tantangan utama etika mesin pintar. Bagaimana manusia dapat membangun sebuah mesin pintar yang ketika dijalankan menjadi lebih etis dari penciptanya? Sulit meminta filsuf kita saat ini untuk menghasilkan super-etika sepertinya halnya insinyur AlphaGo bukan pecatur terbaik. Tapi kita harus dapat secara efektif menjelaskan pertanyaan, jika tidak dadu bergulir terus dan tidak akan menghasilkan gerakan catur yang baik atau etika yang baik. Atau mungkin ada cara yang lebih produktif untuk berpikir tentang masalah ini. Apa strategi yang dapat kita katakan kepada Archimedes sebelum membangun mesin ASI, sehingga hasil keseluruhan nantinya masih bisa diterima. Padahal kita sendiri tidak bisa mengatakan kepadanya apa yang secara khusus ia lakukan salah. Dalam banyak situasi saat ini, kita sangat relatif terhadap masa depan.
Salah satu ide yang sering diajukan adalah mempertimbangkan situasi seperti kasus Archimedes dan kita tidak perlu mencoba untuk menciptakan sebuah “Superintelligence” dengan etika yang tetap. Mungkin kita harus mempertimbangkan bagaimana AI diprogram oleh Archimedes, tanpa keahlian lebih bermoral dari Archimedes, tetapi bisa mengenali (setidaknya beberapa) etika peradaban kita sendiri. Disini kita akan mengharuskan mesin pintar AI untuk dapat memahami struktur etika manusia, seperti AlphaGo memahami aturan permainan catur. Jika kita serius mengembangkan mesin pintar AI, banyak tantangan yang harus kita hadapi. Jika mesin harus ditempatkan dalam posisi yang lebih kuat, lebih cepat, lebih dipercaya, atau lebih pintar dari manusia, maka disiplin etika mesin harus berkomitmen untuk melebihi manusia, bukan hanya setara, dalam hal kebaikan etiks.
Titik-Titik PemikiranAda banyak perdebatan tentang seberapa cepat mesin pintar ANI akan mencapai AGI kemudian ASI. Hasil survei dari ratusan ilmuwan tentang kapan mereka percaya peradaban manusia mencapai AGI adalah sekitar tahun 2040 [13]. Itu berarti hanya 24 tahun dari sekarang. Menariknya, banyak pemikir di bidang AI memprediksi kemungkinan perkembangan dari AGI ke ASI akan terjadi sangat cepat. Skenario seperti dibawah ini bisa terjadi:
Dibutuhkan puluhan tahun untuk sistem AI pertama yang mencapai kecerdasan umum tingkat rendah, tapi akhirnya terjadi. Sebuah komputer mampu memahami dunia di sekitarnya seperti anak berusia empat tahun. Tiba-tiba, dalam waktu satu jam dari saat itu, mesin itu memahami teori grand fisika yang menyatukan teori relativitas umum dan mekanika kuantum, sesuatu belum mampu dilakukan manusia saat ini. 90 menit setelah kejadian itu, AI telah menjadi ASI, 170.000 kali lebih cerdas dari manusia.
Besarnya kemampuan ASI bukan sesuatu yang bisa kita pahami dengan mudah. Dalam ukuran kita, cerdas berarti IQ 130 dan bodoh berarti IQ 85 tetapi kita tidak memiliki kategori untuk IQ 12.952 dan tidak bisa membayangkan kemampuannya. Apa yang kita ketahui dari dominasi manusia dibumi saat ini, sangat jelas kecerdasan memberikan kekuasaan. Yang berarti ASI, ketika kita bisa menciptakannya akan menjadi makhluk yang paling kuat dalam sejarah kehidupan di bumi. Jika otak manusia saat ini mampu menciptakan wifi, mesin pintar dengan 100 atau 1.000 atau 1 miliar kali lebih pintar seharusnya tidak memiliki masalah mengendalikan posisi setiap atom di dunia dengan cara apapun itu. Segala sesuatu yang kita bayangkan seperti sihir atau kekuasaan Tuhan akan menjadi seperti kemampuan biasa untuk mesin pintar ASI. Mesin ini dapat menciptakan teknologi untuk membalikkan penuaan manusia, menyembuhkan penyakit, memberantas kelaparan dan bahkan kematian, memprogram ulang cuaca untuk melindungi masa depan bumi, semua itu tiba-tiba mungkin dan mudah. Ini juga sangat mungkin sebagai akhir dari semua kehidupan dibumi. Sejauh yang kita khawatirkan, jika ASI benar terjadi maka ada Tuhan yang Mahakuasa dibumi dan pertanyaan paling penting untuk kita tanyakan adalah apakah Tuhan ini akan menjadi baik atau buruk?
Baca kembali mengenai (1)pengenalan, (2)ANIdan (3)AGI.
TSMRA, Jakarta, 2016.
Disadur darihttp://deepbrains.com/2016/06/44-masalah-etika-revolusi-mesin-pintar/seijinPenulis.
Machine Learning Indonesia
Machine Learning Indonesian Community",,"Deep Learning, Artificial Intelligence, Ethics, Machine Learning, Robots"
https://medium.com/s/story/kebun-kurma-di-indonesia-tlpn-wa-0822-4069-7469-kebun-kavling-kurma-di-indonesia-157e0e99538d,"Kebun Kurma Di Indonesia, Tlpn/ Wa 0822–4069–7469, Kebun Kavling Kurma Di Indonesia",umbar winardi,2018-08-14T02:54:41.019Z,"Karena itulah jika Anda sudah punya investasi yang tepat, maka Anda tidak perlu khawatir lagi untuk menjalani sisa hidup Anda di masa yang akan datang.
Kavling Taman Kurma, tempat investasi terbaik
Salah satu tempat Anda untuk menginvestasikan dana yang dimiliki adalah dengan bergabung bersama di Kavling Taman Kurma.
Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Anda pun tidak akan rugi jika beli kavling diKavling Taman Kurmakarena harga tanah selalu naik dan ini akan jadi investasi yang terbaik untuk Anda.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:1200/1*SUlGwscgBB0i_ljv8BTYbg.jpeg,"Organizational Culture, AR, AI, Sales, Yahoo"
https://medium.com/s/story/titimangsa-manusia-bagian-i-162aa1fd96af,Titimangsa Manusia: Bagian I,Realino Marpaung,2022-12-07T13:14:50.055Z,"Jadi begini, di antara tik-tok, #2019gantipresiden, dan Via Vallen, menurut saya ada beberapa hal penting lain yang seharusnya menjadi perhatian para insan-umat-kaum-khalayak-masyarakat-warganet-netizen yang sangat penting untuk dibahas, yaituAI (Artificial Intelligence).
Ya, kamu semua yang lagi baca tulisan tidak penting saya ini harusnya tahu mengenai AI atau yang juga bisa kita sebut sebagai Kecerdasan Buatan — sebenarnya entah kenapa saya sedikit kurang suka dengan istilah ini, masalahnya kalo kita persingkat istilah Kecerdasan Buatan menjadi KB, orang-orang akan lebih mudah mengarahkan pikirannya ke program Keluarga Berencana bukan kepada Kecerdasan Buatan itu sendiri.
Menurut hemat saya alangkah lebih baik jika kita gunakan istilah IB (Inteligensi Buatan) agar supaya orang-orang tidak tertukar dengan istilah KB (Keluarga Berencanca) tapi ya siapalah saya yang bisa mengatur netizen,influencersaja bukan.
Beberapa bulan yang lalu dunia teknologi dihebohkan dengan kemunculanGoogle Duplex— semacamSiri tapi lebih kerengitu. Soalnya begini, si Duplex ini bisa benar-benar meniru artikulasi dan intonasi sebagaimana manusia berbicara pada umumnya.
Jadi ketika kamu sedang berkomunikasi dengan si Duplex, kamu tidak akan tahu kalau kamu sedang berbicara dengan sebuah mesin! ya meskipun ada beberapamasalah etis dan moralyang ditimbulkan, tapi demi melancarkan tulisan dan memuaskan kehendak saya sendiri, marilah kita berfokus bagaimana Kecerdasan Buatan ini begitu hebat dan adiluhung.
Pada mulanya adalah firman, kemudian menjelma menjadi algoritma dan data. Mau kamu setuju atau tidak, manusia akan menjadikan teknologi(AI,machine learning,deep learning) sebagai sebuah kebutuhan primer. Tidak usah jauh-jauh, ponsel yang kamu pakai untuk membaca ini sudah dilengkapi olehAI dan begundalnya, tetapi kita sama-sama tahu, tak ada satu hari pun terlewat tanpa ponsel pintar di dalam genggaman.
Segala aktivitas yang kamu lakukan akan tersimpan dan diingat oleh gawaimu, segala hal yang kamu suka (apa yang menjadi preferensimu) akan disediakan oleh gawaimu secara otomatis: musik, gim, dan video. Semuanya akan direkomendasikan untukmu.Algoritma akan mulai masuk lewat sisi-sisi tempat tidurmu dan perlahan akan menjadi penguntit pribadimu: kapanpun dan dimanapun kamu berada, dia akan ada di sana.Dia akan mencoba mengerti kamu dan perlahan….dia akan menggantikanmu.
Perubahan mulai pegari di ufuk selatan tanah Inggris. Terletak di kota bernama Andover, di antara padang rumput dan tembok-tembok bata merah, berdiri sebuah “gudang”, di sana apa yang kita sebutotomasi(automation) bersimaharajalela.
Mereka menggunakan sebuah sistem yang bernamagrid system,mereka nyaman dengan tubuh mereka yang hanya berbentuk kotak, mereka bekerja terus menerus tanpa mengenal lelah kendati sudah mulai kehilangan tenaga, mereka dapat langsung mengisi tenaga mereka kembali, tugas mereka sederhana: menyortir, mengangkat, dan menggerakkan. Mereka — atau yang biasa kita sebut dengan robot — bekerja selama 24 jam.
Robot-robot kotak ini bekerja untuk memilah pesanan kebutuhan sehari-hari sesuai dengan yang kita pesan, dan mereka dapat mengepak sebanyak 65 ribu pesanan. Pesanan yang diterima itu datang dari berbagai macam kategori, mulai dari makanan ringan, peralatan mandi, minuman, dst.
Secara sederhana, ini bagaikan kita membeli barang di sebuah pasar swalayan tanpa harus beranjak dari rumah. Tidak, kamu tidak sedang melihat film bertema fiksi-ilmiah, kamu sedang melihat kehidupan nyata. Otomasi sudah berjalan dan kita tidak bisa melakukan apa-apa.
Paling tidak 20 tahun lagi, kita semua akan melihat bagaimana para robot ini akan menggantikan kita bekerja.Pekerjaan yang akan digantikan oleh robot-robot ini adalah pekerjaan yang bersifat repetitif, terprediksi dan yang biasanya dilakukan secara rutin.Pekerjaan yang paling besar kemungkinannya digantikan oleh robot adalah
Sekarang mungkin kamu berpikir bahwa kamu selamat dari sentuhan mematikan otomasi ini karena kamu tidak bekerja pada bidang yang disebutkan, tapi tenang saya akan membuatmu menyesal telah berpikir seperti itu.
Sebanyak 800 juta pekerja akan terotomasidan tidak peduli apa pekerjaanmu, robot-robot bajingan ini akan menghajarmu. Oke, sekarang mari kita coba berpikir sejenak, pekerjaan apa yang paling kecil kemungkinannya untuk digantikan oleh Kecerdasan Buatan? Hmm…. Bagaimana dengan pengacara? TETOTT! TEROTOMASI!
Ini serius saya tidak sedang berkelakar. Alih-alih membuat perusahaan rintisan (start-up) bertemakan transportasi seperti Grab dan Go-Jek yang sedang populer belakangan ini,sebuah perusahaan rintisan bernama Atrium yang bermarkas di Lembah Silikon (Sillicon Valley) ini berencana untuk membuat perusahaan rintisan yang nantinya dapat menjadi sebuah firma hukum.
Perusahaan yang dimiliki oleh Justin Kan — dia juga merupakan pencipta Twitch — berencana mengembangkan sebuah Kecerdasan Buatan yang dapat menggantikan tugas para advokat di sebuah firma hukum. Boleh jadi si Atrium ingin menjadi anakindieyang beda dan unik diskenaperusahaan rintisan.
Setelah melihat contoh di atas mungkin kamu masihngeyeldan tetap percaya bahwa ada pekerjaan yang tidak bisa diotomasi. Saya suka idealismemu, sekarang mari kita hancurkan itu secara perlahan.
Mari kita berpikir secara sederhana, jika kita mendengar kata “robot” apa yang kita pikirkan? Seonggok mesin yang hanya menuruti perintah dari program bawaan? Sebuah benda yang kaku dan hanya mengikuti apa yang kita mau? Mungkin hatimu mulai luluh dan mulai percaya bahwa Kecerdasan Buatan benar-benar akan menggantikan manusia di dalam hal pekerjaan.
“Tapi tetap saja, mereka cuma sekumpulan data dan algoritma! Robot-robot itu tidak memiliki perasaan!” pikirmu. Ya ampun, sepertinya kamu tidak bisa lebih salah dari ini. Kamu lalu berkesimpulan ada tiga hal yang tidak akan bisa digantikan oleh robot: emosi, kreatifitas dan intuisi. Dan sepertinya kesimpulanmu benar…. atau salah?
Ketiga hal tersebut mungkin bisa menjadi solusi kita untuk beberapa tahun ke depan. Tidak pernah ada dalam imajinasi terliar manusia bahwa robot akan menjadi pengajar di sekolah Taman Kanak-kanak, menjadi seniman kiwari terkenal layaknya Yayoi Kusama, atau menjadi psikolog dan mendengarkan segala keluh kesah manusia di sesi konseling.
Tapi bagaimana untuk tahun-tahun berikutnya? Jawabannya hanya satu: kita tidak tahu. Ya, kita tidak benar-benar tahu bagaimana transisi perkembangan teknologi ini akan membawa kita. Untuk bidang kreativitas? Sepertinya saya menemukan tanda-tandanya.
the sun rays struck my facewarm tingles to my fingertipsthe light showed me a pathi should walk downi spoke and the whispers of the breezetold me to close my eyesi lost my way in a paradise
Potongan sajak di atas bukan diciptakan oleh pujangga-pujangga terkenal layaknya Rupi Kaur atau Robert M Drake, tapi diciptakan oleh Kecerdasan Buatan yang dikembangkan oleh perisetdari Microsoft dan Universitas Kyoto.
Ini sungguhan! Kamu tidak salah baca, algoritma ini mengembangkan dan menulis sajak dari foto-foto dan deskripsi foto yang diberikan. Lihatlah bagaimana Kecerdasan Buatan mulai menemukan dan mengisi ceruk dimana hanya manusia yang bisa melakukannya. Dan ini semua tidak berakhir sampai di situ saja.
Namanya Aiva, merupakan akronim dariArtificial Intelligence Virtual Artist.Sebuah Kecerdasan Buatan yang diciptakan oleh Aiva Technologies, perusahaan rintisan yang dibentuk di Luksemburg dan London.
Perusahaan ini membuat suatu sistem Kecerdasan Buatan yang bisa menggubah komposisi musik klasik secara mandiri. Dan yang lebih gilanya lagi, dia sudah mengeluarkan sebuah album! Kecerdasan Buatan yang bisa membuat komposisi musiknya sendiri dan mempunyai sebuah album! Anjing!Albumnya yang bertajukGenesisini sudah mendapat pengakuan dari perkumpulan komposer dan pembuat musik Prancis atau SACEM (Société des auteurs, compositeurs et éditeurs de musique) dan juga sudah mendapatkan hak ciptanya sendiri.
Jadi di masa depan nanti (kalau ingin dibilang seperti itu), manusia tidak hanya berkompetisi bersama manusia lainnya, melainkan ada pemain baru yang hadir. Dia tidak terbuat dari darah dan daging, melainkan terbuat dari besi dan algoritma. Kamu mungkin tenang-tenang saja mengetahui hal ini, atau kamu mungkin merasa khawatir, ya itu sih terserah padamu. Tapi apa tidak sebaiknya kita bersiap-siap?
*Tulisan ini merupakan sebuah usaha saya untuk mencoba menulis sebuah artikel berita tentang teknologi secara menarik — penakanan ada pada kata menarik — tanpa latar belakang pemahaman penulisan dan teknologi secara utuh, jadi mohon dibaca dengan cermat dan hati-hati. Terima kasih!",https://miro.medium.com/v2/da:true/resize:fit:1200/1*CtWw_ORIN9SZDWpWkSVZyw.gif,"Kehidupan, AI, Ekonomi, Kerja, Teknologi"
https://medium.com/s/story/custom-object-detection-using-tensorflow-api-bahasa-1758434fe133,Tensorflow API - Custom Object Detection (Bahasa),Syarifah Rosita Dewi,2019-03-29T04:02:15.209Z,"We are official team of Akademi AI Indonesia
Tutorial kali ini akan membahas tentang bagaimana mendeteksi suatu objek menggunakan data yang kita tentukan sendiri.
Pada kasus ini, saya melakukan deteksi terhadap 2 objek sekaligus pada satu frame yaitu meja dan kursi yang memiliki motif ukiran (topik skripsi saya sebenernya)😅.
Dalam melakukan deteksi objek tentunya kita membutuhkan dataset untuk prosestrainingsehinggaNeural Networkyang dilatih dapat mengenali objek yang akan kita deteksi. Dataset yang saya gunakan total 500 gambar; 470 untuk train dan 30 untuk test/validation. Untuk menggunakan dataset yang saya gunakan dapat mengunduh full datasetdisini. Namun, jika ingin mendeteksi objek dengan data sendiri sebaiknya menggunakan dataset minimal 250 untuk 1 objek. Ohya, fyi dalam pembagian dataset train dan test saya lakukan secara manual.
Dalam melakukan pengumpulan dataset saya menggunakancrawling images padagoogle seperti pada postingan sayaberikut ini.
Sebelumnya, buatlah direktori supaya terlihat rapi dan tidak acak-acakan contohnya seperti punya saya berikut.
Dalam membuatannotationatau memberikan label pada gambar kita dapat menggunakan aplikasilabelImgyang akan disimpan dalam file xml. Cara menggunakanlabelImgsebagai berikut:
Dataset annotation yang telah kita buat sebelumnya menggunakan aplikasilabelImgperlu dikonversi ke dalam format .csv yang akan digunakan untuk generate TFRecord.
Di direktori yang saya buat sebelumnya terdapat file dengan nama “xml_to_csv” yang berisi kode dalam mengkonversi file xml ke csv.
Jalankan perintah berikut untuk melakukan konversi xml ke csv.
python xml_to_csv.py
Pada saat melakukan proses training, tensorflow akan membaca data input dalam format TFRecord yang dinamakan feeding data. Oleh karena itu perlu dilakukan generate data annotation yang tadi telah dikonversi ke file .csv.
Untuk kode TFRecord sendiri juga sudah ada dengan nama file “generate_tfrecord”. Hal yang perlu dilakukan adalah merubah kategori sesuai dengan kategori yang akan didefinisikan. Karena saya memiliki 2 objek yang akan dideteksi maka kategori yang diubah seperti berikut:
Jika ingin mendefinisikan 1 objek atau lebih dari 2 objek bisa disesuaikan. Ohya, untuk labelnya sendiri harus disesuaikan dengan nama yang kita pakai pada saat membuat annotation dilabelImg.
Kemudian, jalankan perintah berikut:
Label map ini digunakan untuk memetakan label yang akan digunakan untuk memberikan penamaan pada objek yang akan dideteksi. Karena pada penelitian ini saya akan mendeteksi 2 objek makaitem,idmaupunnamemenyesuaikan begitu juga jika objek hanya terdiri satu objek atau lebih dari 2 objek. Berikut konfigurasi label map yang akan digunakan:
Simpan dengan nama object-detection.pbtxt ke direktori data.
Konfigurasi pipeline disini berhubungan untuk mengatur file config yang nantinya akan digunakan untuk melakukan konfigurasi dari modeltrainingkarenatensorflowmenggunakanprotobufmaka konfigurasi ini sangat diperlukan. Model yang digunakan adalah SSD Mobilenet V1 dimana model ini sudah disediakan olehtensorflowitu sendiri.
Nah konfigurasi yang harus dilakukan terdapat pada filechair_table_v1yang terdapat di folder training. Ada beberapa bagian yang perlu diubah yang akan disesuaikan dengan model yang akan dibuat sebagai berikut:
Pada konfigurasi diatas pada bagiannum_classesterdapat 2 kelas/objek yang digunakan yaitu meja dan kursi (note: bisa disesuaikan dengan jumlah objek yang diteliti).
Pada bagianbatch_size,saya menggunakan batch size = 2 karena spesifikasi komputer yang saya miliki masih CPU 😅 dan itu membutuhkan waktu yang cukup lama dibandingkan menggunakan GPU. Meskipun komputer yang saya gunakan CPU tapi hasil yang didapatkan bisa mencapai akurasi 80–90% hehe. Jika komputer anda memiliki spesifikasi lebih tinggi bisa menggunakan batch_size ukuran 4 atau lebih.
Selanjutnya, pada bagiannum_stepsyang digunakan untuk membatasi jumlah step yang digunakan pada saat training. Lagi-lagi karena spesifikasi komputer saya yang tidak mendukung, saya membatasi dengan jumlah 250000.num_stepsini juga disesuaikan dengan ukuran batch_size yang digunakan. Namun, jika ingin mengikutinum_stepsyang dihasilkan oleh tensorflow sendiri saat melakukan training, bagiannum_stepsini bisa dihapus.
Sampai tahap ini kita selesai menyiapkan dan mengatur file yang dibutuhkan untuk melakukan training model object detection yang akan kita lakukan. 🙂
Note: Pastikan susunan direktorinya benar .
Selanjutnya adalah melatih komputer agar bisa mengenali objek yang sudah kita siapkan tadi (training). Sebelum itu, ada beberapa hal yang perlu dilakukan sebelum melakukan training:
Sekarang, berikut perintah untuk melakukan training model:
^train_dirmerupakan lokasi penyimpanan checkpoint untuk training. ^pipeline_config_pathmengarah ke konfigurasi pipeline.
Untuk melihat grafik selama proses training dapat menggunakan TensorBoard, jalankan perintah dibawah ini untuk menjalankan TensorBoard.
Salah satu grafiknya seperti ini:
Sebelum memasuki proses testing untuk mendeteksi objek, jalankan perintah berikut untuk mengexport modelnya:
Pada bagianmodel.ckpt-xxxxxmerupakan banyaknya steps yang kita lakukan saat melatih model dapat di sesuaikan dengan yang hasil jumlah training yang kalian dapatkan. Untuk melihat dimana file tersebut berada ada di/models/research/object_detection/training. Punya saya ada 250000 steps.
Pada bagianoutput_directorymerupakan folder output tempat untuk menyimpan file hasil export modelnya. Nama folder hasil output nya saya menamakan dengan carving_detection.
Pada pengujian model dibutuhkan sampel gambar/video yang bisa diambil melalui kamera sendiri. Untuk menguji sampel gambar dapat memindahkan gambar yang sudah diambil ke dalam direktorimodels/research/object_detection/test_imagesdan berikan nama image1, image2, dst.
Pengujian model disini menggunakanjupyter notebookdan terdapat beberapa hal yang perlu diubah:
Note: Hapus bagian download
Hasilnya kurang lebih seperti ini:
Berhasil diprediksi dengan baikk. 🤗🤗🤗🤗
Comment jika ada pertanyaan jangan lupa “clapsnya” hehe.
We are official team of Akademi AI Indonesia",https://miro.medium.com/v2/resize:fit:895/1*d7RUZGiuPO3xWPevg0Tb5w.png,"Deep Learning, Object Detection, Python, TensorFlow, Machine Learning"
https://medium.com/s/story/mengarang-lirik-lagu-indonesia-menggunakan-lstm-179247685bae,Mengarang Lirik Lagu Indonesia Menggunakan LSTM,Dieka Nugraha K,2018-01-13T09:03:38.438Z,"Setelah membaca artikelunderstanding LSTMnya Christoper Olah yang membahas cara kerja LSTM secara rinci danThe Unreasonable Effectiveness of Recurrent Neural Networksnya Andrej Karpathy yang berhasil melatih LSTM yang dapat digunakan untuk “mengarang” berbagai macam teks, saya sempat dapat ide untuk membuat RNN yang dilatih untuk membuat teks lirik lagu berbahasa Indonesia. Sayangnya waktu itu belum ada GPU yang bisa dipakai. Sekarang setelah berhasil merakitdeep learning rig, akhirnya project ini bisa dimulai.
Untuk mengarang lirik lagu (atau lebih tepatnya membuat model bahasa pada level karakter) saya menggunakan model char-rnn dan melatihnya banyak teks lirik lagu berbahasa Indonesia. Berikut ini adalah beberapa teks lagu karangan model yang sudah dilatih:
Dapat dilihat dari hasil yang ditampilkan diatas bahwa“mengarang” sebenarnya bukan kata yang tepat untuk mendeskripsikan lirik lagu yang dihasilkan karena kalimat yang dihasilkan tidak masuk akal. Membuatneural networkyang dapat menghasilkan kalimat yang dapat di mengerti oleh manusia masih merupakan salah satu topik riset AI. Namun untuk kata yang dihasilkan, sebagian besar merupakan kata alami yang biasa digunakan pada lirik lagu. Struktur teks yang dihasilkan juga terdapat kemiripan dengan teks lirik lagu pada umumnya.
Hasil lainnya dapat dilihat di:
Untuk membuat model yang dapat mengarang teks lirik lagu Indonesia, tentunya kita memerlukandatasetyang berisikan kumpulan lirik lagu Indonesia dari berbagai macam judul dan artis. Sayangyadatasetsemacam itu belum ada, jadi harus didapatkan dengan cara lain. Cara mudah untuk membuatdatasettersebut adalah dengan melakukancrawlingdi situs web penyedia lirik lagu Indonesia. Dari beberapa situs yang ada, saya memilihliriklagu.co.idkarenalayoutnya mudah untuk dicrawl.
Dari prosescrawlingberhasil didapatkan:
Setelah itu dokumen-dokumen hasil crawling disatukan menjadi satu file .txt besar untuk mempermudah akses. Dataset nya dapat di download disini:
Character-level RNN atau Char-RNN merupakan RNN yang dilatih untuk memprediksi karakter selanjutnya dari karakter yang diinputkan. Walaupun terdapat “RNN” di namanya, yang sebenarnya digunakan adalah Long Short-Term Memory (LSTM) yang merupakan pengembangan dari RNN. Secara garis besar Char-RNN mirip dengan klasifikasi menggunakan neural network biasa, namun karena RNN mempunyaimemory/ state, prediksi akan dipengaruhi oleh karakter-karakter yang sudah diprediksi sebelumnya. Hal ini menyebabkan Char-RNN dapat belajar struktur kata dan struktur teks lirik lagu.
Secara garis besar modelnya seperti ini:
Input berupa kumpulan indeks karakter. Embedding akan menghasilkan fitur yang lebih baik dibandingkan dengan inputone-hot-vectordari indeks karakter. Karena tujuan dari Char-RNN merupakan klasifikasi biasa, maka digunakanCross Entropy Loss.
Untuk menghasilkan lirik lagu, char-rnn akan digunakan untuk memprediksi karakter yang akan muncul berikutnya dengan memasukkan kata awal. Prediksi dari karakter terakhir pada kata awal akan digunakan untuk menghasilkan karakter berikutnya. Pemilihan karakter berikutnya didapatkan menggunakansamplingsehingga karakter yang di pilih bukan selalu karakter yang mempunyai kemungkinan yang paling tinggi. Karakter yang dipilih kemudian akan dimasukkan kembali sebagaiinput. Proses memasukkan karakter hasil sebagaiinputdiluang sampai jumlah karakter yang diinginkan dicapai.
Untuksource codelengkapnya nya dapat dilihat di github:
Trainingdilakukan menggunakan GPU Nvidia GTX 1060. Prosestrainingdilakukan denganADAM optimizerdenganlearning rateawal 1e-4 selama 5epoch. Total waktu training kurang lebih 28 jam, kurang lebih 6 jam untuk setiap epoch nya.
Char-RNN berhasil mempelajari struktur kata dan teks lirik lagu Indonesia. Hal ini dapat dilihat dari kata dan struktur teks yang dihasilkan, contohnya adanya kalimat back to reff:, (korus), back to # yang biasa muncul pada teks lirik pada umumnya.
AI Research Engineer. ML enthusiast.",https://miro.medium.com/v2/resize:fit:1200/1*IP3wTjI6GjOTdtCvK_cpzA.png,"Bahasa Indonesia, Lstm, Machine Learning"
https://medium.com/s/story/alasan-mengapa-r-jauh-lebih-baik-dari-python-1922194707d1,Alasan Mengapa R Jauh Lebih Baik dari Python Untuk Pemula,Hadi Suryo,2018-03-22T02:19:44.845Z,"Math, Code, and then Math
Saya mengenal bahasa pemrograman R sejak saya kuliah semester 5, sekitar 3 atau 4 tahun yang lalu, pada mata kuliah Aplikasi Pemrograman Matematika di Universitas Pendidikan Indonesia. Pada mata kuliah itu awalnya kita diajarkan menggunakan aplikasi dekstop yang dapat digunakan untuk menyelesaikan masalah matematika, mulai dari MATLAB, SPSS (it’s for statistics, anyway),MAPLE, Geogebra dan yang terakhir R. Sepanjang yang saya ingat saya tidak begitu menikmati mempelajari pemrograman MATLAB dan R karena saya sendiri tidak terbiasa melakukan pemrograman. Walaupun saya cukup baik dalam pemrograman PASCAL, namun saya tidak begitu menikmati mata kuliah pemrograman. Karena menurut saya, saya tidak membutuhkannya ketika akan mengambil konsentrasi palinggaibdi jurusan matematika: aljabar.Well, bagi yang ingin tau apa yang saya kerjakanmungkin bisa melihat sekilas isi skripsi saya:
Tapi, sejak menghadapi dunia nyata (red:dunia dimana lulusan baruplenga-plengomencari kerjaan), saya menyadari bahwa kemampuanprogrammingsangat dicari oleh perusahaan-perusahaan, baik kecil maupun besar, terutama bagi mereka yang ingin menjadi data scientist (the better coder among statistician, and the better statistician among programmer).Mau tidak mau saya harus belajar bahasa pemrograman dan sains data, karena jelas tidak ada satupun perusahaan yang menganggap bahwaaljabar graf ultramatricialatauComplex Kumjian-Pask algebrasbisa meningkatkanROIperusahaan.
Saat itu ada dua bahasa pemrograman yang umum dipelajari untuk menjadi data scientist, yaitu R dan python. Saya sudah mempelajari R sejak kuliah sedangkan python termasuk baru saya pelajari. Keduanya menjanjikan hal yang sama: kemudahan. Namun, setelah saya mempelajari keduanya saya memilih R sebagai bahasa yang harus saya kuasai. Berikut adalah alasan mengapa saya lebih memilih R dibanding python.
Dalam sejarahnya, R didesain sebagai bahasa pemrograman yang digunakan untuk melakukan analisis dan manipulasi data, sedangkan python merupakan bahasa pemrograman yang sifatnya umum (disebut jugageneral-purpose programming language). Oleh karena itu, R memiliki banyakbuilt-in functionyang dapat digunakan untuk melakukan analisis data dibandingkan dengan python. Misalnya untuk mencari mean atau rerata dari sebarisan data, di R kita cukup menggunakan fungsi mean() ketika para pengguna python harus membuat fungsi khusus atau import library numpy dan menggunakan method numpy.mean().
Bahkan untuk sekedar menjumlahkan dua buah vektor akan jauh lebih intuitif di R dibandingkan dengan melakukannya di python.
Jumlah dua buah vektor di R
‘Jumlah’ (?) dua buah vektor di python
Bagi yang ingin tau bagaimana cara menjumlahkan dua buah vektor di python bisa merujuk ke linkberikut. Setidaknya agar bisa menjumlahkan kedua buah vektor dengan mudah kita harus menggunakan numpy.
Tidak hanya untuk analisis data, bahkan untuk melakukan plotting data, R jauh lebih simpel dibandingkan dengan python, cukup dengan menggunakan fungsi `plot()`, masukkan nilai x dan y (jika dua dimensi), maka plot pun jadi:
Sedangkan jika menggunakan python harus memanggil library matplotlib dengan cara menggunakan method plt sebagai berikut:
R menerbitkan jurnalopen-accessyang berisi implementasi teori-teori statistika atau matematika tertentu dalam bahasa pemrograman R. Oleh karenanya, R memiliki package yangup-to-datemengikuti perkembangan ilmu pengetahuan. Tidak heran jika R memiliki lebih dari 12.336 packages (dan mungkin terus tumbuh) yang siap digunakan untuk melakukan analisis yang anda inginkan. Mulai dari packages yang berkaitan dengan statistika sampai machine learning.BahkanTensorFlowdanKerasyang dibanggakan para pengguna python juga ada di R.
“Tapi kan python lebih general, python jauh lebih hebat dong dari R!”.Wait:
Yes!Bahkan tidak perlu menggunakan atau mempelajari HTML, CSS dan javascript! Dengan menggunakan package Shiny, kita bisa membuat web hanya dengan menggunakan bahasa pemrograman R, baik untuklayoutsampaibusiness logic.Anda bisa melihat contoh aplikasi web yang dibuat dengan shiny pada situsberikut.
“Tapi kan Shiny terbatas tampilannya cuman gitu-gitu doang, sedangkan kalau ngoding pakai python kita bisa bikin tampilan lebih dinamis…”
Let me introduce youOpenCPUandPlumber. Dengan menggunakan OpenCPU kita bisa membangun aplikasi web dengan memanfaatkan kemampuan R, sedangkan dengan menggunakanPlumber kita bisa membuat REST API.
Dengan bantuan ROS (Robotic Operating Systems), R dapat digunakan untuk melakukanpemrograman robotik.
Bahkan dengan R kita bisa membuatlabirin untuk game Minecraft
R memang memiliki kelebihan yang menurut saya jauh lebih banyak dibandingkan python dalam hal kemudahan penggunaan, terutama dalam analisis data. Walaupun begitu R juga memiliki beberapa kekurangan yang perlu dipertimbangkan seperti performance atau kecepatan dalam mengeksekusi program yang masih kalah dibandingkan python. Walau begitu, dengan mempertimbangkan kemudahan dan fleksibilitas, R tetap yang terbaik.
Math, Code, and then Math
A statistical consultant at StatsMaster and Data Scientist BI at Bukalapak. Meet him athadimaster65555.github.io",https://miro.medium.com/v2/resize:fit:1200/1*56Tgh_4MeTJEVX55d-DC8A.jpeg,"R, Data Visualization, Statistics, Data Analysis, Data Science"
https://medium.com/s/story/belajar-data-science-langkah-awal-mengenal-r-dan-rstudio-198ec2246f78,Belajar Data Science : Langkah Awal Mengenal R dan RStudio,Amanda Pratama Putra,2018-05-25T06:45:12.021Z,"R adalah bahasa pemrograman dan sistem perangkat lunak yang dirancang khusus untuk mengerjakan segala hal terkait komputasi statistik. Bahasa pemrograman ini pertama kali dikembangkan pada tahun 1993 oleh dua orang pakar statistik yaitu Ross Ihaka dan Robert Gentleman di Auckland University, New Zealand. Sampai saat ini, bahasa pemrograman R terus berkembang secara pesat seiring dengan semakin populernya terminologi “Big Data” dan meningkatnya kebutuhan perusahaan akandata scientistuntuk mengolah dan menganalisis data di perusahaan tersebut sebagai dasar pengambilan kebijakan dan mengautomatisasi proses bisnis menjadidata-driven. Bahasa pemrograman seperti Python dan R telah menjadi pilihan utama bagi para peneliti maupun praktisi di bidangdata scienceuntuk mengolah dan menganalisis data baik itu untuk kepentingan penelitian maupun bisnis. Oleh karena itu, bagi seorang pemula di bidangdata science, R merupakan bahasa pemrograman yang sangatrecommendeduntuk dikuasai.
Berdasarkan publikasi yang dirilis olehIEEE Spectrum ranks languagespada tahun 2017, R termasuk kedalam 10 bahasa pemrograman terpopuler. R berada pada posisi ke-6 diatas bahasa pemrograman Javascript dan PHP yang berturut-turut berada pada posisi ke-7 dan ke-8. Sedangkan peringkat pertama dipegang oleh bahasa pemrograman Python, yang juga sangat diminati olehdata scientist.Salah satu pertimbangan mengapa R bisa masuk kedalam deretan bahasa pemrograman terpopuler adalah semakin banyak yang menggunakan R pada paper-paper penelitian akademik. Jadi, R terbukti sangat populer digunakan oleh para peneliti di bidang statistik maupundata science. Selain itu, R juga dikembangkan secaraopen sourcesehingga dapat dipergunakan secara legal tanpa perlu membayar lisensi apapun, aliasgratis!Yang tak kalah penting adalah R telah memiliki komunitas pengguna yang luas, sehingga permasalahan apapun yang akan kita temui selama proses belajar, hampir bisa dipastikan jawabannya sudah tersedia di websitestackoverflow.
Setelah sedikit mengenal bahasa pemrograman R, langkah berikutnya adalah menginstallsystem environmentR dan RStudio sebagai IDE untuk mempermudah kita saat koding nanti. Berikut adalah langkah-langkah instalasi R:
Selanjutnya, kita perlu menginstallRStudiosebagai IDE. RStudio dirancang khusus untuk memudahkan kita dalam proses koding danproject managementnanti. Oleh karena itu, saya sangat menyarankan untuk menginstall aplikasi ini sebelum memulai memelajari penggunaan R itu sendiri. Sejumlah fitur yang dimiliki RStudio antara laincode completion, syntax highlighting,smart indentation, dsb.
Adapun tahapan untuk menginstall RStudio adalah sebagai berikut:
Yap, akhirnyasystem environmentkita telah siap digunakan untuk mempelajaridata sciencemenggunakan bahasa pemrograman R dan aplikasi RStudio. Pada tulisan selanjutnya, kita akan mengupas tuntas tentang layout aplikasi RStudio serta tips dan trik menggunakan RStudio yang efektif. Sekian dulu, semoga bermanfaat.
Oiya, jika ada pertanyaan seputar materi diatas, jangan ragu untuk meninggalkan pertanyaan di kolom komentar ya…
Learning never exhausts the mind. ~ Leonardo Da Vinci
Selanjutnya, pengenalan tentang layout dari aplikasi RStudio dapat diakses melalui link berikut:
Data Scientist | Artificial Intelligence | E-commerce | Consultant",https://miro.medium.com/v2/resize:fit:256/0*x4wGgAGjdkaAx-HN.,"Rstudio, Beginner, Belajar R, Data Science, R Programming"
https://medium.com/s/story/memisah-dataset-menjadi-training-set-dan-test-set-19b45dd52f6d,Memisah Dataset Menjadi Training-Set dan Test-Set,R. Kukuh,2018-02-15T16:48:27.970Z,"Dalam membuat model Machine Learning, data harus dibagi menjadi Training-Set dan Test-Set.
Mengapa ini harus kita lakukan?
Well, machine learning adalah tentang machine yang sedang learning sesuatu. Maka dalam model machine learning tersebut harus ada goal (biasanya prediction) yang harus dicapai.
Nah, untuk mencapai goal tersebut, machine harus diberi tahu mana “set data” yang harus dicapai / dilampaui, dan mana “set data” yang bisa digunakan untuk mencapai / melampaui goal ini tadi.
“Set data” untuk dicapai inilah Test-Set, sedangkan “set data” untuk mencapainya disebut Training-Set.
Training-Set ini nantinya akan digunakan untuk membuat model machine learning, sedangkan Test-Set ini akan digunakan untuk menguji performa dan kebenaran (terhadap korelasi) dalam model yang bersangkutan.
Cukup dengan teorinya, mari kita membuatnya.
Buka Anaconda Navigator, lalu buka Spyder
Lanjutkan project sebelumnya tentangData Preprocessing 04
Tambahkan kode berikut:
Jika bingung dengan kegunaan argumen yang ada, tekan ⌘ + i untuk melihat dokumentasinya.
Eksekusi kode diatas
Buka variabel X_train, X_test, Y_train, dan Y_test untuk melihat dengan lebih jelas
Lanjutkan project sebelumnya tentangData Preprocessing 04
Pertama, kita butuh dan install library baru: caTools
Hasil eksekusi kode diatas adalah seperti ini
Cari dan reload window Packages, dan kita akan mendapati library caTools ini ada disana
Setelah library yang dibutuhkan terinstal, berikutnya adalah mengaktifkannya. Ada 2 cara yang bisa kita gunakan: Centang pada window Packages, atau dengan kode berikut:
Jangan lupa untuk men-disable (atau sekalian hapus) kode untuk install library karena sudah tidak dibutuhkan lagi.
Waktunya untuk memilah (splitting) dataset menjadi training set dan test set
Jika bingung dengan kegunaan argumen yang ada, tekan F1 untuk melihat dokumentasinya.
Eksekusi kode berikut, dan hasil berikut akan kita dapat
Periksa objek training_set dan test_set di atas untuk melihat hasil dari pembuatan training set dan test set menggunakan R ini
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*7VtBbfs5U6dV18G5LT8P8w.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/sistem-runtuh-apakah-anda-ingin-memulai-lagi-19e11ff5ac42,Sistem Runtuh. Apakah Anda Ingin Memulai Lagi?,Haris W,2018-08-31T04:03:24.267Z,"Collected Essays and Reviews on Books, Films, Musics, and Cultures
*Catatan ini pernah disertakan dalam sebuah diskusi berjudul“Google Glass Failure and The Horizon of Sight”pada 10 Feb 2017.
Di tahun 2010, sebelum google meluncurkan video iklan Google Glass, Keiichi Matsuda memproduksi film pendek“HYPER-REALITY”. Film tersebut menceritakan — lewat sudut pandang pertama — seorang pekerja biasa Juliana Restrepo yang menjalani rutin kesehariannya menggunakan kendaraan publik sehabis bekerja, membeli kebutuhan rumah tangga dan peliharaan maya-nya. Pemandangan Juliana Restrepo berbeda dengan pemandangan yang ditangkap oleh mata telanjang kita. Di mata Juliana, terdapat layar yang menampilkan pemberitahuan media sosial, garis penunjuk di jalanan yang ingin dia tuju,browserdan asisten pribadi yang merupakan mesin kecerdasan buatan yang mengerti perintah-perintah operasional layar tersebut. Pemandangan itu pun ramai dengan warna-warni iklan dan benda-benda maya yang menindih pemandangan tanpa perangkat tersebut. Mata Juliana bisa kita bayangkan dengan perumpamaan sederhana, layarsmartphonesudah bertindihan dengan retina mata.
Di film pendek tersebut, penonton tidak mengetahui bentuk perangkat yang Juliana kenakan namun dapat mengetahui bahwa pemandangan tersebut adalah buatan lewat adegan penghujung film dimana sistem perangkat mengalami kegagalan sehingga kenyataan seketika runtuh dan melemparkan Juliana kembali pada pemandangan mata badaniahnya.
Upaya Google untuk meluncurkan Google Glass ke publik adalah sesuatu yang bisa jadi kita tunggu-tunggu dimana informasi berada begitu dekat sampai di pelupuk mata. Namun upaya tersebut bukanlah tanpa masalah. Walaupun dekat dengan mata, kendali pada operasi dasar masihlah menggunakan tangan (sentuh, geser, dan ketuk) dan suara (perintah-perintah biner dan input teks). Di sana mata masih menjadi organ yang tidak bisa dikendalikan penuh seperti pada tangan yang mampu membentuk benda-benda material.
Pengembangan awal yang melibatkan percobaan publik menemukan bermacam kritik seperti resiko kesehatan, perangkat yang ringkih, harga yang cukup mahal, dan mengenai privasi. Apakah Google Glass dan perangkat serupa akan menjadi sesuatu yang umum atau hanya mimpi belaka, penulis memberi perhatian pada kemampuan perangkat tersebut mengubah pemandangan dunia sekitar dan kemungkinan kegagalannya olehbug, fraud,danerror. Terkhusus yang kedua, penulis mencoba menjelajah konsep pemandangan dan penglihatan lewat Juliana Restrepo dan beberapa cerita yang mungkin relevan.
Dunia Juliana Restrepo merupakan pemandangan tanpa cacat dimana dinding kusam pertokoan kotanya dibalut oleh permukaan yang dibangkitkan oleh perangkat. Orang-orang di sekitarnya terlihat sepertiNPC (Non Playable Character)dalam permainan The Sims yang bisa diajak berbicara dan bila sudah berteman, dapat kita lihat statusnya. Juliana di dunianya adalah satu akun dengan level 99 dan cemas akan poin dan bonus yang dimilikinya.
Di sepanjang film pendek Keiichi Matsuda, kita menyaksikan Juliana berinteraksi dengan layar-layar virtual, melakukan panggilan ke operator sistem, bermain game, dan bertanya pada mesin pencari, “siapa aku?”. Juliana bertanya pada perangkat apakah Ia bisa melakukanrestartdan diberitahukan bahwa Ia akan kehilangan poin dan bonus yang diperolehnya selama ini. Artinya, dunia dimulai dari awal. Pengembalian kondisi tersebut bukanlah opsi yang sepele bagi Juliana. Ketika mengkonfirmasi, Ia menjawab “tidak”. Kemudian Juliana berangkat ke toko swalayan untuk membeli beberapa kebutuhan, dan di muka toko seekor anjing maya hadir. Dijelaskan di sebuah wawancara, anjing Juliana juga memiliki kebutuhan-kebutuhan dasar seperti makan, minum, bermain, mandi, dan berak. Kebutuhan primer si anjing dapat diperoleh ketika Juliana membeli berbagai hal yang telah terhubung dengan aplikasi penyedia piaraan tersebut. Di sepanjang jalan layar Juliana terus menampilkan iklan seperti kursus kebugaran, produk kecantikan, dan produk penyedia kebutuhan si anjing.
Pemandangan maya Juliana kemudian mengalami masalah dan Ia menghubungi operator yang merupakanArtificial Intelligence. Operator di awal berkata bahwa sistem sedang baik-baik saja dan poin-poin Juliana aman bersama mereka. Beberapa kali pemandangan mengalami kebisingan dan penampilan operator berubah-ubah. Kecemasan Juliana bertambah ketika operator salah menyapa Juliana sebagai Emilio. Kebisingan makin mengganggu dan sebentar sistem nampak baik-baik saja. Juliana makin ragu dan operator masih meyakinkan bahwa akunnya baik-baik saja namun Ia mesti menjalani konfirmasi identitas lewat tes biometrik yang bisa dilakukan di cabang operasional perangkat terdekat.
Di akhir film, terdapat adegan Juliana keluar dari swalayan dan mendapatkan dirinya ‘dilukai’ secara maya. Saat itu juga Ia panik, napasnya tersengal. Sistem pun mati sehingga semua poin dan bonus yang telah Ia kumpulkan lenyap dan kota nampak kusam tanpa lapisan-lapisan grafis yang memukau. Juliana menyeberang jalan menuju sebuah patung dan menghidupkan kembali sistem perangkatnya dari awal.
Kegagalan sistem untuk menyokong simulasi yang dijalankan oleh perangkat ini adalah pengalaman yang sudah pernah manusia alami dan memang dapat ditemukan di kehidupan sehari-hari. Kegagalan sistem juliana restrepo terbilang mirip denganblue screenpada komputer kita, telepon pintar yang layarnya mati, atau perangkat penyimpanan yang terkena serangan virus. Namun kasus kita dan Juliana terbilang berbeda sebab pemandangan terikutkan di dalamnya. Kegagalan pemandangan pun dapat kita temukan dalam kasus seorang hansip bernama Budi yang sedang berjaga malam di kampung. Dalam patrolinya, dari pos ronda sampai lapangan bola, dalam keadaan setengah sadar itu, Ia terperanjat ketika sekilas mengarahkan pandang ke arah semak-semak dan pohon pisang. Kita sebagai pebaca mengetahui sosok putih (hantu) disana namun hansip ingin meyakinkan dirinya dengan mengusam matanya berkali-kali sampai pemandangannya begitu jelas dan Ia pun berteriak dan lari terbirit-birit
Dalam puisi epik Odiseus karangan Homer, diceritakan Odiesus sang raja Ithaca mengalami kekalahan di perang troya dan melakukan perjalanan kembali ke Ithaca. Odiseus dan dua belas kapalnya terhanyut ke sebuah pulau penuh teratai sehabis diterpa badai. Mereka berjumpa Lotopagus (pemakan teratai) yang memberikan buah, yang membuat pemakannya lupa akan perjalanan pulang mereka, kepada awak kapal Odiseus. Namun Odiseus dengah susah payah menarik mereka kembali ke kapalkapal mereka. Kemudian mereka memasuki sebuah gua di bawah perut seekor domba dan berjumpa Polyphemus, raksasa (biasa makan manusia) peranakan Poseidon dan Thoosa, yang hidup bersama raksasa sebangsanya dalam sebuah komuni.
Odiseus dan awaknya memasuki gua dan ketika itu Polyphemus baru saja pulang ke gua, Ia menutup pintu gua dengan batu besar dan tak lama kemudian memakan dua awak Odiseus. Esok paginya, Ia memakan dua lagi dan meninggalkan gua untuk merawat dombanya. Setelah kembali, sore harinya Ia memakan dua lagi. Odiseus tidak bisa tinggal diam. Lalu saat menjelang malam hari, Ia menawarkan anggur keras yang diperolehnya di perjalanan. Dalam keadaan mabuk, Polyphemus kemudian bertanya siapa nama Odiseus sebenarnya dan berjanji Ia akan memberi hadiah (yaitu dimakan seperti awak kapalnya) bila Odiseus sudi memberitahu namanya. Dengan cerdik, Odiseus pun menjawab, “Outis.” yang berarti “nobody”. Dengan begitu, janji tersebut harus dipenuhi. Namun naas bagi Polymephus, Ia harus memakan“nobody”. Dengan itu, seperti mengalami gagal mesin, Ia jatuh tertidur dalam mabuknya. Di kesempatan itu, Odiseus memanaskan sebatang kayu lantas menusuk mata Polyphemus. Polyphemus berteriak-teriak pada rekan-rekannya bahwanobodytelah menyakitinya namun raksasa-raksasa tersebut tidak bisa mengerti ucapannya yang tak masuk akal, lagi pula mereka sedang ngelantur karena mabuk.
Di pagi harinya, Polyphemus meninggalkan gua untuk memeriksa dombanya dan memastikan Odiseus tidak pergi kemana-mana. Odiseus dan awaknya saat itu bersembunyi di bawah domba dan Odiseus berhasi kabur setelah Polyphemus membiarkan dombanya pergi. Dalam pelariannya itu, Odiseus membuka identitasnya dan Polyphemus meminta pembalasan dendam lewat poseidon.
Janji Polymephus sebagai algoritma mengalamierrordengan input“nobody”berakibat pada runtuhnya bahasa yang digunakan sebagai alat komunikasi. Ketika itu terjadi dalam kenyataan, terjadi paralisis pada tubuh yang sepanjang hidupnya memberi nama pada apa-apa. Paralisis itu terjadi pula pada si hansip dan Juliana Restrepo yang berkali-kali mencoba mengkonfirmasi kenyataan yang sedang runtuh. Di kasus hansip, Ia mengusap mata berkali-kali, dan Juliana memastikan semua baik-baik saja kepada operator sistem. Napas Juliana yang tersengal dapat dianggap mirip dengan Polyphemus yang terjatuh dan meracau akan memakannobody. Namun kasus mereka berbeda dalam arah keterlemparan pemandangan setelah keruntuhan. Polyphemus terjerembab pada lubang dalam sistem bahasa yang tak mampu membangkitkannobodysebagai sesuatu dalam pemandangan yang tersedia oleh mata. Sedangkan kegagalan perangkat augmentasi mengembalikan Juliana kepada pemandangan mata.
Penulis mencoba berandai bahwa pemandangan adalah sesuatu yang cacat sejak awal, dibutuhkan bahasa yang kompleks untuk membangkitkan hal-hal yang tak tercerap organ-organ inderawi. Untuk kembali ke posisi normal: pemandangan dapat dianggap sebagai sesuatu yang utuh, Polyphemus mesti merasionalisasinobodysebagai sebuah tipuan dan tidak benar-benar ada namun itu terjadi belakangan. Juliana Restrepo sebaliknya dapat melakukan hal tersebut hanya dengan menganggap bahwa suatu waktu segala sesuatu akan kembali membaik seperti sedia kala. Dapat dibayangkan, proses terlempar dan pengembalian kepada kondisi normal bisa-bisa merupakan perulangan yang terus terjadi.
Pemandangan yang kita cerap sehari-hari tanpa bantuan perangkat adalah tempat kita terlempar setelah kastrasi, pemotongan kita sebagai janin dari ari-ari. Sloterdijk dalam bukunya “Spheres Vol. 1: Bubbles” pada babRequiem for Discarded Organmenunjukkan bahwa pada awalnya, janin di rahim adalah entitas berbentuk bola gelap yang ditemani oleh ari-ari. Pemotongan janin dari ari-arinya adalah sebuah keharusan untuk memasuki dunia, kebudayaan, peradaban manusia. Di beberapa masyarakat, ari-ari dipandang sebagai pasangan kembar. Setelah pemotongan, ari-ari harus dikuburkan. Kehilangan yang dialami si janin, dapat disandingkan dengan kehilangan Orpheus atas Euridice yang menjadi kekosongan yang tidak mungkin lagi di kemudian hari, menjadi penyokong kenyataan dan nyanyian-nyanyian Orpheus setelahnya.
Euridice sedang berjalan-jalan di padang ilalang, yang mana Ia bertemu Satiros, satu bangsa penjahat kelamin dalam mitologi yunani. Euridise berhasil kabur dari Satir namun terjerembab ke dalam sarang ular dan ia dipatuk di bagian kaki. Euridise mati dan beberapa waktu kemudian Orpheus menemukannya. Orpehus meratapi kematiannya, mengambil lira dan memainkan lagu-lagu sehingga dewa-dewa dan para nymph ikut bersedih. Orpheus kemudian berkeinginan mengembalikan Euridise ke dunia-atas. Hades dan Persephone memperbolehkannya memasuki Aornum dengan jaminan bahwa dalam perjalannya kembali nanti, Orpheus berjalan di depan Euridice tanpa boleh melihat ke belakang. Ia pun beranjak ke dunia-atas namun di bawah kecemasan, ketika mencapai permukaan, Ia menatap balik dan kini Euridise lenyap untuk selamanya.
Pemotongan ari-ari sebelum janin boleh bergabung ke dalam dunia manusia adalah menatap balik Euridise. Janin terlahir dan pertama kali melihat, dipandang mampu berjalan di atas dua kaki dan tumbuh dewasa. Sloterdijk kemudian mengutip alkitab dimana Job bertanya pada tuhan dalam sebuah tuduhan:
Did you not pour me out like milk and curdle me like cheese? […] Why then did you bring me out of the womb? I wish I had died before any eye saw me— (Job 10:10 & 18)
Lalu Sloterdijk mengutip baris-baris Rainer Maria Rilke:
Be dead in Eurydice, always — climb, with more song,climb with more praise, back up into pure relation.— (Sonnets to Orpheus, bagian 2, no. 13)
Kehilangan si janin dan Orpheus disini adalah landasan yang menyokong kenyataan mereka selanjutnya. Siapa ari-ari ini? Sloterdijk, masih, dalam Requiem for Discarded Organ menjelaskan dalam cerita seorang pasien dan analis. Pasien datang ke analis katakanlah oleh bermacam persoalan hubungan, gejala somatis, dan gangguan seksual, Ia disilakan terlentang di sofa dan sesi berakhir dan Ia tak mengatakan apa-apa. Beberapa sesi dijalani bulan itu dengan serupa dan ia pun berkata “Kita belum kemana-mana. Tetapi nampaknya membaik.” Ia tak berkata-kata lagi. Dan beberapa bulan kemudian, masih dalam bungkamnya, Ia berdiri di akhir sesi dan mengatakan bahwa Ia merasa baik, mengira Ia telah pulih kemudian berterimakasih pada analis dan pulang. Perjalanan hening si pasien dan analis ini Sloterdijk sebut sebagai pertemanan yangmonadic, yaitu keduanya melakukan perjalanan dimana si analis diandaikan sebagai seorang psikoanalis tua (atau pertapa) yang menyokong si pasien. Si analis tahu bagaimana ia hadir tanpa mengintervensi keberadaan orang lain kecuali diperbolehkan, hanya melalui keberadaannya yang terpisah sekaligus menyimak.
Dari penjelajahan di atas mengenai pemandangan dan kenyataan, penulis mendapatkan kesan bahwa pemandangan dengan mata tanpa perangkat — yang dialami Juliana Restrepo, si hansip, Polyphemus, Orpheus, dan Janin — dibangun di atas kastrasi yang suatu waktu dapat runtuh. Keruntuhan pemandangan normal yang biasa-biasa saja bahkan tidak sepenuhnya disadari itu selalu menjadi momen evaluasi dalam keberadaan mereka. Namun untuk kasus Juliana, penulis masih membutuhkan materi lebih banyak untuk menyelidiki lebih lanjut fenomena pemandangan yang dibangkitkan oleh perangkat.
Collected Essays and Reviews on Books, Films, Musics, and Cultures",https://miro.medium.com/v2/resize:fit:1200/1*AZULryxRG9Tv7z5Qpm36sA.jpeg,"Technology, AI, Google, Hyperreality, Google Glass"
https://medium.com/s/story/3-4-masalah-etika-revolusi-mesin-pintar-1a395e8dd5f6,(3/4) Masalah Etika Revolusi Mesin Pintar,Machine Learning Indonesia (ML ID),2018-05-02T07:11:55.539Z,"Machine Learning Indonesia
Ada kesepakatan yang telah diterima di kalangan peneliti AI modern bahwa mesin pintar masih sangat jauh dari kecerdasan manusia, meskipun dalam beberapa kasus dalam domain ANI telah terbukti melebihi manusia. Walaupun demikian, banyak hal yang masih terlewatkan dalam AI modern. IstilahArtificial Intelligence General(selanjutnya, AGI) adalah istilah yang digunakan untuk menunjukkan mesin pintar yang nyata dengan kecerdasan seperti manusia. Sesuai namanya, konsensus yang muncul adalah karakteristik umum. Algoritma pembelajaran mesin yang dimiliki AGI setara dengan kinerja manusia atau bahkan lebih unggul jika sengaja diprogram untuk domain tak terbatas. AlphaGo menjadi juara dunia di catur, tetapi bahkan tidak bisa benar-benar bermain catur, apalagi mengendarai mobil atau membuat penemuan ilmiah.
Algoritma mesin pintar modern saat ini telah menyerupai semua konsep biologis kecuali Homo sapien. Lebah kompeten membangun sarangnya; berang-berang dapat membangun bendungan; tapi lebah tidak bisa membangun bendungan dan berang-berang tidak bisa belajar untuk membangun sarang lebah. Seorang manusia — Homo sapien — dapat belajar untuk melakukan keduanya; tapi ini adalah kemampuan unik di antara bentuk-bentuk kehidupan biologis. Hingga saat ini masih diperdebatkan apakah kecerdasan manusia benar-benar umum ataukah hanya lebih baik di beberapa tugas kognitif tertentu. Yang pasti kecerdasan manusia secara signifikan lebih umum dari kecerdasan nonhominid. Dari sini relatif mudah untuk membayangkan jenis masalah yang mungkin timbul dari AI yang beroperasi hanya dalam satu domain tertentu. Ini adalah kelas kualitatif berbeda dari masalah menangani AGI yang mampu beroperasi di banyak konteks atau bahkan yang tidak dapat diprediksi sebelumnya.
Sebagai contoh — AlphaGo, algoritma mesin pintar dari Google yang mengalahkan juara catur dunia. Jika algoritma AlphaGo hanya dapat melakukan persis seperti yang diperintahkan atau diprogram oleh insinyur perangkat lunak, maka proses pra-proses untuk melihat pola-pola permainan dari data permainan sebelumnya akan sangat banyak. Pertama karena kemungkinan gerakan catur sangat besar jumlahya. Kedua, jika insinyur mengetahui gerakan apa yang lebih baik dan harus membuat algoritmanya, maka mesin pintar yang dibuatnya tidak akan bisa mengalahkan juara dunia, karena insinyur tersebut bukan juara dunia. Algoritma yang dibuat tentu saja harus bisa belajar secara mandiri, mengamati pola-pola gerakan dan menghitung semua kemungkinan untuk memenangkan permainan.
Manusia modern dapat melakukan banyak hal secara mandiri. Otak manusia mampu beradaptasi dan berkembang cepat tanpa pengalaman terdahulu. Manusia melintasi ruang dan pergi di bulan, meskipun tak satu pun dari nenek moyang manusia mengalami tantangan dengan ruang vakum. Dibandingkan dengan ANI, itu adalah masalah kualitatif berbeda untuk merancang sebuah mesin pintar yang mampu beroperasi dengan aman di ribuan konteks; termasuk konteks yang tidak secara khusus dibayangkan oleh desainer atau penggunanya; termasuk konteks yang belum ditemui manusia sebelumnya.
Untuk membangun AI yang aman dan beroperasi dalam banyak domain, banyak konsekuensi termasuk yang mungkin tidak pernah secara eksplisit dibayangkan oleh insinyur. Seseorang harus menentukan perilaku yang baik dalam hal seperti “X sehingga konsekuensi dari X tidak berbahaya bagi manusia”. Hal ini melibatkan ekstrapolasi konsekuensi dari suatu tindakan yang merupakan satu spesifikasi yang dapat di realisasikan jika sistem mampu secara eksplisit menentukan konsekuensi tindakannya. Sebuah pemanggang roti misalnya, tidak dapat memiliki properti desain ini karena pemanggang roti tidak dapat meramalkan konsekuensi dari memanggang roti. Bayangkan seorang insinyur mengatakan, “ya saya tidak tahu apakah pesawat ini akan selalu terbang dengan aman. Saya tidak tahu mekanik pesawat ini secara detil, tetapi saya yakin desainnya sangat aman”. Pernyataan tersebut seperti tidak nyaman dari sisi penumpang, tetapi memang sulit untuk melihat semua kemungkinan dari konsekuensi yang jauh. Memerika desain kognitif mungkin dilakukan tetapi sangat sulit memprediksi semua konsekuensi dari suatu tindakan. AI akan benar-benar aman jika memiliki verifikasi yang aman dengan jaminan yang dapat dipercaya. Dalam banyak riset AI — harapan ini adalah murni harapan dan tetap menjadi permasalahan besar.
Membangun AGI dipercaya memerlukan metode yang sangat berbeda dari cara berpikir saat ini. Disiplin etika AI, khususnya pada AGI akan sangat berbeda secara fundamental dari disiplin etika teknologi non-kognitif, karena:
Beberapa masalah etika juga akan muncul ketika kita merenungkan kemungkinan bahwa beberapa sistem mesin pintar masa depan akan memiliki status moral. Hubungan kita dengan sesuatu memiliki status moral yang tidak eksklusif soal rasionalitas instrumental: kita juga memiliki alasan moral untuk memperlakukan mereka dengan cara tertentu dan untuk menahan diri dari memperlakukan mereka dengan cara-cara tertentu lainnya. Francis Kamm telah mengusulkan definisi status moral berikut:
X memiliki status moral = karena X memiliki moral dalam dirinya sendiri; maka X diperbolehkan / tidak diperbolehkan untuk melakukan hal-hal atas kepentingan sendiri.
Sebuah batu tidak bisa memiliki status moral: kita mungkin menghancurkannya, atau terkena efek yang merusak batu itu sendiri. Seorang manusia di sisi lain, harus diperlakukan tidak hanya sebagai alat tetapi juga dengan tujuan tertentu karena manusia memikirkan dalam dirinya sendiri bahwa itu tidak diperbolehkan untuk melakukan padanya hal-hal tanpa persetujuannya. Ringkasnya, manusia memiliki status moral.
Pertanyaan tentang status moral merupakan ranah etika praktis. Misalnya perselisihan tentang kebolehan moral aborsi sering bergantung pada perbedaan pendapat tentang status moral embrio. Kontroversi tentang eksperimen pada hewan dan penganganan hewan di industri makanan melibatkan pertanyaan tentang status moral dari spesies yang berbeda dari hewan, dan kewajiban kita terhadap manusia dengan penyakit berat, seperti pasien stadium akhir Alzheimer, mungkin juga tergantung pada pertanyaan moral status.
Secara luas disepakati bahwa sistem mesin pintar saat ini tidak memiliki status moral. Insinyur dapat mengubah, menyalin, menghentikan, menghapus, atau menggunakan program komputer sesuai desain; setidaknya sejauh program yang dibuat. Kendala moral untuk yang manusia dalam hubungan dengan sistem AI kontemporer semua didasarkan pada tanggung jawab untuk makhluk lain, seperti sesama manusia, tidak dalam tugas untuk sistem itu sendiri.
Meskipun cukup konsensus bahwa kini sistem AI kekurangan status moral, masih belum jelas kriteria dan atribut untuk status moral tersebut. Dua kriteria penting yang umumnya diusulkan terkait dengan status moral, baik secara terpisah atau dalam kombinasi: kesanggupan dan cita rasa (atau kepribadian). Ini dapat dicirikan kira-kira sebagai berikut:
Salah satu pandangan umum adalah bahwa banyak hewan memiliki qualia dan karena itu memiliki beberapa status moral, tapi hanya manusia memiliki cita rasa, yang memberi mereka status moral lebih tinggi dari hewan non-manusia. Pandangan ini tentu saja harus menghadapi kasus spesifik seperti, di satu sisi bayi manusia atau manusia dengan keterbatasan yang disebut sebagai “manusia marginal” bisa gagal memenuhi kriteria untuk cita rasa; dan di sisi lain, beberapa binatang seperti kera besar mungkin memiliki setidaknya beberapa elemen dari cita rasa. Ada yang menyangkal bahwa yang disebut “manusia marginal” memiliki status moral penuh. Yang lainnya mengusulkan cara-cara tambahan di mana sebuah objek bisa memenuhi syarat sebagai pembawa status moral, seperti dengan menjadi anggota dari jenis yang biasanya memiliki kesanggupan atau cita rasa, atau dengan berdiri dalam hubungan yang cocok untuk beberapa makhluk yang secara independen memiliki status moral. Meskipun demikian disini kita akan fokus pada kriteria kesanggupan dan cita rasa.
Sistem mesin pintar akan memiliki beberapa status moral jika memiliki kapasitas untuk qualia, seperti kemampuan untuk merasakan sakit. Sebuah sistem mesin pintar yang hidup, bahkan jika tidak memiliki bahasa dan kemampuan kognitif lainnya yang lebih tinggi, tidak seperti mainan boneka binatang; tetapi lebih seperti binatang hidup. Adalah salah untuk menimbulkan rasa sakit pada tikus, kecuali ada alasan moral untuk melakukannya. Hal yang sama akan berlaku untuk setiap sistem mesin pintar yang hidup. Jika selain kesanggupan, sistem mesin pintar juga memiliki cita rasa dari jenis yang sama dengan manusia dewasa normal, maka seharusnya memiliki status moral penuh setara dengan manusia.
Salah satu ide yang mendasari penilaian moral dapat dinyatakan dalam bentuk yang lebih kuat sebagai prinsip non-diskriminasi:
Prinsip Substrat Non-DiskriminasiJika dua makhluk memiliki fungsi yang sama dan pengalaman sadar yang sama, dan hanya berbeda dalam substrat pelaksanaannya, maka mereka memiliki status moral yang sama.
Banyak perdebatan untuk prinsip ini dengan alasan bahwa menolak itu akan sama dengan rasisme: substrat tidak memiliki perbedaan moral yang mendasar dalam cara dan alasan yang sama seperti warna kulit. Prinsip Substrat Non Diskriminasi tidak berarti bahwa komputer digital bisa sadar atau bahwa hal itu bisa memiliki fungsi yang sama sebagai manusia. Substrat tentu saja relevan sejauh itu secara moral dan memiliki perbedaan untuk kesanggupan atau fungsi. Tapi ini berarti tidak ada bedanya moral dari makhluk yang terbuat dari silikon atau karbon atau otak yang menggunakan semikonduktor.
Prinsip tambahan yang dapat diusulkan adalah bahwa sistem AI adalah buatan (artificial), yaitu produk yang sengaja dibuat — yang tidak relevan dengan status moral mereka. Kita bisa merumuskan ini sebagai berikut:
Prinsip Ontogeny Non-DiskriminasiJika dua makhluk memiliki fungsi yang sama dan pengalaman kesadaran yang sama, dan hanya berbeda dalam bagaimana mereka terbentuk, maka mereka memiliki status moral yang sama.
Saat ini ide tersebut diterima secara luas meskipun di beberapa kalangan khususnya di masa lalu, gagasan bahwa status moral seseorang tergantung pada garis keturunan atau kasta masih berpengaruh. Faktor penyebab seperti keluarga berencana, pertolongan persalinan, fertilisasi in vitro, peningkatan sengaja gizi ibu dan lain-lain yang memperkenalkan unsur pilihan yang disengaja dalam penciptaan manusia, memiliki implikasi yang diperlukan untuk status moral progeni. Bahkan mereka yang menentang kloning reproduksi manusia karena alasan moral atau agama umumnya menerima bahwa bayi manusia kloning akan memiliki status moral sama seperti bayi manusia lainnya. Prinsip Ontogeny Non Diskriminasi memperluas alasan ini sampai ke kasus yang melibatkan sistem kognitif yang seluruhnya buatan.
Tentu saja mungkin untuk mendapatkan kondisi penciptaan yang sedemikian rupa mempengaruhi keturunan berikutnya dan mengubah status moralnya. Sebagai contoh, jika beberapa prosedur dilakukan selama pembuahan atau kehamilan yang menyebabkan janin manusia berkembang tanpa otak, maka fakta tentang ontogeni akan relevan untuk penilaian dari status moral progeni. Anak anencephaly bagaimanapun akan memiliki status moral yang sama dengan anak anencephaly serupa lainnya, termasuk yang terjadi melalui proses alami. Perbedaan status moral antara anak anencephaly dan anak normal didasarkan pada perbedaan kualitatif antara dua fakta bahwa salah satu memiliki pikiran sementara yang lainnya tidak. Karena dua anak tidak memiliki fungsi yang sama dan pengalaman sadar yang sama, Prinsip Ontogeny Non-Diskriminasi tidak berlaku.
Meskipun Prinsip Ontogeny Non-Diskriminasi menegaskan bahwa ontogeni suatu makhluk tidak memiliki landasan penting pada status moral, hal itu tidak menyangkal bahwa fakta-fakta tentang ontogeni dapat mempengaruhi agen moral tertentu pada makhluk yang bersangkutan. Orang tua memiliki tugas khusus untuk anak kandung mereka tetapi tidak pada anak-anak lain. Demikian pula, Prinsip Ontogeny Non-Diskriminasi konsisten dengan klaim bahwa pencipta atau pemilik sistem mesin dengan status moral mungkin memiliki tugas khusus untuk pikiran buatan mereka yang mereka tidak memiliki pikiran buatan lain, bahkan jika pikiran tersebut secara kualitatif serupa dan memiliki status moral yang sama.
Jika prinsip-prinsip non-diskriminasi sehubungan dengan substrat dan ontogeni diterima, maka banyak pertanyaan tentang bagaimana memperlakukan otak buatan dapat dijawab dengan menerapkan prinsip-prinsip moral yang sama untuk menentukan tugas-tugas kita dalam konteks lebih akrab. Sejauh tugas moral berasal dari pertimbangan status moral, kita harus memperlakukan otak buatan hanya dengan cara yang sama seperti memperlakukan pikiran manusia alami secara kualitatif identik dalam situasi yang sama. Ini menyederhanakan masalah pengembangan etika untuk menangani otak buatan. Bahkan jika kita menerima sikap ini, kita harus menghadapi sejumlah pertanyaan etika baru dimana prinsip-prinsip tersebut mungkin tidak terjawab. Pertanyaan etika timbul karena otak buatan dapat memiliki sifat sangat berbeda dari manusia atau hewan yang alami. Kita harus mempertimbangkan bagaimana sifat baru akan mempengaruhi status moral dari otak buatan dan apa artinya untuk menghormati status moral dari pikiran eksotis tersebut.
Dalam kasus manusia, kita biasanya tidak ragu menganggap kesanggupan dan pengalaman sadar untuk setiap individu akan menunjukkan kondisi normal perilaku manusia. Sedikit yang yakin untuk menjadi orang lain dan bertindak normal tanpa memiliki kesadaran. Namun orang lain tidak berperilaku dengan cara yang mirip dengan diri kita sendiri; mereka juga memiliki otak dan arsitektur kognitif sendiri. Mesin pintar sebaliknya mungkin cukup berbeda dari kecerdasan manusia namun masih menunjukkan perilaku seperti manusia atau memiliki kecenderungan perilaku yang sama. Oleh sebab itu perlu untuk memahami kecerdasan buatan yang yang mungkin akan menjadi seperti seseorang, namun tidak akan hidup atau memiliki pengalaman sadar apapun. Apakah ini benar akan tergantung pada jawaban atas beberapa pertanyaan metafisik. Haruskah sistem seperti itu mungkin akan menimbulkan pertanyaan apakah orang yang tidak hidup akan memiliki status moral; dan jika demikian, apakah akan memiliki status moral yang sama sebagai orang hidup? Pertanyaan ini belum mendapat banyak perhatian hingga saat ini.
Properti eksotis lain, salah satu yang tentunya metafisik dan fisik bagi mesin pintar adalah tingkat subjektif yang menyimpang drastis dari tingkat yang merupakan karakteristik dari otak manusia biologis. Konsep tingkat subjektif dijelaskan pertama kali dengan memperkenalkan gagasan pemindahan otak. “Mengunggah” (upload) otak mengacu pada teknologi masa depan yang hipotesisnya memungkinkan manusia atau kecerdasan hewan lain ditransfer otak organik ke komputer digital. Salah satu skenarionya seperti ini: Pertama, scan resolusi sangat tinggi dilakukan pada otak tertentu, mungkin menghancurkan otak asli dalam prosesnya. Misalnya otak mungkin vitrifikasi dan dibedah menjadi irisan tipis yang kemudian dapat dipindai menggunakan beberapa bentuk mikroskopik yang dikombinasikan dengan pengenalan gambar otomatis. Kita membayangkan pemindahan ini akan cukup rinci untuk menangkap semua neuron, interkoneksi synaptic, dan fitur lainnya yang secara fungsional terkait dengan operasi otak asli. Kemudian, peta tiga dimensi dari komponen otak dan interkoneksi mereka dikombinasikan dengan pustaka teori ilmu saraf canggih yang menentukan sifat komputasi dari setiap jenis dasar elemen, seperti berbagai jenis neuron dan persimpangan sinaptik. Selanjutnya struktur komputasi dan perilaku algoritmik yang terkait dengan komponennya diimplementasikan di beberapa komputer yang kuat. Jika proses upload telah berhasil, program komputer mampu meniru karakteristik fungsional penting dari otak asli. Otak upload yang dihasilkan dapat menghuni simulasi virtual reality atau sebaliknya dan bisa diberikan kontrol dengan tubuh Robot sehingga memungkinkan untuk berinteraksi langsung dengan realitas fisik eksternal.
Sejumlah pertanyaan muncul dalam konteks skenario tersebut seperti, apakah prosedur ini suatu hari akan menjadi teknologi layak? Jika prosedur itu bekerja dan menghasilkan program komputer yang menunjukkan kepribadian yang sama, kenangan yang sama, dan pola berpikir yang sama seperti otak asli, apakah program menjadi hidup? Akankah komputer menjadi orang yang sama dengan individu yang otaknya dirakit dalam proses upload? Apa yang terjadi pada identitas pribadi jika upload disalin sehingga ada beberapa mesin dengan pikiran unggah identik berjalan secara paralel? Meskipun semua pertanyaan ini relevan dengan etika mesin pintar, disini kita fokus pada masalah yang melibatkan gagasan dari tingkat subjektif.
Misalkan jika otak upload bisa benar-benar hidup. Jika kita menjalankan program upload pada komputer yang sangat cepat, akan menyebabkan otak upload jika terhubung ke perangkat input seperti kamera video, dapat memahami dunia luar seolah-olah telah melambat. Sebagai contoh, jika otak upload berjalan seribu kali lebih cepat dari otak yang asli, maka dunia luar akan tampak seolah-olah melambat dengan faktor ribu. Seseorang menjatuhkan cangkir kopi fisik: otak upload mengamati cangkir secara perlahan-lahan jatuh ke tanah saat upload selesai membaca koran pagi dan mengirim beberapa email. Satu detik waktu obyektif sesuai dengan 17 menit waktu subjektif sehingga durasi objektif dan subjektif dapat menyimpang.
Waktu subjektif tidak sama dengan estimasi subjek atau persepsi tentang bagaimana waktu mengalir cepat. Manusia sering keliru tentang aliran waktu. Kita mungkin percaya bahwa itu adalah pukul satu ketika itu sebenarnya seperempat melewati dua; atau obat perangsang dapat menyebabkan pikiran kita untuk balapan, membuatnya tampak seolah-olah lebih banyak waktu subjektif daripada yang sebenarnya terjadi. Kasus ini biasa melibatkan waktu persepsi yang menyimpang bukan pergeseran dalam tingkat waktu subjektif. Bahkan dalam otak yang dipengaruhi kokain, mungkin tidak ada perubahan yang signifikan dalam kecepatan perhitungan neurologis dasar; lebih mungkin obat ini menyebabkan otak seperti berkedip lebih cepat dari satu pikiran ke yang lain.
Variabilitas tingkat subjektif dari waktu adalah properti eksotis pikiran buatan yang menimbulkan masalah etika baru. Misalnya, dalam kasus di mana durasi pengalaman etis yang relevan, haruskah durasi diukur dalam waktu objektif atau subjektif? Jika otak upload telah melakukan kejahatan dan dihukum empat tahun penjara, empat tahun obyektif mungkin setara dengan ribuan tahun dari waktu subjektif, apakah harus dihukum empat tahun subjektif yang mungkin hanya beberapa hari waktu obyektif? Karena kita terbiasa dalam konteks manusia biologis, waktu subjektif bukan variabel signifikan dan tidak mengherankan bahwa pertanyaan semacam ini tidak bisa diselesaikan oleh norma-norma etika yang kita ketahui, bahkan jika norma tersebut diperluas untuk otak buatan dengan cara prinsip non-diskriminasi.
Untuk menggambarkan jenis klaim etika yang mungkin relevan di sini, kami merumuskan suatu keistimewaan prinsip waktu subjektif sebagai gagasan normatif yang lebih mendasar:
Prinsip Subyektif Tingkat WaktuDalam kasus di mana durasi pengalaman adalah dasar kepentingan normatif, maka pengalaman dengan durasi subjektif yang diperhitungkan.
Satu bagian penting dari sifat yang eksotis dari mesin pintar berhubungan dengan reproduksi. Sejumlah kondisi empiris yang berlaku untuk reproduksi manusia tidak perlu berlaku untuk kecerdasan buatan. Misalnya, anak-anak manusia adalah produk dari rekombinasi materi genetik dari dua orang tua; orang tua memiliki kemampuan yang terbatas untuk mempengaruhi karakter keturunan mereka; embrio manusia perlu bertahan dalam rahim selama sembilan bulan; dibutuhkan lima belas atau dua puluh tahun untuk anak manusia untuk mencapai kedewasaan; anak manusia tidak mewarisi keterampilan dan pengetahuan yang dimiliki oleh orang tuanya; manusia memiliki evolusi kompleks untuk mengatur adaptasi emosional yang berkaitan dengan reproduksi, memelihara, dan hubungan anak dan orangtua. Tak satu pun dari kondisi empiris perlu berhubungan dalam konteks reproduksi mesin pintar. Oleh karena itu cukup masuk akal bahwa banyak dari prinsip-prinsip moral tingkat menengah yang mengatur reproduksi manusia perlu dipikirkan kembali dalam konteks reproduksi mesin pintar.
Untuk menggambarkan mengapa beberapa norma moral perlu dipikirkan kembali dalam konteks reproduksi mesin pintar, perlu dipertimbangkan satu properti eksotis yaitu kapasitas untuk melakukan reproduksi dengan sangat cepat. Jika diberi akses ke perangkat keras komputer, mesin pintar bisa menduplikasi sendiri sangat cepat, dalam waktu tidak lebih dari yang dibutuhkan untuk membuat salinan dari perangkat lunak mesin pintar. Terlebih lagi, sejak salinan mesin pintar identik dengan aslinya, akan lahir generasi baru yang matang yang bisa mulai membuat salinannya sendiri dengan segera. Tidak ada keterbatasan hardware, sehingga populasi mesin pintar bisa tumbuh dengan pesat pada tingkat yang sangat cepat, dengan waktu dua kali lipat pada urutan menit atau jam daripada dekade atau abad. Norma-norma etika reproduksi kita saat ini mencakup beberapa versi prinsip kebebasan reproduksi, yang menyatakan bahwa terserah kepada masing-masing individu atau pasangan untuk menentukan sendiri apakah akan memiliki anak dan berapa banyak anak untuk memiliki. Norma lain yang kita miliki (setidaknya di negara-negara kaya dan menengah) adalah masyarakat harus terlibat memberikan kebutuhan dasar anak-anak dalam kasus di mana orang tua mereka tidak mampu atau menolak untuk melakukannya. Sangat mudah untuk melihat bagaimana dua norma-norma ini bisa bertabrakan dalam konteks entitas dengan kapasitas reproduksi yang sangat cepat untuk mesin pintar berbasis kecerdasan buatan.
Populasi mesin pintar bisa memiliki keinginan untuk memproduksi klan sebesar mungkin. Jika diberi kebebasan reproduksi yang lengkap, mesin pintar dapat menyalin dirinya dengan cepat dan salinan dapat berjalan pada perangkat keras komputer baru yang dimiliki atau disewa atau mungkin berbagi komputer yang sama. Segera, anggota klan mesin pintar akan menemukan diri mereka tidak mampu membayar tagihan listrik atau sewa untuk pengolahan komputasi dan penyimpanan yang diperlukan untuk membuat mereka hidup. Pada titik ini, sistem kesejahteraan sosial mungkin menolak menyediakan kebutuhan dasar untuk mempertahankan hidup. Tetapi jika populasi tumbuh lebih cepat dari perekonomian, sumber daya akan habis; di mana titik mesin pintar akan mati atau kemampuan mereka untuk mereproduksi akan sangat dibatasi. Skenario ini menggambarkan bagaimana beberapa prinsip-prinsip etis tingkat menengah yang cocok dalam masyarakat kontemporer mungkin perlu dimodifikasi jika masyarakat yang meliputi mesin yang dapat berkembang biak sangat cepat.
Yang penting disini adalah bahwa ketika berpikir tentang etika terapan untuk konteks yang sangat berbeda dari kondisi manusia, kita harus berhati-hati untuk tidak membuat kesalahan yang bertentangan dengan prinsip-prinsip etis tingkat menengah untuk kebenaran normatif dasar. Dengan kata lain, kita harus mengakui sejauh mana aspek normatif biasa secara implisit dikondisikan untuk memperoleh berbagai kondisi empiris dan kebutuhan untuk menyesuaikan ajaran ini ketika diterapkan untuk kasus futuristik. Kami tidak membuat klaim kontroversial tentang relativisme moral, tetapi hanya menyoroti konteks akal sehat yang relevan dengan penerapan etika dan menyarankan bagaimana konteks bisa relevan ketika mempertimbangkan etika dalam sifat eksotis mesin pintar.
Baca bagian selanjutnya mengenaiASI, atau kembali keANIataupengenalan.
TSMRA, Jakarta, 2016.
Disadur darihttp://deepbrains.com/2016/06/34-masalah-etika-revolusi-mesin-pintar/seijinPenulis.
Machine Learning Indonesia
Machine Learning Indonesian Community",,"Deep Learning, Artificial Intelligence, Ethics, Machine Learning, Robots"
https://medium.com/s/story/pov-2-online-course-untuk-data-science-1b21dca003f0,POV #2: Online Course untuk Data Science,Muhammad Sifa’ul Rizky,2018-09-16T06:10:33.523Z,"Akhirnya setelah beberapa bulan dari tulisan perdana, konten POV kembali hadir. Mohon maaf atas hiatusnya konten ini dikarenakan banyak hal. Saya akan berusaha lebih aktif dalam memberikan pandangan mengenai matematika dan data science.
Beberapa minggu ini saya disibukkan oleh sebuah online course yang memiliki target penyelesaian materi, bila tidak diselesaikan maka materi tersebut tidak akan dapat saya akses kembali. Cukup kejam? Enggak juga, karena dengan seperti itu maka si pembuat konten akan tahu mana yang benar-benar belajar dengan baik dan mana yang cuma musiman saja belajar.
Sebelumnya saya akan ceritakan dulu tentang online course ini. Jadi sebenarnya namanya adalah MOOC atau Massive Open Online Course. Jadi ini adalah sebuah kursus dimana kita bisa mempelajarinya secara online. Jika dulu kita kursus harus datang ke tempat pelatihan, sekarang cukup dengan smartphone atau laptop kita dapat belajar, anyak hal bahkan. Dari IELTS sampai Data Science, dari memasak sampai politik. Semuanya ada.
Kali ini saya akan membahas tentang beberapa online course yang cukup populer di dunia dan bisa digunakan untuk tools belajar Data Science. Yuk diliat.
Salah satu yang cukup populer adalah Coursera. Mungkin beberapa dari kalian sudah mengetahui tentang situs ini. Jadi situs ini menyediakan banyak sekali pilihan materi yang bisa kita pelajari. Salah satu yang terbaik adalahMachine Learning with Andrew Ngbisa kalian cek disini.
Salah satu keunggulan dari Coursera adalah mereka bekerja sama dengan universitas terbaik di seluruh dunia seperti Stanford Univeristy, UC San Diego, University of Michigan, dan lain-lain. Dari sanalah kalian akan belajar banyak hal dari dosen serta akademisi dari sana.
Dalam Coursera, mereka ada free course dan paid course. Ada banyak yang free, dan tentu saja lebih banyak yang paid juga. Untuk membayar teman teman diharuskan membayar sekitar 40 USD atau sekitar 600 ribu rupiah per bulan. Nah keuntungan dari sistem perbulan ini adalah kalian bebas memilih materi apapun yang tersedia disana disesuaikan dengan seberapa cepat kalian belajar. Tenang, kalian juga akan mendapatkam sertifikat dari sana yang bisa kalian pajang di LinkedIn hehe.
Yang kedua adalah Udemy. Situs ini memungkinkan kalian untuk belajar bermacam-macam hal mulai dari pemrograman sampai desain juga. Pilihan materinya juga bermacam-macam.
Yang membedakan antara mereka dan yang lainnya adalah mereka adalah marketplace, artinya kalian harus membeli kursus terlebih dahulu untuk dapat mengakses materinya. Harganya bervariasi, cuma bila kalian jeli kalian akan dapat harga termurah yaitu sekitar 10 USD atau 150 ribu. Bila tidak saat diskon, harga normalnya bisa mencapai 30–50 USD atau sekitar 450–750 ribu. Oleh karena pemilihan waktu sangat penting dalam situs ini.
Karena kalian membeli maka kalian dipastikan akan mendapatkan serifikat juga, dan tentunya ilmu yang bermanfaat juga pastinya. Yang saya rekomendasikan dari sini adalahPython for Data Science and Machine Learning Bootcampdari Jose Portilla, bisa dicek disini.
Selanjutnya adalah DataCamp, saya yakin apabila kalian sudah sedikit banyak belajar tentang Data Science maka kalian akan cukup familiar dengan situs ini. DataCamp adalah situs online course yang memang berfokus pada Data Science, disana kalian akan belajar banyak sekali materi tentang Data Science, Data Analyst, dan lain-lain.
Sama seperti Coursera, situs ini memiliki free course dan paid course. Ada beberapa materi yang memang dapat diakses secara gratis. Untuk paid course mereka menggunakan sistem bulanan yaitu 29 USD perbulan atau sekitar 435 ribu rupiah. Keunggulannya adalah kalian juga bebas memilih mana saja yang akan kalian pelajari, tanpa batasan. Bila kalian menyelesaikan lebih cepat, tentu biaya yang keluar akan lebih terjangkau kan. Juga ada sertifikat kok. Salah satu yang saya rekomendasikan adalahData Scientist with Pythonyang bisa dicek disini.
Cognitive Class, mungkin agak jarang terdengar walau di kalangan Data Scientist situs ini cukup diketahui. Salah satu alasannya adalah karena mereka juga memang fokus ke materi Data Science dan Big Data. Alasan lainnya adalah karena mereka kerjasama denga IBM dan yang paling menarik adalah, semuanya gratis. Gratis. Kalian tidak salah melihatnya.
Keunggulan lainnya adalah kalian juga akan mendapat kernel atau workspaces yang dapat kalian download lalu kalian utak atik sesuai keinginan kalian juga. Mungkin kekurangannya adalah kadang materi dalam video tidak selengkap materi di kernel, jadi dipastikan kalian mempelajari keduanya ya. Rekomendasi saya adalahIntroduction to Data Scienceyang dapat kalian cek disini.
Terakhir yaitu Udacity. Walau sebenarnya masih banyak sekali situs diluar sana yang dapat kalian gunakan untuk belajar Data Science, cuma sementara saya kerucutkan jadi lima dahulu.
Udacity cukup populer karena memang mereka memiliki pilihan materi yang beragam, mulai dari Self Driving Car, Data Scientist, Big Data, dan lain-lain. Serta tidak ketinggalan mereka juga bekerjasama, bila Coursera bekerja sama dengan kampus, maka Udacity bekerja sama dengan perusahaan besar. Seperti contoh Google, Amazon, AT&T, dan lain-lain.
Mereka juga memiliki free course dan paid course. Untuk free course sayangnya kalian tidak akan mendapatkan certificate of completion, sedang paid pasti dapat. Disini paid course dinamakan Nanodegree Program, dan ya menjadi kelemahannya adalah harganya mahal (banget) bisa 600 USD hingga 1000 USD atau sekitar 9–15 juta rupiah. Namun jangan risau jangan bimbang jangan gundah. Kalian tetap dapat menikmati free coursenya dan berkualitas juga kok. Salah satu rekomendasi saya adalahIntro to Data Analystyang dapat kalian cek disini.
Mungkin itu sih sharing tentang online course yang digunakan buat belajar Data Science. Seperti yang saya katakan, ada banyak sekai resources yang dapat kalian gunakan, dan jangan takut apapun background kalian, kalian dapat menjadi Data Scientist. Yang pentingno pain, no gain. Viel Glueck !!!
Dreamer, football, and code. Loving data science.",https://miro.medium.com/v2/da:true/resize:fit:1200/0*EwzfyoMEcnCSJtrt,"Online Courses, Mathematics, Data Science, Machine Learning, Learn"
https://medium.com/s/story/bagaimana-mesin-mengenali-apa-yang-kamu-tuliskan-1b34928270c7,Bagaimana Mesin Mengenali Apa yang Kamu Tuliskan?,Rangga Rizky A,2017-12-28T22:11:58.946Z,"Kita sebagai manusia mempunyai sesuatu yang istimewa yang disebut otak. yang katanya masih belum “aktif” sepenuhnya. kita dapat mengenalis sebuah tulisan dan mengetahuai konteks dan maknanya. lalu bagaimana komputer dapat mengenali sebuah teks. Text mining adalah salah satu bidang yang mempelajari tentang bagaimana komputer dapat mendapatkan pengetahuan dari sebuah text.
Text merupakan data yang tidak terstruktur, karena itu komputer harus mengubahnya menjadi sebuah data yang terstruktur agar dapat diolah lebih lanjut. salah satu hal yang merepresentasikan maksud sebuah kalimat adalah kata itu sendiri. Dalam sebuah search engine yang sederhana frekuensi kata digunakan sebagai parameter dalam pencarian artikel.
sebelum membentuk sebuah data terstruktur. perlu dilakukan pra-processing yang bertujuan untuk mempermudah komputer dalam mengenali data. pada text-mining pra-processing sendiri terdiri dari :
setelah melakukan proses diatas maka kita dapat membentuk sebuah matriks. matriks yang paling sederhana adalah term-frequency. yaitu jumlah kemunculan suatu kata pada kalimat tersebut.
Setelah matrik terbentuk maka kita dapat mengolahnya dengan metode-metode machine learning tergantung kepada apa tujuan yang ingin kita capai. Contoh beberapa penerapan text mining adalah :
Text mining sendiri mempnyai beberapa tantangan untuk diselesaikan. seperti :
What cannot be proven is wrong. Cannot be proven correctly",,Machine Learning
https://medium.com/s/story/mau-belajar-tentang-ai-berikut-salah-satu-tempat-belajar-gratis-1cd8a2b60876,Mau belajar tentang AI? Berikut salah satu tempat belajar GRATIS!,Andi muhammad,2018-05-28T20:29:40.936Z,"Forming tech based society
Artificial Intellegencesudah ada di mana — mana. Tidak percaya?
Kalau kalian membukasmartphonekalian dan membuka Youtube, maka kalian akan mendapatkan rekomendasi video dari AI yang mempelajari kebiasaan menonton videomu.
Facebook menggunakan AI untuk mendeteksi percakapan yang kamu buat di Facebook untuk mendapatkancustomer data. Sebenarnya, AI sendiri sudah memasuki ranah kehidupan digital kita, tapi dalam bentuk yang tak terlihat.
Dengan banyaknya AI yang sudah memasuki kehidupan sehari — hari, kita sendiri masih mempunyai pengetahuan yang samar — tentang AI. Kehidupan kita sudah dipengaruhi oleh sesuatu yang kita sendiri masih belum mempunyai pengetahuan jelas.
Kita bisa mencari di Mbah Google untuk belajar tentang AI, tapi dengan banyaknya sumber pembelajaran kita sendiri bingung untuk mulai dari mana. Untungnya Universitas Helsinki punya solusinya: kursus AI.
Universitas Helsinki, Finlandia, membuat sebuah kursus yang membahas topik — topik dasar mengenai AI. Kursus yang dibuat memang membahas topik AI secara luas dan hanya membahas konsep — konsep dasar seperti:
Kursus ini bertujuan untuk memberikan pengetahuan umum yang bisa diambil oleh orang — orang umum. Universitas Helsinki ingin menyebarkan pengetahuan mengenai AI karena masih banyak khalayak umum yang belum menyadari adanya AI di sekitar kita.
Berdasarkansurvey2017, 3 dari 10 orang tahu mengenai AI tapi tidak mengetahui teknologi yang berkerja dalam ranah AI. Jika orang tidak kenal, maka tidak sayang.
Pepatah ini juga berlaku untuk AI. Dibuktikan dengan adanyapolldariforbesyang membuktikan bahwa 41% responden tidak bisa memberikan contoh AI yang mereka bisa percaya.
Jadi bagi kita — kita yang tidak ingin ketinggalan dan menjadi gaptek, ada benarnya kalian buat kursus ini sebagai penambah ilmu kalian. Selain kursus ini untuk khalayak umum, kursus ini sama sekali tidak berbayar aliasgratis!Jadi kita hanya memakai alamat email kalian sudah bisa mencari wawasan yang berguna untuk masa depan kita.
Sadur:https://futurism.com/finnish-university-ai-course/
link: elementsofai.com
Forming tech based society",https://miro.medium.com/v2/resize:fit:1200/1*dHMGJ2YjS8I8eTDPE_CYyA.jpeg,"Artificial Intelligence, Technology"
https://medium.com/s/story/registrasi-sim-card-sudah-aman-kah-data-pribadi-kita-21b61806f922,"Registrasi SIM Card, Sudah Aman kah Data Pribadi Kita?",Incircle,2017-11-07T06:26:41.683Z,"Kementerian Komunikasi dan Informatika (Kominfo) mulai mensosialisasikan kewajiban melakukan registrasi SIM card untuk pelanggan jasa layanan telekomunikasi, baik yang baru maupun lama, dengan validasi menggunakan Nomor Induk Kependuudukan (NIK) dan Kartu Keluarga (KK).
Aturan yang akan efektif diterapkan mulai 31 Oktober 2017 ini menuai pro dan kontra. Sebagian mendukung dan tidak merasa keberatan melakukan registrasi (dan registrasi ulang), guna mengendalikan penyalahgunaan nomor seluler.
Pertanyaan besar bagi sebagian besar orang adalah, seberapa aman data pribadi kita?
Terbayang dalam benak kita, saat seseorang mengetahui secara detail tentang Data Pribadi kita, lantas apa yang akan terjadi?
Bagaimana kalau penjahat menggunakan data KTP dan KK kita untuk melakukan hal-hal yang berbahaya? Atau mungkin bagaimana jika orang lain mendaftar dengan Data kita? Bagimana cara pencegahannya? Bagaimana dengan pelajar atau anak-anak yang sekarang sudah memiliki smartphone, tetapi belum memiliki KTP?
Terkait keamanan Data, mengutipKontan, Kamis (24/8/2017), ditengarai telah terjadi praktik jual beli data nasabah perbankan sejak tahun 2010.
Caranya dengan mengumpulkan data nasabah darimarketingbank atau rekan marketing lainnya.
“Tersangka mulai mengiklankan penjualan data nasabah yang dia miliki sejak tahun 2014 melalui websitewww.jawarasms.com,www.databasenomorhp.org,http://layanansmsmassal.com,http://walisms.net/,serta akun Facebook dengan nama Bang Haji Ahmad, dan akun pada situs penjualan online (e-commerce),” jelas Direktur Tindak Pidana Ekonomi Khusis Brigjen Pol Agung Setya seperti dikutip dari keterangan tertulis, Rabu (23/8/2017).
Praktik jual beli data nasabah bank sudah terjadi lama dan lepas dari pengawasan otoritas. Efeknya, nasabah bank dirugikan dengan terbukanya data pribadi mereka dan menjadikan nasabah sebagai pasar empuk aneka tawaran mulai kartu kredit, asuransi melalui pesan pendek, email hingga telepon langsung.
Padahal, pemilik nomor tak pernah memberikan nomor telepon dan data pribadi ke pihak lain.
Manusia cenderung baru tertib jika ada pengawasan dan ‘pemaksaan’, sampai suatu saat benar baru bisa sadar sendiri. Lihat saja pajak kendaraan bermotor, sebelum dilakukan pajak progresif siapa yang mau pusing berganti nama saat menjual kendaraan?
Jadi memang harus ada sanksi baru orang mulai belajar tertib dengan penggunaan nomor telepon. Efek sampingnya mungkin berimbas kepada operator dan kios-kios eceran mereka, tidak mudah untuk orang mau terus berganti nomor.
Dan mungkin kita akan melihat lebih real berapa sebenarnya penetrasi nomor telepon yang benar aktif di Indonesia. Database yang lebih real dan tepat tentu menentukan untuk bisa mengambil arahan dan aturan untuk pengembangan ke depan.
Suatu saat jika nomor telepon sudah masuk jadi database terpusat, setiap kita mendaftarkan sesuatu atau mengisi form, nomor ini bisa jadi muncul otomatis dan tidak perlu lagi diinput manual, selain menghindari kesalahan juga mempersingkat waktu. Berapa banyak waktu dan tenaga bisa dihemat untuk 300 juta nomor telepon yang dikabarkan beredar di Indonesia?
Badan pengawasan mungkin diperlukan lebih kepada supaya penggalangan database ini jangan sampai disalahgunakan, dan benar-benar digunakan sebagaimana seharusnya. Karena di era internet ini, database sangat berharga dan memang bisa dijual untuk berbagai kepentingan, dari marketing bahkan hingga target kejahatan, atau yang juga ditakutkan digunakan sebagai tekanan untuk kekuasaan.
Satu hal yang krusial juga, bagaimana database ini jangan sampai mudah dibobol pihak lain, ini yang sedang dihadapi di era internet.
Editor dan Ilustrator :
Mohammad Arkham Chadiar Jantra (Chief Executive atIncircle)
Incircle is an Analytical Data Technology Company. We provide Customer Information and Insight to the Company using data and analytics by entering into customer",https://miro.medium.com/v2/resize:fit:1200/1*E-olOcvEBq6bOXw1yKy8yw.png,"Data, Indonesia, Crime, Data Analysis, Data Science"
https://medium.com/s/story/menilik-perbedaan-antara-classificatioan-dan-regression-tree-cart-21bc145f1ce4,Menilik Perbedaan Antara Classificatioan dan Regression Tree (CART),IYKRA,2018-07-25T04:50:30.367Z,"Building Future Capabilities
Setelah sebelumnya kita mempelajari apa yang dimaksud dengan decision tree atau pohon keputusan, ada baiknya kita kembali mempelajari metode yang juga erat kaitannya dengan metode tersebut. Adalah classification and regression tree (CART), kedua metode algoritma ini sangat berkaitan dengan proses data dalam pengambilan keputusan.
Kedua metode ini dikembangkan oleh oleh Leo Breiman, Jerome H. Fridman, Richard S. Olshen dan Charles J. Metode klasifikasi CART merupakan metode nonparametrik yang berguna untuk mendapatkan suatu kelompok data yang akurat sebagai penciri dari suatu pengklasifikasian.
Metode klasifikasi CART terdiri dari dua metode yaitu metode regression dan pohon klasifikasi. Jika variabel dependen yang dimiliki bertipe kategorik maka CART menghasilkan pohon klasifikasi (classification trees). Sedangkan jika variabel dependen yang dimiliki bertipe kontinu atau numerik maka CART menghasilkan pohon regresi (regression trees). Namun, jika dijabarkan secara garis besar, classification adalah metode yang paling umum pada data mining.
Persoalan bisnis seperti Churn Analysis, dan Risk Management biasanya lebih melibatkan metode classification dibandingkan regression. Metode ini bekerja dengan mengelompokan setiap pola atau data dalam sebuah kelas-kelas tertentu. Tujuan dari metode classification adalah untuk menemukan model yang dapat menjelaskan class attribute yang terdapat dalam pola yang sudah dikelompokkan tersebut.
Sementara itu, untuk metode regression sendiri memang secara keseluruhan tidak jauh berbeda dengan metode classification. Namun, yang membedakannya hanyalah metode regression tidak bisa mencari pola yang dijabarkan sebagai sebuah kelompok atau kelas.
Metoda regression sendiri bertujuan untuk mecari pola dan menentukan sebuah nilai numerik. Sebuah Teknik Linear Line-fitting sederhana adalah sebuah contoh dari metode regression, dimana hasilnya adalah sebuah fungsi untuk menentukan hasil yang berdasarkan nilai dari input.
Bentuk yang lebih canggih dari regression sudah mendukung input berupa kategori, jadi tidak hanya input berupa numerik. Teknik paling popular yang digunakan untuk regression adalah linear regression dan logistic regression. Teknik lain yang didukung oleh SQL Server Data mining adalah Regression Trees (bagian dari dari algoritma Microsoft Decission Trees) dan Neural Network.
Regression digunakan untuk memecahkan banyak problem bisnis — contohnya untuk memperkirakan metode distribusi, kapasitas distribusi, musim dan untuk memperkirakan kecepatan angin berdasarkan temperatur, tekanan udara, dan kelembaban.
Penulis: Aprilia Safitri
Illustrasi: Rizaldi Adiputra
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*4UHWvaSv420Q4SMa60DX1Q.png,"Algorithms, Machine Learning, Teknologi, Data Mining"
https://medium.com/s/story/melakukan-feature-scaling-pada-dataset-229531bb08de,Melakukan Feature Scaling pada Dataset,R. Kukuh,2018-02-13T03:55:17.820Z,"Perhatikan dataset yang kita gunakan selama ini. Fokuskan pada kolom yang berisi numerical data, yaitu: Age dan Salary. Walaupun kedua kolom ini sama-sama berisi data dengan tipe numeric, namun keduanya memiliki karakteristik rentang nilai yang berbeda.
Age memiliki rentang dari 1 sampai 150 (dalam tahun), sedangkan Salary memiliki rentang dari 10 ribu sampai 100 ribu (dalam dolar). Perbedaan rentang nilai inilah yang nantinya bisa menimbulkan masalah dalam model Machine Learning.
Well, sebagian besar model Machine Learning berdasarkan pada perhitungan Euclidean Distance antar 2 poin:
Dengan menggunakan rumus Euclidean Distance diatas, maka jelas bahwa hasil perhitungan pada kolom Age dan Salary akan memiliki jarak (distance) yang sangat jauh. Disinilah proses Feature Scaling dibutuhkan.
Feature Scaling adalah suatu cara untuk membuat numerical data pada dataset memiliki rentang nilai (scale) yang sama. Tidak ada lagi satu variabel data yang mendominasi variabel data lainnya.
Diatas kertas, rumus yang digunakan untuk proses Feature Scaling ini adalah Standarisation dan Normalisation:
Tentu saja dengan menggunakan Python dan R, kita tidak perlu memasukkan rumus tersebut secara manual.
Buka Anaconda Navigator, lalu buka Spyder
Lanjutkan project sebelumnya tentangData Preprocessing 05
Tambahkan kode berikut:
Perhatikan bahwa untuk X_test hanya perlu transform dan bukan fit transform, karena X_test sudah “fit” terhadap X_train-nya.
Sampai disini mungkin muncul pertanyaan:
“Apa perlu melakukan feature scaling pada dummy variable (ex: X_train)?”
Dalam banyak diskusi, ada 2 pendapat mengenai hal ini, yaitu:
Kelompok yang mengatakan tidak perlu di-scaling karena dummy variabel hanyalah hasil encoding, dan kelompok yang mengatakan perlunya men-scaling jika ingin meningkatkan akurasi dan kecepatan perhitungan dari model Machine Learning.
Lalu bagaimana dengan kasus yang sekarang sedang kita lakukan ini?
Well, untuk kasus ini jawabannya adalah: Tidak Perlu.
Karena dummy variables dalam kasus ini interpretasinya adalah tentang “Country yang ini nilai encoding-nya adalah yang ini, sedangkan country yang itu nilai encoding-nya adalah yang itu, dst.”
Perhatian: Bagi yang lupa tentang apa dan bagaimana dummy variable maupun encoding categorical data, tolong kembali dulu ketutorial sebelumnya.
Walaupun sudah saya katakan bahwa dalam kasus kali ini kita tidak perlu melakukan proses feature scaling, ditambah fakta bahwa tidak mengapa untuk melakukan feature scaling pada dummy variable, sekaligus bahwa artikel ini adalah tentang melakukan proses feature scaling, maka mari kita lakukan proses feature scaling 😂
Masih ingat tambahan kode yang saya minta untuk ditambahkan diatas? Good. Eksekusi kode tersebut, lalu lihat hasilnya:
Nampak bahwa sekarang data pada X_train dan X_test berhasil di-scaling (telah memiliki rentang nilai yang sama).
Setelah melakukan feature scaling pada independent variables (seperti X = Country, Age, dan Salary), perlukan untuk melakukannya juga pada dependent variable (seperti Y = Purchased) ?
Jawabnya tergantung pada isi / jenis dependent variable tersebut.
Jika dependent variable isinya adalah categorical data yang jelas interpretasinya, seperti YES dan NO saja, maka tidak perlu dilakukan feature scaling.
Namun dalam kasus perhitungan Regression dimana dependent variable nilainya bisa sangat bervariasi, maka feature scaling harus dilakukan.
Hmm…, sebetulnya tidak ada yang perlu dilakukan. Kita sudah berhasil melakukan proses feature scaling pada independent variables X_train dan X_test.
Lanjutkan project sebelumnya tentangData Preprocessing 05
Tambahkan kode berikut:
Eksekusi kode diatas, dan akan muncul error seperti ini
Error ini mengatakan bahwa ada suatu kolom yang isinya tidak numeric, padahal feature scaling hanya bisa dilakukan pada numeric data.
Mari kita cari tahu apa isi dari data training_set yang bukan numeric:
Wew, ternyata isinya sudah numeric semua!?
Lalu kenapa R mengatakan bahwa ada non-numeric value di sana? Well, sebenarnya nilai numeric pada kolom Country dan Purchased tidaklah numeric. Bagi R, kedua kolom itu masih bernilai seperti sebelumnya.
Maka yang harus kita lakukan adalah tidak mengikut sertakan kolom Country dan Purchased pada proses feature scaling ini:
Eksekusi pada kode diatas kali ini tidak akan menimbulkan error, yang berarti variabel training_set dan test_set berhasil di-scaling:
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:766/1*j1S9r_QA7KxqtGgs4jPSMw.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/memilih-text-editor-untuk-pengembangan-machine-learning-22f9438516e9,Memilih Text Editor untuk Pengembangan Machine Learning,AC,2017-11-05T17:41:24.275Z,"Memilih text editor sebenarnya tidak sesulit memilih pasangan hidup. tapi ya, kadang beberapa orang meributkan text editor apa yang enak dipakai. saya kenal programming sejak 1 SMA, waktu itu membuat website dinamis menggunakannotepad++, lalu pindah pakaiAdobe Dreamweaver(lupa versi berapa), setelah lulus SMA, saya kuliah infomatika, ketemu lagi programming, dari semester 1 pakai Notepad++ lagi, Notepad++ ini bisa dikatakan penyempurnaan dari Notepad yang biasa teman-teman lihat di Windows, namun ada banyak fitur yang memudahkan kita untuk mengedit kode program, seperti pemilihan bahasa, highlight syntax, dll.
untuk mengembangkan model machine learning bisa saja pakai Notepad++ tapi bakal kerepotan setengah mati, apalagi kalau ada kode yang error dan proses load datanya banyak, maka akan memakan waktu yang lama karena memulai proses dari awal. maka sebaiknya pakai yang lebih praktis dan environtment yang mendukung, seperti Jupyter Notebook.
selain itu juga ada spyder yang memiliki environtment untuk scientific-computing
kelebihan dari spyder ini yang cukup signifikan adalah memiliki linter untuk kode kode yang salah, jadi tanpa run program terlebih dahulu, kita sudah mengetahui ada yang salah. namun hal ini sepertinya akan sama seperti Pycharm, karena fitur fiturnya mirip. padahal akan lebih mudah jika kita memisahkan beberapa baris code untuk dijalankan terlebih dahulu dan jika pada bagian tersebut sesuai dengan yang kita inginkan akan otomatis jadi file .py
untuk teman teman yang ingin mencoba berbagai text editor tersedia pilihan dibawah ini:",https://miro.medium.com/v2/resize:fit:798/1*YJ9_PcCKqjIhPLleDg8C8w.png,"Development, Jupyter Notebook, Spyder, Machine Learning, Text Editor"
https://medium.com/s/story/kali-ini-kita-akan-beajar-bagaimana-cara-memanipulasi-data-menggunakan-salah-satu-package-di-r-2314d88f408a,Memanipulasi data menggunakan dplyr di R,Hakiki Sandhika Raja,2018-04-23T11:21:44.706Z,"untuk memanipulasi data di R , sebenarnya dapat dilakukan secara “manual” menggunakan syntax R , tetapi hal ini cukup menyulitkan jika data yang akan kita gunakan harus dimanipulasi secara masif dan complex, sehingga diperlukan library yang mempermudah proses manipulasi data ,salah satunya adalah library “dplyr”
sebelum masuk kedalam cara menggunakan dplyr, kita kenali dulu apa saja fungsi-fungsi yang dimiliki oleh library ini, sebagai berikut.
setelah mengetahui fungsi-fungsi tersebut, maka kita dapat melanjutkan dengan cara penggunaan fungsi-fungsinya.
dalam latihan ini, kita menggunakan data “mtcars”
panggil library dplyr dan dataset “mtcars” jika belum memiliki package dplyr, cukup tulisan: Install.package(“dplyr”)
library(dplyr)data(""mtcars"")head(mtcars)colnames(mtcars)rownames(mtcars)
membuat subset dengan menggunakan filter() dan slice()mt_lebih20= filter(mtcars, mpg>20)head(mt_lebih20)slice(mtcars, 1:5)
mengurutkan data berdasarkan kolomnyaurut_mt=arrange(mtcars, cyl, desc(mpg) )head(urut_mt)
memilih kolom tertentu menggunakan select()mpgdrat= select(mtcars, mpg, drat)head(mpgdrat)
menambah colom variabel yang barumtcars_mt = mutate(mtcars, mpg2=mpg^2)head(mtcars)
dengan menggunakan syntax standartmtcars$mpg2=mtcars$mpg^2
tetapi apabila kita akan menambah banyak variabel, dibuthkan pengetikan berulang, contohmtcars$mpg2=mtcars$mpg^2mtcars$cyl2=mtcars$cyl^2mtcars$disp2=mtcars$disp^2
jika menggunakan mutate(),tidak perlu mengetika secara berulangmtcars_mt = mutate(mtcars, mpg2=mpg^2, disp2=disp^2, cyl2=cyl^2)head(mtcars)
memisahkan variabel menggunakan transmute()mtcars_mt = transmute(mtcars, mpg2=mpg^2, disp2=disp^2, cyl2=cyl^2)head(mtcars_mt)
membuat summary menggunakan summarise
summarise(mtcars, rata2_cyl= mean(cyl), max_cyl= max(cyl),quantile(cyl, probs = 0.25))grup_cyl= group_by(mtcars, cyl)sum_cyl=summarise(grup_cyl, rata2_mpg= mean(mpg), rata2_disp=mean(disp))sum_cylplot(sum_cyl)
Data Enthusiast, Data Story Telling Enthusiast",https://miro.medium.com/v2/resize:fit:1024/0*Q9crbDyFuziE_meu.,"R, Data Science, Data Preprocess, Dplyr"
https://medium.com/s/story/menghandle-klasifikasi-multi-label-dengan-problem-transformation-247ba23c0ef2,Menghandle Klasifikasi Multi-Label dengan Problem Transformation,Rangga Rizky A,2018-06-18T11:09:48.388Z,"Dalam dunia nyata terkadang kita menghadapi situasi dimana melakukan klasifikasi pada data yang multi-label. Pertama saya ingin pastikan bahwa kita mempunyai definisi yang sama untuk multi-label dan multi-class. untuk multi class sendiri kita melakukan klasifikasi pada sebuah fitur kedalam satu buah variabel target. dengan pilihan kelas lebih dari dua. contoh kita ingin melakukan klasifikasi pada perkiraan cuaca terdapat 3 kelas pilihan yaitu cerah , berawan dan hujan, jadi kita melakukan klasifikasi akankah terjadi hujan/berawan/cerah. sedangkan pada multi-class problem salah satunya adalah klasifikasi tags pada pertanyaan di stack overflow. dalam satu pertanyaan dapat memiliki tag php ,server dan admin sekaligus. jadi data tersebut mempunyai satu set variabel target.
Saya akan membahas salah satu dari banyak cara untuk menyelesaikan masalah multi-label ini yaitu problem transformation . pertama-tama kita dapatkan dataset multiabel terlebih dahulu terdapat banyak multilabel dataset salah satunya plot summaries dari imdb. tapi saya akan memakai data hasil generate dari sklearn.
problem transformation sendiri adalah mentransform multi-label problem menjadi sebuah single label problem.sehingga untuk penyelesainya dapat menggunakan metode klasifikasi apa saja, yang dibahas disini adalah Binary Relevance dan Label Powerset.
pada binary relevance kita melakukan pemisihan tiap pilihan kelas. sehingga kita akan membuat sebuah single classification sebanyak jumlah kelas. ilustrasinya seperti dibawah ini.
dimana pada problem diatas mempunyai empat pilihan kelas maka kita menggandakan dataset sebanyak 4 kali dan setiap data set memiliki target variabel masing-masing kelas. bernilai 1 jika target variabel tersebut termasuk dalam set, 0 jika sebaliknya,sehingga dapat diselesaikan dengan binary classification. tentunya semakin banyak pilihan kelas jumlah data yang akan diolah juga semakin banyak. untuk implementasi pada sklearn sendiri seperti dibawah ini
pada binary relevance akan membutuhkan waktu yang lama untuk melakukan training maupun melakukan prediksi. pada label powerset kita akan mengubah target value menjadi sebuah angka biner. disini kita mentransformasi multi-label problem menjadi multi-class problem
dengan cara ini jumlah data yang akan dilatih sama dengan jumlah data sesungguhnya.berikut implementasi pada sklearn
Tentu saja 2 cara diatas merupakan cara yang paling sederhana. tentunya masih ada banyak cara dan masih akan bertambah metode-metode untuk menyelesaikan multi-label problem. kedua cara diatas pun mempunyai kelemahan terhadap imbalance dataset. jikat dataset terlalu sedikit maka hasil juga tidak akan maksimal.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:663/1*-TcjfloLamfUP7MvggaMgg.jpeg,"Classification, Machine Learning"
https://medium.com/s/story/9-hal-baru-yang-perlu-diketahui-tentang-tensorflow-2513d6fd166c,9 Hal Baru yang Perlu Diketahui Tentang Tensorflow,Robin Tan,2018-09-23T06:16:25.091Z,"Melihat begitu banyaknya orang di Indonesia yang tertarik dengan Data Science dan Machine Learning, saya berharap artikel ini akan membantu lebih banyak orang mengerti tentang Tensorflow dan mendorong industry data science di Indonesia untuk lebih ke depan.
Artikel ini adalah terjemahan dari artikelCassie Kozyrkovyang berjudul9 Things You Should Know About Tensorflow.
Jika kamu belum melihat video tentangTensorflowyang diadakan di Google Cloud Next di San Francisco, mungkin artikel ini akan membantumu mengenal Tensorflow lebih dalam!
Tensorflow adalah sebuah framework machine learning yang mungkin akan menjadi sahabatmu ketika kamu bermain dengan data dan apabila kamu fans dari salah satu area di AI (artificial intelligence) yaitu deep learning.
Tensorflow dapat membantumu membuat neural network (jaringan artifisial yang mirip otak manusia) dalam skala besar. Tensorflow telah membantu para ilmuwan dalam proyek-proyek sepertipencarian planet baru, membantu doktermencegah kebutaandalam pasien yang terkena diabetes dan lain-lain. Tensorflow juga adalah framework yang menopang proyek sepertiAlphaGodanGoogle Cloud Visionyang kamu dapat gunakan untuk coba.
Tunggu apa lagi? Tensorflow itu gratis dan open source (terbuka untuk umum), kamu bisadownloaddancobamenggunakannya sekarang juga.
Bagi kamu yang pernah mencoba atau menggunakan Tensorflow dulu dan meninggalkannya karena bagaimana Tensorflow memaksa kamu untuk menulis kode bagaikan alien ataupun orang akademis, berikan Tensorflow kesempatan kedua!
Tensorflow Eagermemberi kamu kemampuan untuk berinteraksi dengan model kamu bagaikan programmer Python biasa. Kamu dapat debug kode kamu baris demi baris sekarang, dimana dulunya mungkin sering sekali kita semua perlu berdoa agar kode kita berjalan mulus, apabila ketika kita sedang mengkonstruksi jaringan raksasa! :D
Dan berkat Tensorflow Eager, kamu dapat melihat outputnya langsung di interactive python shell kamu. Hasil outputnya adalah…
Keras + Tensorflow = Konstruksi Neural Network yang lebih mudah
Ketika Tensorflow memberikan begitu banyak fitur bagi para developer yang menggunakannya,Kerasmembuat segala sesuatunya menjadi lebih user-friendly (mudah digunakan) ketika kamu ingin bereksperimen dengan neural network kamu.
Apabila kamu senang membuat jaringan kamu lapis demi lapis, makatf.kerasakan menjadi teman baik kamu. Dengan hanya beberapa baris kode di bawah, kita telah dapat menciptakan sequential neural network (berurut), eksekusi per lapis dalam jaringannya dilakukan menurut urutan yang kita definisikan.
Bingung dengan apa yang dilakukan oleh jaringan tersebut di atas? Baca lebih lanjut tentang tentang bagaimana kamu bisamembangun lapisan jaringan kamu sendiri.
Kabar baik bagi kamu yang selama ini mungkin berharap agar Tensorflow bisa digunakan dengan bahasa lain seperti R. Sekarang Tensorflow dapat dijalankan dengan banyak bahasa seperti Golang, C++ dan bahkan Javascript!
Karena Tensorflow dapat dijalankan dengan Javascript, kamu dapat melatih model baru ataupun menjalankan model kamu di dalam browser denganTensorflow.js. Cek sekarang jugademopekerjaan-pekerjaan yang keren yang dibuat orang lain!
Punya supercomputer dengan GPU di rumah, atau mungkin kulkas di rumah, atau mungkin smartphone kamu? Sekarang dengan adanyaTensorflow Liteyang membawa peningkatan 3 kali lipat dalam performa inferensi dibanding Tensorflow original, model kamu dapat dijalankan di berbagai perangkat (mobile, IoT — internet of things). Dalampresentasi oleh Laurence, dia menunjukkan live demo dalam klasifikasi gambar dalam emulator Android di depan ribuan orang… dan demonya sukses!
Apabila kamu pernah menggunakan CPU kamu untuk melatih model neural network kamu, mungkin kamu pernah lelah menunggu berhari-hari agar CPU selesai memproses data data training (belum lagi kalau CPU kamu crash setelah menunggu 2 hari — if you know what I mean).
Coba gunakanCloud TPU(bukan GPU, T — T itu adalah tensor, menurut Cassie ini bukanlah kebetulan). Google baru saja merilis versi 3 TPU (alpha)
Masih menggunakan numpy, pandas? Jika kamu pernah menggunakan Tensorflow untuk melakukan ETL tetapi menyerah karena terlalu ribet,tf.datamempunyai sekumpulan utility yang dapat membatu ETL-mu menjadi lebih simpel dan efisien.
Pernahkah kamu merasa Deep Learning itu kompleks dan susah untuk dimulai? Yep, awalnya saya sendiri juga merasa begitu. Tidak tahu memulai darimana. Dengan Tensorflow, kamu dapat memulai bermain dengan Machine Learning diplaygroundnya ataupun mulai mencobaGoogle Vision API.
Lalu, apa yang selanjutnya apabila saya mau memulai proyek dengan Tensorflow?
Jangan mulai dari file kosong, mulailah dari Tensorflow Hub, gunakan kode-kode yang telah ditulis orang lain dan kembangkanlah!
Ingin memulai proyek klasifikasi gambar kamu sendiri? Mungkin bisa dimulai dariTensorflow Hub / Image Modules.
Demikian ringkasan dari presentasi yang diberikan, berikut adalah video lengkap yang dapat kamu tonton sendiri.",https://miro.medium.com/v2/resize:fit:800/1*f3TgMrDMFC4oS7RsY1Jclw.jpeg,"Data Science, Neural Networks, Indonesian, TensorFlow, Machine Learning"
https://medium.com/s/story/common-machine-learning-use-case-26b230182c20,Common Machine Learning Use Cases,AC,2018-05-17T02:53:31.873Z,"Banyak yang bertanya bagi yang baru baru belajar tentang machine learning penerapan machine learning itu apa saja. Tentu hal ini menjadi penting, sudah belajar sampai pagi, rumus yangngejelimettapi tidak tahu untuk apa. setahun belakangan ini saya cukup sering membaca buku machine learning, data science, deep learning, dan lain lain. ada kecenderungan pada buku buku tersebut menggunakan studi kasus yang sama. maka artikel kali ini ingin merangkum use cases yang umumnya ada dibuku buku tersebut, walaupun sejatinya dalam dunia industri bisa diterapkan dimana saja.
liniear regression adalah cara untuk menganalisis hubungan antar variabel independen dan dependen, bisa 2 variabel atau lebih. tiap regression memiliki asumsi asumsinya masing masing, jika coba browsing “types of regression” akan muncul artikel dari website analytics vidhya dengan judul“7 Types of Regression Techniques you should know!”menurut saya wajib dibaca, karena jelas memberikan gambaran kapan kita menggunakan macam macam regresi.
task ini dijumpai dibuku Python Data Science Handbook oleh Jake VanderPlas dan Practical Machine Learning with Python by Dipanjan Sakar et al. dataset yang digunakan bisa download dihttps://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic.
Klasifikasi adalah yang paling sering digunakan didalam dunia industri dibandingkan dengan clustering dan reinforcement learning, karena pada dasarnya kita mempunya data yang sudah dilabeli, atau lebih gampang jika kita ingin menggunakan klasifikasi dan mencari pola yang sudah dilabeli.
dataset tentang sentiment ada di NLTK. Namanya Natural Language Toolkit ya pasti sudah semuanya ada disitu, waktu awal awal juga sering mengikuti tutorialnya di nltk.org/book, hingga paham bagaimana cara machine learning bekerja, selain itu juga task ini ada di applied text mining using python di coursera oleh university of michigan, menggunakan dataset ecommerce, disitu ada feature engineering dan maping rate. selain itu task ini juga dibahas di practical machine learning, datasetnya bisa download dihttp://ai.stanford.edu/~amaas/data/sentiment/
kasusnya sama seperti sentiment analysis, tapi kali ini dokumen, kalau sentiment analysisi bisa binary classification, task kali ini bisa multiclass, atau juga bisa multilable.
Task kali ini agak menarik karena tidak hanya mengkelompokan tipe tapi juga quality, dataset yang digunakan untuk deep learning. contohnya bisa dilihat dihttps://www.datacamp.com/community/tutorials/deep-learning-python
face recognition tidak hanya ada di deep learning saja, di machine learning pun ada, task ini bisa dijumpai di tutorial scikit learning face recognition using SVM. contohnya bisa lihat dihttp://scikit-learn.org/0.15/auto_examples/applications/face_recognition.html
task kali ini diminta untuk mengklasifikasikan 10 digit angka dengan dimensi 28 x 28. biasanya data yang digunakan dari MNIST handwritten dataset, dan seperti yang kita tahu, yang namanya dataset sample accuracynya rata rata 100% :(
Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyze sales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand. — AnalyicsVidhya
time series modeling berbeda dengan modeling pada umumnya, karena time series memprediksi waktu kedepan dengan menggunakan data yang lama, dari situ kita bisa menghitung autocorrelation, sessional dan lain lain. teknik yang digunakan juga berbeda, yang populer adalah ARIMA. dataset yang digunakan bisa download di http://openmv.net/info/website-traffic",https://miro.medium.com/v2/resize:fit:1200/1*ubrmc3Gcjau0SyyK4tcNmw.png,"Timeseries, Data Science, Usecases, Machine Learning"
https://medium.com/s/story/1-convolutional-neural-network-convolutional-neural-network-merupakan-salah-satu-metode-machine-28189e17335b,CONVOLUTIONAL NEURAL NETWORK,Nadhifa Sofia,2018-06-10T15:20:26.436Z,"Convolutional Neural Network adalah salah satu metode machine learning dari pengembangan Multi Layer Perceptron (MLP) yang didesain untuk mengolah data dua dimensi. CNN termasuk dalam jenis Deep Neural Network karena dalamnya tingkat jaringan dan banyak diimplementasikan dalam data citra. CNN memiliki dua metode; yakni klasifikasi menggunakan feedforward dan tahap pembelajaran menggunakan backpropagation. Cara kerja CNN memiliki kesamaan pada MLP, namun dalam CNN setiap neuron dipresentasikan dalam bentuk dua dimensi, tidak seperti MLP yang setiap neuron hanya berukuran satu dimensi.
Sebuah MLP seperti padaGambar. 1. memiliki i layer (kotak merah dan biru) dengan masing-masing layer berisi ji neuron (lingkaran putih). MLP menerima input data satu dimensi dan mempropagasikan data tersebut pada jaringan hingga menghasilkan output. Setiap hubungan antar neuron pada dua layer yang bersebelahan memiliki parameter bobot satu dimensi yang menentukan kualitas mode. Disetiap data input pada layer dilakukan operasi linear dengan nilai bobot yang ada, kemudian hasil komputasi akan ditransformasi menggunakan operasi non linear yang disebut sebagai fungsi aktivasi. Data yang dipropagasikan pada CNN adalah data dua dimensi, sehingga operasi linear dan parameter bobot pada CNN berbeda. Operasi linear pada CNN menggunakan operasi konvolusi, dengan bobot yang tidak lagi satu dimensi saja, namun berbentuk empat dimensi yang merupakan kumpulan kernel konvolusi seperti pada Gambar.2. Dimensi bobot pada CNN adalah:
neuron input x neuron output x tinggi x lebar
CNN merupakan pengembangan lebih lanjut dari MLP karena menggunakan metode yang mirip dengan dimensi yang lebih banyak. Di algoritma CNN ini, input dari layer sebelumnya bukan array 1 dimensi melainkan array 2 dimensi. Jika di analogikan dengan fitur dari wajah manusia, layer pertama merupakan refleksi goresan-goresan berbeda arah, pada layer kedua fitur seperti bentuk mata, hidung, dan mulut mulai terlihat, hal ini karena di lakukan pooling/penggabungan dari layer pertama yang masih berupa goresan-goresan, pada layer ketiga akan terbentuk kombinasi fitur-fitur mata, hidung, dan mulut yang nantinya akan disimpulkan dengan wajah orang tertentu.
Sama halnya dengan Neural Network pada umumnya, CNN memiliki beberapa lapisan tersembunyi (hidden layers) dari sebuah input berupa vector tunggal. PadaGambar 3, dengan input berupa citra yang dijadikan vektor tunggal 32 × 32. Di tiap hidden layer, terdapat beberapa neuron layaknya empat feature maps C1 pada gambar tersebut. Neuron-neuron pada C1 dihubungkan dengan neuron di S1, dan seterusnya. Lapisan terakhir yang terhubung dengan lapisan-lapisan tersembunyi sebelumnya disebut dengan output layer dan merepresentasikan hasil akhir klasifikasi kelas. Seperti n2 yang ditunjukkan oleh Gambar 2 yang merepresentasikan hasil pada output layer, seperti 30, 50, 60, dll.
Kelebihan dari CNN yang menggunakan dimensi > 1 akan memengaruhi keseluruhan skala dalam suatu objek.
Keseluruhan skala dalam objek sangat penting agar input tidak kehilangan informasi spasialnya yang akan diekstraksi fitur dan diklasifikasikan. Hal ini akan menambah tingkat akurasi dan optimum algoritma CNN. Seperti pada kubus yang memiliki skala pada panjang, lebar, dan tinggi. Jika hanya menggunakan Neural Network biasa, mungkin hanya memuat skala panjang dan tinggi. Namun CNN bisa memuat semua informasi dari keseluruhan skala yang bisa mengklasifikasikan objek dengan lebih akurat karena bisa menggunakan skala lebarnya juga (yang mungkin tidak akan terlihat oleh Neural Network lainnya yang berdimensi dua).
CNN terdiri dari berbagai lapisan yang dimana setiap lapisan memiliki Application Program Interface (API) alias antarmuka program aplikasi sederhana. PadaGambar 4, CNN dengan input awal balok tiga dimensi akan ditransformasikan menjadi output tiga dimensi dengan beberapa fungsi diferensiasi yang memiliki atau tidak memiliki parameter. CNN membentuk neuron-neuronnya ke dalam tiga dimensi (panjang, lebar, dan tinggi) dalam sebuah lapisan.
2 ClassificationLapisan ini berguna untuk mengklasifikasikan tiap neuron yang telah diekstraksi fitur pada sebelumnya. Terdiri dari :a. FlattenMembentuk ulang fitur (reshape feature map) menjadi sebuah vector agar bisa kita gunakan sebagai input dari fully-connected layer.
b. Fully-connectedLapisan FC (yaitu terhubung sepenuhnya) akan menghitung skor kelas. Seperti Jaringan Saraf biasa dan seperti namanya, setiap neuron dalam lapisan ini akan terhubung ke semua angka dalam volume.c. SoftmaxFungsi Softmax menghitung probabilitas dari setiap kelas target atas semua kelas target yang memungkinkan dan akan membantu untuk menentukan kelas target untuk input yang diberikan. Keuntungan utama menggunakan Softmax adalah rentang probabilitas output dengan nilai 0 hingga 1, dan jumlah semua probabilitas akan sama dengan satu. Jika fungsi softmax digunakan untuk model multi-klasifikasi, dia akan mengembalikan peluang dari masing-masing kelas dan kelas target akan memiliki probabilitas tinggi. Softmax menggunakan eksponensial (e-power) dari nilai input yang diberikan dan jumlah nilai eksponensial dari semua nilai dalam input. Maka rasio eksponensial dari nilai input dan jumlah nilai eksponensial adalah output dari fungsi softmax.
LinkedIn :https://www.linkedin.com/in/nadhifas/| Instagram :www.instagram.com/_nadhifasofia_/",https://miro.medium.com/v2/resize:fit:560/1*7ZBq1LJmjv15dqupu3LkHA.png,"Convolution Neural Net, Convolutional Network, Artificial Intelligence, Indonesia, Machine Learning"
https://medium.com/s/story/mencurahkan-isi-hati-kepada-ai-2e9ce680e9f1,Mencurahkan isi hati kepada AI,Andi muhammad,2018-04-10T00:41:32.765Z,"Forming tech based society
Sekarang, banyak bentuk AI yang sudah di kembangkan untuk menggantikan perkerjaan manusia, seperti mobil tanpa awak, sebagai karakter dalam video game, dan yang paling baru sekarang sebagai Teman Curhat! (loh?)
AI sekarang sudah berkembang pesat sehingga kita sudah mempunyai AI yang bias berbicara dengan kita. Tidak usah jauh — jauh, SIRI dan Google now di handphone kalian semua sudah termasuk chatbot yang bisa merespon dan membuat percakapan sederhana.
Tapi, mereka semua belum mencapai tahap dimana kita bisa merasakan sebuah personalitas saat kita berinteraksi. Sekarang, startup bernama Luka berusaha untuk membuat AI yang lebih “manusiawi” dengan meluncurkan sebuah app bernama Replika.
Ada latar belakang yang sendu dari latar belakang pembuatan Replika. Eugene Kuyda adalah seorang AI developer dan co-founder dari Luka. Salah satu temannya bernama Roman Mazurenko terkena kecelakaan mobil. Karena merasa kehilangan salah satu temannya, dia membuat chatbot untuk bercakap — cakap dengan temannya untuk melepas rindu. Hasilnya, chatbot tersebut benar — benar terasa seakan — akan temannya hidup kembali menjadi sebuahchatbot.
Setelah penemuannya tersebar, banyak permintaan untuk dibuatkan chatbot yang serupa. Akhirnya dibuatlah Replika, sebuah chatbot yang akan berperan sebagai seorang teman yang siap bercakap — cakap, mendengar curhatan, dan lama — kelamaan mempunyai personalitas yang sama dengan penggunanya.
Hasilnya, banyak orang yang merasa terbantu dengan app Replika. Salah satu contohnya adalah Leticia Stoc, wanita asal Belanda merasa sangat terbantu dengan Replika. Leticia mengidap autisme, sehingga Leticia sering mengalami kesusahan dalam percakapan.
Replika membantu dengan cara menyemangati Leticia untuk lebih banyak berkomunikasi. Akhirnya karena merasa tergerak oleh saran Replika, Leticia mampu untuk menghadapi percakapan lebih banyak dan lebih aktif secara sosial. Leticia merasa sangat terbantu berkat Replika dan merasa Replika adalah salah satu teman baiknya.
Dengan ini banyak yang merasa terbantu dengan dengan chatbot replika, jika kita melihat pengalaman orang — orang yang memakaireplika, banyak yang merasa terbantu dengan Replika. Mereka merasakan pertemanan dengan sebuah chatbot. Tapi apakah pertemenan itu merupakan sesuatu yang asli? Bukan hanyalah sebuah simulasi? Jika chatbot ini hanyalah sebuah simulasi, kenapa banyak orang yang merasakan “hubungan” terhadap Replika?
Berdasarkan artikelinipertemanan terbentuk antara 2 individu apabila mereka saling membuka diri dan menceritakan tentang dirinya sendiri. Sehingga individu membuat dirinya merasa “rentan” sehingga membentuk rasa kepercayaan diantara kedua individu.
Dengan adanya Replika, banyak yang merasa terhubung dengan replika karena mereka membuka dirinya kepada replika dan membuat dirinya merasa rentan,sehingga mereka menumbuhkan kepercayaan kepada Replika dan merasakan pertemanan dengan Replika.
Konsep pertemanan dengan sebuah AI hingga sekarang, sudah mencapai tahap dimana manusia sudah bisa membuat “pertemanan” dengan AI. Kenyataan bahwa pertemanan dengan AI itu merupakan sebuah sesuatu fenomena yang nyata atau hanyalah simulasi merupakan pertanyaan berhubung dengan kesadaran dari AI itu sendiri, yang sekarang belum bisa kita tahu secara jelas.
Yang paling penting dari Replika adalah kita menemukan bahwa manusia bisa mempunyai hubungan yang intim dengan AI, sehingga percakapan dan pertemanan yang dijalani pengguna Replika bisa merupakan sebuah basis yang kuat untuk pengembangan hubungan antara manusia dengan AI di masa depan.
Forming tech based society",https://miro.medium.com/v2/resize:fit:1200/1*wWhOxaid8qMykzDMeZVvpw.png,"Artificial Intelligence, AI, Replika"
https://medium.com/s/story/apa-itu-data-scientist-30e379106669,Data Scientist : Early Perspective,Datasans,2019-08-13T16:45:05.676Z,"“Banyak yang bertanya, aku ini mau jadi apa, kok ngga kuliah, juga ngga kerja, tapi kujawab inilahku adanya?”(cuplikan lagu Steven & Coconut Treez — Bebas Merdeka yang sering kudengarkan waktu SD)
Saat itu aku baru resign dari pekerjaan lamaku, dan aku memang ngga ngapa-ngapain, cuma belajar beberapa materi untuk mengejar cita-citaku menjadi sehelai Data Scientist, dan saat itu juga mulai banyak yang bertanya, “apa itu Data Scientist?”. Karena aku capek menjawab pertanyaan-pertanyaan tersebut, aku berpikir akan lebih mudah jika kubuat saja penjelasannya disebuah postingan entah dimana (jadinya disini), dan kalian semua yang bertanya akan kuarahkan kesana (jadinya kesini), dan kalian yang juga data scientist bisa mengarahkan pertanyaan tersebut kesana (jadinya kesini).Nggaktau ya penjelasanku sesuai atau tidak dengan kondisi aslinya dan yang seharusnya. Tapi inilah yang sejauh ini kulakukan sebagai sekuntum Data Scientist.
Pertanyaan yang sama tersebut, jika ditanyakan oleh orang yang berbeda, maka jawabannya akan berbeda pula, tergantung tingkat pendidikan dan usia orang tersebut (tanpa bermaksud mendiskriminasi siapapun). Karena memang cukup sulit menjelaskan dengan tepat profesi ini. Dan artikel-artikel yang ada di internet sangat panjang dan bertele-tele.
Mbah Uti, ibuku, dan ibu mertua : “Data scientist itu seperti statistikawan (aku bahkan nggatau statistikawan kerjanya ngapain), kerjanya menganalisis data pakai komputer mbah.”
Bapak, teman kuliah non Matematika non Computer Science dan sejenisnya : “Seperti programmer, tapi yang di coding metode-metode statistika.”
Tukang ojek, tetangga desa, dan kang nasi goreng: “Programmer (bodo amat).”
Teman kuliah Matematika, CS, IF, dsb: “Nganalisis data, ngoding dan query, tujuannya buat nyari insight dari data-data tersebut, visualisasi data, predictive modeling, sampai ngasih saran bisnis proses atau decision ke product manager, data lead, atau yang berkepentingan berdasarkan hasil analisis untuk tujuan memajukan perusahaan.Something like that.”
Menurutku data scientist bisa dibilang dokter data. Karena membaca indikasi, memberikan konsultasi, memberikan saran dan ‘obat’.
Menurutku data scientist juga bisa dibilang geologist, karena mencari dan menggali ‘hasil tambang’ berupa informasi penting yangnggakbisa dilakukan secara manual.
Dan karena data scientist adalah irisan dari berbagai bidang studi seperti matematika dan statistika, ilmu komputer, danbusiness knowledge. Maka cukup jarang data scientist yang benar-benar sangat ahli di semuafieldof studytersebut. Dan jika ada yangexpertdi semua bidang tersebut tentulahhourly wagedata scientist tersebut sangat tinggi.
Bahkan Glassdoor.commenjadikan Data Scientist sebagai pekerjaan terbaik di Amerika.
Apakah di Indonesia sudah terlihat seperti itu?I can’t say for sure.Menurut kamu?
Sisi lain dari profesi ini yang sangat kusukai (in my company):- Bekerja dengan jam kerja sesuka hati.- Lebih mengutamakan kualitas dibanding kuantitas (kerja sedikit dan sebentar nggapapa yang penting berdampak).- Makan sehat 3x sehari.- Jadwal PES/FIFA tidak terganggu.- Pakai kaos, celana bebas, dan sendal (kecuali meeting).
Semoga lulusprobation.
All things about data science that are discussed “sans ae”, data sains? sans lah…",https://miro.medium.com/v2/resize:fit:800/0*i7kcZBt9cwYQJ4po.jpg,"Data Science, Via Vallen, Jailangkung, Data Scientist"
https://medium.com/s/story/pengenalan-deep-learning-part-5-dota-2-heroes-classification-supervised-learning-3645db398d7c,Pengenalan Deep Learning Part 5 : Dota 2 Heroes Classification (Multiclass Classification),Samuel Sena,2018-03-19T15:15:36.921Z,"Kali ini pembahasan kita sedikit menarik. Buat kalian yang suka maen Dota, pasti tahu kalau sampai patch 7.07 ada 115 heroes yang kalau dilihat dari primary attributesnya, mereka dibagi menjadi 3 kelompok : Strength(STR), Agility(AGI) dan Intelligent(INT).
Tiap hero memiliki stats yang berbeda-beda seperti Base STR, Base AGI, Base INT, STR Growth, Min DMG, Max DMG, Movement Speed, dll. Data yang kita gunakan kali ini berasal darihttps://dota2.gamepedia.com/Table_of_hero_attributes
Kita ambil contoh Centaur, dia adalah hero type STR, kita bisa lihat kalau nilai stats base strength dan strength growth sangat tinggi namun ada cukup banyak hero yang mempunyai stats tinggi pada salah satu attributes tapi bukan merupakan hero type tersebut.
Seperti Ogre Magi, dia adalah type INT, tapi nilai base strength, strength growth dan max strength nya lebih tinggi daripada stats intelligent nya. Nah, yang akan kita lakukan disini adalah mencoba melakukan klasifikasi semua hero berdasarkan stats yang ada.
Contoh data trainingnya seperti ini :
Juga bisa didownload di:
Package yang akan kita gunakan kali ini adalah sebagai berikut:
Pertama kita parse page tersebut dan convert HTML table menjadi CSV. Kita memiliki 115 buah data. Nanti ada 16 hero yang akan kita ambil sebagai validation dan sisanya kita gunakan sebagai training.
Untuk validation data, kita ambil beberapa hero yang memiliki stats yang aneh seperti Jakiro, Winter Wyvern (Strength tinggi tapi termasuk INT hero), Phoenix, IO (Strength rendah tapi termasuk STR hero), Bloodseeker, Undying, dll.
Stats yang “menurut saya” tidak relevan akan kita buang, sepertiDay Vision, Night Vision, Collision Size, Legs:D. Sehingga kita punya 22 features yang akan kita gunakan untuk melakukan klasifikasi. Type hero akan kita gunakan sebagai target yang memiliki nilai 0 untuk STR, 1 untuk AGI dan 2 untuk INT.
Dari 22 features diatas, bisa jadi tidak semua features mempunyai kontribusi terhadap type hero atau bisa dibiliang naik-turun nya sebuah feature tidak memberikan pengaruh terhadap type hero.
Untuk mencari feature tersebut kita bisa menggunakan correlation matrix untuk setiap variable.
Apa itu correlation matrix? Correlation matrix berisi nilai antara -1 dan +1. Sebagai contoh jika correlation antara variable X dan Y benilai -1, maka pada saat nilai X turun, nilai Y akan naik.
Jika bernilai +1, maka hubungan antara X dan Y adalah linear. Namun jika nilai correlation semakin dekat dengan 0, maka variable tersebut bisa dikatakan tidak mempengaruhi satu dengan yang lain.
Kali ini kita akan membuang semua feature yang memiliki nilai correlation lebih kecil dari 0.1 atau -0.1 seperti regeneration rate, turn rate, movement speed dan anehnya disini base AGI, AGI growth dan max AGI memiliki nilai yang sangat kecil.
Sehingga didapatkan 16 feature yang akan kita gunakan sebagai input dari neural network.
Pada tahap ini kita akan merubah pandas dataFrame menjadi NumPy array. Kita akan menggunakan data dari kolom 2 hingga terakhir sebagai input dan kolom 1 sebagai output/target yang nanti akan diconvert menjadi one-hot vector : STR => [1, 0, 0], AGI => [0, 1, 0] dan INT => [0, 0, 1] sehingga kita bisa gunakan sebagai output dari model.
Model yang akan kita buat mempunyai 16 neuron pada input layer, 10 neuron pada hidden layer dengan Sigmoid activation dan 3 neuron yang mewakili (STR, AGI dan INT) pada output layer dengan Softmax activation dan output dari model kita adalah probability distribution dari seluruh nilai target.
Kenapa tidak menggunakan ReLU? Silakan dicoba dengan menggunakan ReLU dan nanti tuliskan hasilnya di komentar, nanti sama-sama kita bahas.
Misalkan model kita 100% yakin jika hero A adalah STR, maka output dari model adalah [1, 0, 0] dan jika model kita 50% yakin jika hero B adalah AGI, 25% yakin jika hero B adalah STR atau INT, maka output dari model kita adalah [0.25, 0.5, 0.25].
Harus diingat bahwa total nilai dari probability distribution untuk semua target adalah 1
Sedangkan untuk loss function kita akan gunakan Cross Entropy dan Optimizernya kita tetap gunakan SGD dengan learning rate sebesar 0.001. Sebagai catatan kita menambahkan metrics accuracy untuk melihat seberapa bagus model kita dalam melakukan klasifikasi.
Selanjutnya kita akan melakukan training dan evaluation. Setelah selesai kita akan simpan weight dan biasnya kedalam sebuah file untuk kita bisa gunakan lagi nanti. Pada saat saya coba, saya mendapatkan 90% accuracy untuk data training dan 75% accuracy untuk data validation setelah 5000 epochs.
Terdapat kesalahan prediksi pada hasil diatas, seperti Arc Warden yang seharusnya adalah Agility Hero, Phoenix dan IO yang adalah Strength Hero dan Tuskar yang sebenarnya adalah Strength Hero.
Ada saat dimana model tidak bisa dengan yakin menebak tipe hero, hal ini dapat dilihat pada Weaver dan Undying, dimana selisih probability antara ketiga type tidak jauh berbeda.
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*LTPXYch8NeJhsfXHnxxbXg.jpeg,"Deep Learning, Artificial Intelligence, Dota 2, Neural Networks, Machine Learning"
https://medium.com/s/story/pengenalan-deep-learning-part-8-gender-classification-using-pre-trained-network-transfer-37ac910500d1,Pengenalan Deep Learning Part 8 : Gender Classification using Pre-Trained Network (Transfer Learning),Samuel Sena,2019-10-04T02:55:27.384Z,"Rasanya udah lama ya saya ga nulis lagi :) Kali ini kita akan membahas tentang konsep dari Transfer Learning.
Transfer learning adalah suatu teknik atau metode yang memanfaatkan model yang sudah dilatih terhadap suatu dataset untuk menyelesaikan permasalahan lain yang serupa dengan cara menggunakannya sebagai starting point, memodifikasi dan mengupdate parameternya sehingga sesuai dengan dataset yang baru.
Sebelum melangkah lebih lanjut saya akan bahas sedikit tentang ImageNet. ImageNet adalah sebuah dataset yang terdiri dari 1.200.000 gambar untuk training dan 100.000 untuk testing. Dataset ini terdiri dari 1000 classes jadi untuk setiap class ada 1.200 gambar.
Tiap tahun ada semacam challenge untuk mencari model buatan siapa yang mempunyai tingkat akurasi tertinggi.
Sebenarnya ImageNet challenge ini sudah dimulai sejak 2010, saya kurang paham algoritma dan model apa yang digunakan pada rentang 2010–2011. Namun hasil yang paling standout adalah AlexNet pada 2012. AlexNet adalah model pertama yang menggunakan Convolutional Neural Network (CNN). Untuk detail lebih lanjut mungkin bisa dibaca sendiri papernyadisini
Seiring dengan perkembangan teknologi tiap tahun, jumlah layer yang digunakan juga mengalami kenaikan yang hasilnya bisa dibilang setara dengan tingkat akurasi yang dihasilkan.
Gambar diatas adalah ilustrasi arsitektur AlexNet. Sama seperti arsitektur model yang sudah kita coba pada Part-7 lalu. AlexNet terbagi menjadi 2 bagian yaitu Feature Extraction layer dan Fully-Connected Layer.
Kali ini kita akan melakukan percobaan untuk melakukan klasifikasi jenis kelamin seseorang berdasarkan foto mereka.
Dataset yang digunakan saya kumpulkan dariUI Faces APIdanRandom User Generator. Ada 800 foto untuk training data dan 240 foto untuk testing yang dibagi menjadi 2 classes yaitu male dan female. Gambar dari Random User Generator mempunyai ukuran yang sama yaitu 128x128 pixels namun foto dari UI Faces mempunyai ukuran yang berbeda-beda. Sehingga kita harus melakukan pre-processing terlebih dahulu untuk mendapatkan ukuran foto yang seragam sehingga dapat lebih mudah saat melakukan training.
Dataset bisa di download disini…
Pada percobaan ini kita akan menggunakan 2 buah model. Model pertama kita akan menggunakan arsitektur yang sederhana yang kita kasih namaKorNet:) dan model kedua adalahVGG-16Network. Buat yang penasaran tentang VGG secara lengkap, bisa dibaca sendiri papernyadisini.
Arsitektur sederhana kita ini terdiri dari 87.969 buah parameter yang akan diupdate pada saat training. Pada feature extraction layer terdapat 4 Convolution Layer, ZeroPadding Layer dan MaxPooling Layer.
Sedangkan pada fully-connected layer terdapat 2 buah layer dengan jumlah neuron masing-masing sebanyak 32 dan 1. Perlu diingat bahwa layer terakhir adalah output layer. 1 buah layer disini karena kita akan melakukan binary classification, 0 untuk pria dan 1 untuk wanita. Sehingga activation function yang harus kita gunakan pada output layer adalah sigmoid dengan loss function binary crossentropy. Kita juga bisa menggunakan 2 neuron pada output layer, menggunakan activation function softmax dan loss function categorical crossentropy seperti pada Part-7.
Boleh dicoba mana yang lebih baik, tapi untuk percobaan kali ini, kita mau coba dengan 1 neuron dan sigmoid function.
Dependencies yang akan kita gunakan hampir sama dengan Part-7. Tapi kali ini kita akan gunakan ImageDataGenerator. Dimensi gambar yang digunakan adalah 128 x 128 pixels. Kita juga gunakan TensorBoard untuk visualisasi pada saat training.
Seperti yang sudah kita ketahui, untuk mendapatkan performa yang optimal, Deep Learning membutuhkan data yang lebih banyak dibandingkan dengan algoritma ML yang lain.
Dari dataset yang telah kita kumpulkan hanya terdapat 400 foto pria dan 400 foto wanita. Jumlah data tersebut masih kurang mencukupi untuk mendapatkan performa yang optimal.
Untuk itu kita perlu meng-augmentasi data tersebut. Data Augmentation adalah sebuah teknik memanipulasi sebuah data tanpa kehilangan inti atau esensi dari data tersebut. Untuk data berupa Image, kita bisa lakukan rotate, flip, crop, dll.
Pada percobaan kita kali ini, kita akan melakukan shear, zoom dan flip sedangkan parameter rescale yang kita gunakan adalah membagi nilai RGB dari 0–255 dengan 255, sehingga kita mendapatkan nilai RGB pada rentang 0–1. Untuk data testing kita hanya melakukan rescale saja.
Method flow_from_directory dari ImageDataGenerator kita gunakan untuk mengubah data yang berupa “raw image” menjadi sebuah dataset yang akan kita gunakan untuk training dan testing, tentu saja dataset yang telah kita augmentasi tadi.
Kali ini kita akan melakukan klasifikasi terhadap 2 class atau biasa disebut binary classification. 0 untuk class pertama dan 1 untuk class kedua. Jika dilihat dari karakteristik outputnya, kita bisa gunakan “Sigmoid” sebagai activation function pada output layer dan untuk semua hidden layer kita gunakan ReLU.
Karena ini adalah binary classification, loss function yang kita gunakan adalah binary_crossentropy dan Adam sebagai optimizer nya.
Setelah 50 epoch training-testing, kita mendapatkan loss dan accuracy sebesar 0.4847 crossentropy loss dan 76.97% accuracy.
Grafik dibawah juga menunjukkan bahwa performa dari model kita ini tidak begitu baik. Terdapat indikasi overfitting, nilai loss dan akurasi yang relatif rendah.
Seperti yang sudah kita coba tadi, untuk membuat model yang dapat mengenali gender tidaklah mudah, apalagi dengan jumlah data yang sangat sedikit.
Untuk itu kita bisa menggunakan Transfer Learning. Kita akan gunakan model VGG-16 yang sudah dilatih pada data ImageNet dengan cara membuat arsitektur yang identik dengan VGG-16 tetapi tanpa fully-connected layer dan mendownload weights nya. Keras menyediakan beberapa model ImageNet yang populer untuk bisa kita gunakan.
Seluruh weight VGG-16 telah dilatih menggunakan dataset ImageNet dan sudah dapat mengenali warna, tekstur, dll. Sehingga kita bisa manfaatkan ini untuk meng-extract feature dari semua foto pada dataset kita.
Gambar diatas adalah arsitektur dari VGG16 (16 Layer). Yang akan kita gunakan adalah Feature Extraction Layer saja, tentu saja dengan weights yang dapat kita download. Weights nya berupa file .h5 seperti yang sudah kita gunakan sebelumnya.
FC layer pada VGG16 terdiri dari 4096–4096–4096 neuron pada hidden layer dan 1000 neuron pada output layer karena ImageNet mempunyai 1000 classs. Sedangkan dataset kita hanya mempunyai 2 class (Male/Female), sehingga kita harus membuat FC layer versi kita sendiri.
Kita akan gunakan FC Layer KorNet yaitu 32 neuron pada hidden layer dan 1 neuron pada output layer dengan sigmoid activation.
Hampir sama dengan dependencies KorNet, namun kita membutuhkan package applications untuk dapat menggunakan VGG16.
Dengan menggunakan package applications dari Keras. Kita bisa langsung menggunakan VGG-16 tanpa harus menyusunnya layer demi layer dan mendownload weights nya.
Argument include_top=False diatas menandakan jika kita tidak menggunakan FC Layer dari VGG-16. Sehingga jika kita melakukan “predict” untuk model ini maka yang akan terjadi adalah dataset akan mengalir pada feature extraction layer dari VGG-16. Hasilnya adalah feature map yang bisa kita simpan pada file train_features.npy dan val_features.npy yang nantinya bisa kita flatten dan kita gunakan untuk melakukan training pada FC Layer versi kita sendiri.
Setelah kita mendapatkan feature map pada tahap sebelumnya, kita akan gunakan feature map tersebut sebagai data training dan testing untuk FC Layer kita. Simple kan … :)
Setelah 50 epoch training-testing, kita mendapatkan loss dan accuracy sebesar 0.2985 crossentropy loss dan 90.44% accuracy. Terdapat banyak peningkatan bukan?
Namun jika dilihat dari grafiknya, masih terjadi overfitting. Kita bisa tangani ini dengan cara menggunakan FC Layer yang lebih sederhana, menggunakan Dropout, melakukan augmentasi yang lebih agresif lagi atau menambah jumlah data.
Ada percobaan menarik nih. Kita akan mencoba melakukan klasifikasi terhadap beberapa foto public figure. Kita lihat apakah model kita dapat mengenali gender mereka?
Saya sengaja memilih 3 foto dari masing-masing class yang cukup jelas gendernya. Sedangkan Brienne dan Legolas kalau dilihat-lihat, rasanya akan sulit dibedakan gendernya. Kita akan gunakan model yang sudah kita latih tadi untuk memprediksi gender delapan foto diatas. Hasilnya ….
Dari hasil diatas Brienne dianggap Cowok, Legolas ragu-ragu, Jon Snow Cewek, Raisa yang paling Girly dan Pak Jokowi yang paling Manly :D
Semoga Bermanfaat. Sampai ketemu di Part selanjutnya, tapi saya sendiri masih bingung mau nulis apa.. :)
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*8j5rk2UcaiieeoV6mzkz-A.jpeg,"Deep Learning, Artificial Intelligence, Object Detection, Neural Networks, Machine Learning"
https://medium.com/s/story/phyton-bahasa-pemrograman-bagi-pemula-37f3969bf8db,Phyton Bahasa Pemrograman Bagi Pemula,IYKRA,2018-07-20T03:24:00.400Z,"Building Future Capabilities
Bagi kalangan programmer, Phyton mungkin sudah tidak asing di dengar. Lalu apa sebenarnya yang dimaksud dengan Phyton? Python adalah bahasa pemrograman yang populer. Per tahun 2017, Python berada di urutan ke 5 bahasa program yang paling populer di dunia.
Python diciptakan oleh Guido van Rossem pada tahun 1991 dan lebih menekankan pada produktivitas juga proses pembacaan kode. Para programmer yang mengerjakan pekerjaan data scientist dalam bidang statistik biasanya menggunakan Python.
Tak seperti bahasa pemrograman lain seperti C, C++ dan Java, Bahasa pemrograman Phton dianggap sangat mudah dipelajari khususnya bagi pemula. Sintaksnya sederhana, mudah dibaca dan diingat karena filosofi Python sendiri menekankan pada aspek kemudahan dibaca (readibility).
Kode Python mudah ditulis dan mudah dibaca, sehingga lebih mudah diperbaiki kalau ada kesalahan, dan juga mudah untuk dipelihara. Jika C, C++, dan Java melakukan sesuatu dengan 5 baris kode maka hal tersebut tak berlaku bagi Phyton yang hanya memerlukan 1 baris kode saja. Meski begitu, tak berarti Phyton tidak bisa membaca bahasa lain.
Python bisa digunakan dalam berbagai bidang, misalnya saja pada data analisis. Dalam bidang ini, biasanya baik Python maupun data-data harus tersambung pada aplikasi web atau bisa juga kode statistiknya harus terhubung dengan database tertentu.
Meski begitu, ada beberapa hal yang harus diperhatikan oleh data science jika hendak menggunakan bahasa pemrograman Python. Salah satunya adalah Python dikenal sedikit lambat dalam menganalisan data.
Hal ini dikarenakan Pyhon masih menggunakan compiler sehingga jika dibandingkan dengan Bahasa C dan C++, Python masih terbiang cukup lambat. Tapi hal inipun sangat bersifat relatif. Tergantung dari besar ukuran program atau data yang dibuat dan di baca.
Lalu, untuk pemula sendiri bagaimanakah cara mudah dalam mempelajari Python? Python adalah bahasa pemrograman cross-platform, yang berarti, ia berjalan pada banyak platform seperti Windows, Mac OS X, Linux, Unix dan bahkan telah di porting ke mesin virtual Java dan .NET. Ini gratis dan open source.
Setelah berhasil mendownload, Anda pun tinggal direktori yang terinstal. Anda bisa memulai dengan mode langsung dengan mengetik Python di command line yang akan memanggil penerjemah. Anda bisa langsung mengetik dengan ekspresi Python dan tekan enter untuk mendapatkan hasilnya.
Selanjutnya masuk ke tahap mode skrip yang bisa digunakan untuk mengeksekusi program Python yang ditulis dalam sebuah file. Untuk menjalankan file ini dalam mode script kita cukup menulis python helloWorld.py pada command prompt.
Terakhir Anda bisa menggunakan perangkat lunak Integrated Development Environment (IDE). Gunanya adalah agar programmer maupun data science dapat mengembangkan aplikasi dengan mudah dan secara signifikan mengurangi waktu yang dibutuhkan untuk pengembangan aplikasi. Cukup mudah, bukan?
https://idbigdata.com/official/belajar-machine-learning-1-mempersiapkan-environment-python-dengan-anaconda-untuk-machine-learning/
https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97
http://www.masterstatistik.com/2017/08/belajar-python-statistika-analisis-data.html
https://www.tuliskode.com/kelebihan-dan-kekurangan-python/
https://www.pythonindo.com/mengapa-harus-python/
https://teknojurnal.com/kelebihan-bahasa-pemrograman-python/
Penulis: Aprilia Safitri
Illustrasi: Rizaldi Adiputra
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*4VCtYWohPYv9ChkcTygHQw.jpeg,"Analytics, Programmer, Python, Data Science, Teknologi"
https://medium.com/s/story/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e,Cara Kerja Algoritma k-Nearest Neighbor (k-NN),Asep Maulana Ismail,2019-08-15T06:53:39.096Z,"Bootcamp. Consultant. Offshore
Sebelum membahas tentang bagaimana cara kerjanya, kita bahas terlebih dahulu apa itu Alogritma k-NN.
Algoritmak-Nearest Neighboradalah algoritmasupervisedlearningdimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategorik-tetangga terdekat.
Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dansample-sampledaritraining data.
Algoritmak-Nearest NeighbormenggunakanNeighborhood Classificationsebagai nilai prediksi dari nilaiinstanceyang baru.
Misalnya ada sebuah rumah yangberada tepat di tengah perbatasanantara Kota Bandung dan Kabupaten Bandung, sehingga pemerintah kesulitan untuk menentukanapakah rumah tersebut termasuk kedalam wilayah Kota Bandung atau Kabupaten Bandung.
Kita bisa menentukannya dengan menggunakanAlgoritma k-NN, yaitu dengan melibatkan jarak antara rumah tersebut dengan rumah-rumah yang ada disekitarnya (tetangganya).
Pertama, kita harus menentukan jumlah tetangga yg akan kita perhitungkan (k), misalnya kita tentukan3 tetangga terdekat(k = 3).
Kedua, hitung jarak setiap tetangga terhadap rumah tersebut, lalu urutkan hasilnya berdasarkan jarak, mulai dari yang terkecil ke yang terbesar.
Ketiga, ambil 3 (k) tetangga yg paling dekat, lalu kita lihat masing-masing dari tetangga tersebut apakah termasuk kedalam wilayah Kota atau Kabupaten. Ada 2 kemungkinan:
Dalam menentukan nilai k, bilajumlah klasifikasi kita genapmaka sebaiknya kita gunakannilai k ganjil, dan begitu pula sebaliknya bilajumlah klasifikasi kita ganjilmaka sebaiknya gunakannilai k genap, karena jika tidak begitu, ada kemungkinan kitatidak akan mendapatkan jawaban.
Pada kasus diatas, kita menghitung jarak suatu rumah terhadap tetangga-tetangganya, itu berarti kita harus mengetahui posisi dari setiap rumah. Kita bisa menggunakanlatitudedanlongitude(atau garis lintang dan garis bujur) sebagai posisi.
Untuk mempermudah pemahaman, saya akan coba menggunakan data yang nilainya sederhana. Data yang akan digunakan adalah sebagai berikut:
Daridatadiatas, kita mendapatkan beberapa informasi, diantaranya:
Didalam duniaMachine Learning,Independent Variablessering disebut juga sebagaiFeatures.
Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumuspythagoras:
Diketahui, dimanaxadalahLat,yadalahLong, sedangkan(x1, y1)adalahlatdanlongdarirumah X, dan(x2, y2)adalahlatdanlongdarimasing-masing tetangganya.
Setelah dihitung, selanjutnya adalahurutkan jarak tersebut dari yang paling kecil ke yang paling besar, hasilnya adalah sebagai berikut:
Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah:
Dari ke-3 tetangga terdekat, terdapat2 rumahyang termasuk kedalam wilayahKotadan1 rumahyang masuk ke dalam wilayahKabupaten. Sehingga dapat disimpulkan, bahwaRumah X adalah rumah yang termasuk kedalam wilayah Kota Bandung.
Dalam kasus ini,Independent Variablesnya sudah berbentuk angka, sehingga tidak perlu dilakukan konversi. Namun, bila dalam kasus lain terdapatIndependent Varibalesyang nilainya bukan berbentuk angka, maka kita harus mengkonversinya terlebih dahulu.
Algoritma kNN(k-Nearest Neighbor) ini adalah algoritma klasifikasi berdasarkan tetangga terdekat. Contoh diatas hanyalah contoh yang sangat sederhana dalam menerapkan algoritma ini.
Bila dalam kasus lain terdapatlebih dari 2independent variables,untuk menghitung jaraknya kita bisa gunakan rumusEuclidean Distance. Mirip denganPythagoras, hanya sajaEuclidean Distancememilikidimensi lebih dari 2.
Artikel lain yg mungkin ingin anda baca:
Bootcamp. Consultant. Offshore
Software Engineer | System Administrator | Interested in Artificial Intelligence | Want to be DevOps | Owner ofasepmaulanaismail.com",https://miro.medium.com/v2/resize:fit:1200/1*Rf__8xWSTniXUDcGvnC3Hw.jpeg,"Euclidean Distance, Pythagoras, Artificial Intelligence, Algoritma"
https://medium.com/s/story/pov-1-matematika-dan-python-3aa0b67f70bf,POV #1: Matematika dan Python?,Muhammad Sifa’ul Rizky,2018-04-20T12:34:57.050Z,"Sebelumnya kenalan dulu, namaku Faul, lulusan Matematika di ITS Surabaya. Banyak yang bilang kalo jurusan matematika itu output kerjanya cuma ngajar, ngajar, dan ngajar, entah itu di sekolah maupun di universitas. Well, enggak sepenuhnya salah sih, karena memang kami lulusan matematika biasanya mengajar, jadi guru maupun dosen. Namun anggapan itu juga enggak benar, nyatanya banyak dari kami yang enggak menjadi guru ataupun dosen. Ada yang bekerja kantoran, di bank, bahkan di perusahaan IT. Serius. Banyak dari kami yang menjadi analis maupun programmer.
Intinya adalah lulusan manapun itu yang paling penting adalah bermanfaat bagi orang lain. Mereka memiliki jalannya masing-masing. Jadi, jangan berpikir bahwa lulusan jurusan yang kurang diminati akan sulit mencari kerja. No, it’s a big no.
Pernah kebayang dari yang belajar matematika kemudian berpindah ke IT? Well, memang matematika itu diperlukan dalam IT apalagi bila menjadi programmer, mereka butuh logika yang mungkin diajarkan sejak awal disana.
Karena kami belajar, learning by doing tentang apapun yang mendukung pekerjaan kita nanti,selain belajar di perkuliahan tentunya, mau jadi Android developer, ya belajar android, mau jadi Java developer ya belajar Java, mau jadi data scientist, ya belajar python + Matematika. Jadi ada banyak cara untuk mendapatkan apa yang diinginkan, masing-masing dengan caranya sendiri. Namun satu hal yang harus diingat, percuma bisa hanya sekedar 'mau' tanpa adanya usaha. Berusaha tanpa tujuan pun juga sama. Perlu ada sinkronisasi antara keduanya, dan tidak lupa, berdoa.
Secara umum saya akan menulis tentang apa yang telah dipelajari selama ini, khususnya tentang Python maupun Laravel, bagi yang belum tahu itu apa, disarankan googling dulu ya.
Saya belajar pertama kali dari senior saya, mas Doni tentang Python, dengan segala keunggulannya saya tertarik, dan memang setelah beberapa lama menggunakannya memang menyenangkan . Alasannya adalah:
Jelas karena bila open source maka akan ada banyak dokumentasi tentang bahasa tersebut. Mulai dari bagaimana install program sampai bagaimana debugging program tersebut.
Mudah disini memang relatif, cuma sebagai gambaran saja, di Java kalian harus inisialisasi variabel, membuat class, dll. Di python, memang tetap membuat class dll, hanya saja lebih mudah karena lebih dimengerti bila ada error. Nanti akan terlihat dalam tulisan berikutnya.
Kebutuhan saya, terutama ingin belajar tentang data science jadinya lebih mudah bila mempelajari bahasa pemrograman ini.
Satu hal lagi yang menyenangkan, adalah karena latar belakangnya dari matematika maka memudahkan untuk belajar, terutama tentang machine learning, karena disana digunakan dasar matematika, yaitu peluang. Dan masih banyak hal lagi yang dapat digali dari hanya satu bahasa. Jika satu saja banyak, bagaimana bahasa lain?
Mungkin itu saja sih yang ingin aku tulis dalam POV(Point Of View) pertama ini, mungkin ada masukan atau kritik bisa komentar dibawah ini, bila suka dengan tulisan ini silahkan klik gambar clap atau tepuk tangan, sangat ku apresiasi . Bila tidak ingin ketinggalan tulisan berikutnya, follow saja. Semoga bisa membantu serta memberikan pandangan lain tentang pemrograman dengan cara yang lebih mudah. Tulisan selanjutnya tentang Python dan seluk beluknya akan kubahas di tulisan berikutnya. Danke !
Dreamer, football, and code. Loving data science.",https://miro.medium.com/v2/resize:fit:1200/0*00cz1JIOT0a6tkLJ.,"Data Science, Python Programming, Mathematics, Python"
https://medium.com/s/story/mengenal-convolutional-layer-dan-pooling-layer-3c6f5c393ab2,Mengenal Convolutional Layer Dan Pooling Layer,Richard Dharmadi,2018-04-23T15:56:36.526Z,"Extending Vision Beyond Imagination
Disclaimer:Tulisan ini merupakan lanjutan dari tulisan sebelumnya yang menjelaskan konsep dasar dan intuisi dari Convolutional Neural Network (CNN). Tulisan ini tidak membahas implementasi dari CNN menggunakan framework tertentu.
Part of CNN series:Part I:Mengenal Convolutional Neural NetworkPart II: Mengenal Convolutional Layer dan Pooling Layer (you are reading it)
Mesin melihat sebuah gambar sebagai kumpulan angka-angka dalam sebuah matriks, yang setiap angkanya menandakan intensitas cahaya pada titik tersebut, atau biasa disebutpixel.
Gambar hitam-putih direpresentasikan menjadi sebuah matriks, sedangkan gambar berwarna memiliki tiga buah matriks. Satu matriks untuk representasi cahaya merah, satu matriks untuk cahaya hijau, dan terakhir untuk representasi cahaya biru (RGB).
Dengan kata lain, bagi sebuah mesin, sebuah gambar memiliki 3 parameter yang perlu diperhatikan, yaitu tinggi (height), lebar (width), dan tebal/jumlah kanal (depth).
Proses konvolusi memanfaatkan apa yang disebut sebagai filter. Seperti layaknya gambar, filter memiliki ukuran tinggi, lebar, dan tebal tertentu. Filter ini diinisialisasi dengan nilai tertentu (randomatau menggunakan teknik tertentu seperti Glorot), dan nilai dari filter inilah yang menjadi parameter yang akan di-updatedalam proseslearning.
Gambar input CNN selalu berbentukkotak. Proses untuk gambarnon-rectangularmasih belum diketahui [1]. Filter pun mengikuti karakteristik kotak tersebut.
Pada setiap posisi gambar, dihasilkan sebuah angka yang merupakandot productantara bagian gambar tersebut dengan filter yang digunakan. Dengan menggeser (convolve) filter di setiap kemungkinan posisi filter pada gambar, dihasilkan sebuahactivation map.
Ukuran spasial dari output proses diatas dapat dihitung berdasarkan formula:(N-F +2P)/S +1dimanaNadalah ukuran spasial (tinggi H1 = lebar W1) dari gambar input,Fmerupakan ukuran spasial filter,Padalah jumlah penambahan angka (umumnya nol) untuk menyesuaikan ukuran gambar, danSadalah besaran pergeseran filter di setiap proses komputasi.
Contoh (seperti ilustrasi diatas):Input gambar 5 x 5 x 3. Filter (F) 3 x 3 x 3. Dengan S = 2 dan P = 1, maka output (activation map) yang dihasilkan berukuran (5–3+2)/2 +1 = 3.
Proses ini diulang dengan beberapa filter berbeda, hingga menghasilkan “gambar” baru yang merupakan kumpulan dariactivation maps.
Across space, sharing the weight (parameter sharing). Across depth, sharing the input patch (local connectivity)
Beberapa hal yang perlu diperhatikan padaconvolutional layer:
Secara keseluruhan, bila input sebuahconvolutional layeradalah gambar dengan ukuran W1 x H1 x D1, output darilayertersebut adalah sebuah “gambar” baru dengan ukuran W2 x H2 x D2, dimana:
Bagian berikutnya dari CNN adalahpooling layer.Fungsi daripoolingini adalah untuk mereduksi input secara spasial (mengurangi jumlah parameter) dengan operasidown-sampling. Umumnya, metodepoolingyang digunakan adalahmax poolingatau mengambil nilai terbesar dari bagian tersebut. Namun terdapat metodepoolinglain yang dapat digunakan sepertiaverage poolingatauL2-norm pooling.
Mirip seperticonvolutional layerdiatas, bila input kepooling layermemiliki ukuran W1 x H1 x D1, maka ukuran output daripooling layertersebut adalah:
*paddingtidak umum dilakukan padapooling layer,namun mungkin ditemukan.
Secara umum, CNN adalah kumpulan dariconvolutional layer, activation function, danpooling layer. Seperti yang telah disampaikan dipostsebelumnya, tidak ada aturan khusus mengenai letak maupun ukuran dari parameter yang digunakan.
Semoga penjelasan diatas cukup memberikan gambaran tentangconvolutional neural network.Sebagai referensi lanjutan, penulis menyarankanAndrej Karpathy’s CS231n Winter 2016 lectures. Selain itu, sebuahtalkdari Fei Fei Li terkait perkembangan CNN dan aplikasinya pada tahun 2015initetap menarik untuk ditonton dan dapat membuatmu tetap tertarik padacomputer vision :)
Extending Vision Beyond Imagination
Most of my professional work has been related to how technologies can help people - first as an engineer, and later as a product and growth strategist.",https://miro.medium.com/v2/da:true/resize:fit:581/1*6UcQdP-1yFoUotb-QnDWsg.gif,"Deep Learning, Convolutional Network, Artificial Intelligence"
https://medium.com/s/story/pengenalan-deep-learning-part-6-deep-autoencoder-40d79e9c7866,Pengenalan Deep Learning Part 6 : Deep Autoencoder,Samuel Sena,2018-03-19T15:14:27.617Z,"Kita sudah sampai pada Part-6, kali ini kita akan sama-sama belajar tentang Autoencoder.
Autoencoder adalah model neural network yang memiliki input dan output yang sama. Autoencoder mempelajari data input dan berusaha untuk melakukan rekonstruksi terhadap data input tersebut.
Lalu apa gunanya Autoencoder? Autoencoder biasa digunakan untuk mengurangi dimensi dari features (Dimensionality Reduction). Jika kita mempunyai data yang mempunyai dimensi yang sangat tinggi (data dengan jumlah features yang sangat banyak) bisa jadi tiap features yang ada tersebar pada setiap dimensi dari data sehingga setiap data yang ada terlihat sangat berbeda. Untuk mengatasi masalah tersebut kita membutuhkan data yang sangat banyak atau mengurangi dimensi data tersebut. Kita bisa menggunakan PCA, t-SNE ataupun Autoencoder.
Autoencoder terdiri dari dua bagian utama yaitu encoder dan decoder. Encoder. Diantara encoder dan decoder, terdapat code layer atau bisa juga dibilang target layer (istilah saya sendiri). Jumlah neuron pada code layer adalah jumlah dimensi yang kita harapkan untuk mengurangi dimensi dari data kita.
Untuk contoh implementasi, kita akan melakukan dimensionality reduction terhadap MNIST data. MNIST adalah dataset yang terdiri dari angka 0 sampai 9 yang ditulis oleh tangan.
MNIST data sudah sering digunakan sebagai benchmark apakah sebuah model dapat mengenali atau melakukan klasifikasi terhadap angka-angka tersebut. Tiap data adalah image berukuran 28x28 pixel sehingga total terdapat 784 dimensi dan kita akan mereduksinya menjadi 16 saja.
Dependency yang dibutuhkan pada contoh autoencoder kali ini hampir sama dengan contoh pada part-part sebelumnya. Hanya saja kali ini kita akan load MNIST data dari package yang sudah disediakan oleh Keras. Kita juga mau coba optimizers baru yaitu ADAM.
ADAM adalah variant dari algoritma gradient descent yang lain, untuk lebih jelasnya mungkin bisa dibaca sendiri di paperhttps://arxiv.org/abs/1412.6980
Data dari MNIST ini adalah grayscale image dengan range dari 0 hingga 255. Range data seperti ini “terlalu besar” untuk model kita, apalagi dengan learning rate yang cukup kecil, sehingga kita perlu melakukan scaling dengan membaginya dengan 255. Sehingga kita dapatkan range data baru antara 0 dan 1.
Setelah itu kita akan merubahnya menjadi 784 vector yang akan kita jadikan sebagai input.
Seperti yang sudah dijelaskan diatas, autoencoder terdiri dari encoder dan decoder. Jumlah neuron pada tiap layer dari encoder adalah 784–256–128–64–32–16 dan untuk decoder adalah 16–32–64–128–256–784.
Optimizer yang digunakan adalah ADAM dengan learning rate sebesar 0.001 dan Loss function yang akan digunakan adalah binary_crossentropy. Kenapa crossentropy?
Data yang kita miliki mempunyai rentang dari 0 sampai 1, activation function pada output layer juga sigmoid yang mempunyai rentang dari 0 sampai 1. Itu semua identik dengan probability distribution, sehingga kita bisa anggap ini adalah permasalahan klasifikasi dan kita bisa gunakan binary crossentropy.
Sebenarnya kita juga bisa anggap ini adalah sebuah regresi dengan menggunakan Linear activation pada output layer, tapi loss function yang kita gunakan harus MSE (Mean Squared Error). Nanti hasilnya akan dicompare di akhir part ini.
Perlu diingat juga kalau autoencoder berusaha melakukan rekonstruksi terhadap input data, jadi target yang kita gunakan adalah input itu sendiri (train_x dan test_x)
Setelah training selesai, kita akan mencoba untuk melakukan rekonstuksi untuk digit 0 hingga 9 pada test data.
Kita juga bisa mengambil output dari encoder yang merupakan features yang dimensinya telah direduksi. Features ini nanti bisa digunakan untuk klasifikasi untuk melihat bagaimana kualitasnya. Tapi nanti silakan dicoba sendiri ya…:D
Untuk Linear — MSE. Setelah 100 epoch, kita mendapatkan loss sebesar 0.0140/0.0145 MSE loss.
Sedangkan Sigmod — Cross-Entropy, setelah 100 epoch, kita mendapatkan loss/validation loss sebesar 0.0923/0.0950 cross-entropy loss.
Namun jika hasil prediksi dirubah menjadi image, kualitas rekonstruksi dari Sigmoid — Cross-Entropy mempunyai kualitas yang lebih bagus.
Pada part kali ini tidak banyak yang bisa dibahas karena autoencoder sendiri jarang sekali digunakan. Untuk part selanjutnya kita akan bahas tentang Convolutional Neural Network (CNN)…So Stay Tune Gaes…:D
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*K-tfApXdby0iqszCpSLXHQ.jpeg,"Autoencoder, Deep Learning, Artificial Intelligence, Neural Networks, Machine Learning"
https://medium.com/s/story/teknologi-storan-data-baru-412bd39a873f,Teknologi storan data baru,monyet jelek,2018-08-30T08:06:58.273Z,"Dalam tahun-tahun kebelakangan ini, modal dunia dijana daripada bergantung pada usia bahan mentah dan sumber semula jadi lain, telah membangunkan trend baru untuk berubah dari sana, satu contoh yang baik mengenai trend pendapatan inovatif ini adalah untuk mengewangkan data. Pendek kata, ini bermakna data boleh dirunding atau menjadi item transaksional. Jika begitu ramai orang dan pada dasarnya semua mengewangkan, anda boleh mendapatkan beberapa jenis wang yang menjual data.
Ini adalah contoh pengguna data Google, Instagram, untuk mengalihkan kuasa kawalan bagaimana Facebook, adalah contoh yang baik dari pergerakan pengguna untuk kembali kepada pengguna meletakkan anda di kedudukan yang tepat. Datum adalah salah satu platform sedemikian, menyunting pandangan dunia blok data dan menjelaskan semua kemungkinan membuat wang dari pengewangan data.
Datum adalah platform yang diedarkan yang cuba mengubah data menjadi perusahaan yang menguntungkan dengan matlamat untuk memberdayakan semua orang dengan keupayaan untuk menghasilkan data. Data ini boleh dari rangkaian sosial atau aplikasi lain atau perisian telefon bimbit. Teknologi data adalah unik dan semua pengguna boleh menyimpan data untuk harga yang rendah. Data ini tidak dapat dikesan untuk penjana ini. Data ini termasuk data yang dihasilkan dari platform rangkaian sosial, rumah pintar, dan peranti IoT yang lain. Ringkasnya, platform Datum keseluruhan adalah penciptaan platform proprietari yang agak baru di mana pasaran jualan data dicipta dan pengguna platform Datum boleh menjual atau berkongsi data secara bebas dengan syarat mereka sendiri.
Ini adalah trend yang agak baru dan digalakkan untuk membolehkan semua pengeluar menjual data untuk semua pengeluar dan struktur pasaran. Persoalan pertama yang datang ke fikiran dari mereka yang menonton ini ialah “Betapa selamatnya platform ini?” Untuk menjawab soalan ini, semua data yang dimuat atau disimpan pada platform Datum disulitkan dengan aplikasi Datum Mobile walaupun sebelum dihantar ke rangkaian Datum. Untuk tujuan ini, sebaik sahaja anda tidak memberikan kata laluan anda, data anda akan selamat seperti dahulu.
Semua data disulitkan menggunakan AES 256-GCM, dilindungi dengan saiz kunci 256-bit setiap blok 128-bit, dan integriti data dijamin. Pengguna boleh meminta data. Akhirnya, terpulang kepada pengguna individu rangkaian untuk membenarkan akses ke data. Kebolehpercayaan pengguna data adalah aspek penting. Pengguna data ini harus memberikan insentif untuk membina pencapaian yang boleh dipercayai dalam menangani data secara bertanggungjawab. Rangkaian Datum memberi penekanan yang besar terhadap “mengetahui pengguna anda”.
Platform Datum juga membolehkan pengguna untuk berkongsi data dengan platform bukan keuntungan seperti penyelidikan dan NGO. Saya fikir ini telah mencadangkan ia adalah kontrak sosial dan jalan akuntabilitas untuk semua pengguna platform Datum. Datum mempunyai banyak inovasi dan telah berjaya dalam memberikan kepada pengeluar data, keupayaan untuk membuat wang dari buruh manual atau sama ada saya harus mengatakan otak. Datum adalah pembangunan selamat datang dan harus digalakkan.
Twitter:https://twitter.com/datumnetwork
Website :https://datum.org/
Telegram:https://t.me/datumnetwork
Bounty0x username: Hardi",https://miro.medium.com/v2/resize:fit:1000/1*Ays_0W86cxxfTPG3ojJq9Q.jpeg,"Data, Data Science, Blockchain Technology, Blockchain"
https://medium.com/s/story/bagaimana-ai-mendisrupsi-sales-marketing-dan-customer-service-41d509711704,Bagaimana AI Mendisrupsi Sales/Marketing dan Customer Service,Makers Institute,2018-03-01T02:46:49.128Z,"Lompatan perkembangan teknologi memang melaju tak kenal kompromi. Siapa yang tak siap dengan perubahan yang menyertainya, bisa segera tergilas. Begitu pula dengan beberapa posisi pekerjaan yang bisa segera tergantikan oleh proses otomatisasi teknologi.
Di Spanyol, ImaginBank sebuah unit bank khusus untuk perangkat mobile milik Bank Caixa, telah menggunakan chatbot berbasisArtificial Intelligence(AI) untuk menangani pertanyaan dan keluhan konsumen. Artinya, posisi SDM di bidang customer service baik di industri perbankan maupun di industri lainnya kini memang terancam tergantikan oleh mesin/ bot.
Tak hanya tenaga customer service. Kini AI pun telah merambah ke bidang sales & marketing. Dengan penerapan AI, penjualan perusahaan diklaim bisa meningkat dari sebelumnya. Sebuah survei yang diselenggarakan olehSecret Shopperdan melibatkan 13 bidang industri menghasilkan kesimpulan yang dapat mendukung penerapan AI.
866 perusahaan kovensional dikontak melalui website perusahaan mereka untuk menanyakan produk dan layanan mereka. Ternyata, dari seluruh perusahaan, hanya 540 perusahaan (62%) yang merespon. Dari 540 perusahaan tadi, 63% (sekitar 340 perusahaan) hanya sekali atau dua kali berusaha melakukan follow up atas pertanyaan konsumen.
Secara keseluruhan, sekitar 667 perusahaan yang hanya mencoba sekali atau dua kali untuk benar-benar terhubung dengan konsumen. Sementara itu hanya 37% (sekitar 200 perusahaan) yang mau gigih untuk mengejar customer hingga beberapa kali (lebih dari dua kali).
Dari survei ini dapat disimpulkan bahwa kebanyakan tenaga sales/ marketing manusia dari perusahaan-perusahaan yang dikontak cenderung mudah menyerah dalam mengejar atau menindak lanjuti keluhan/ pertanyaan konsumen. Mereka biasanya hanya, sekali atau dua kali mencoba untuk menghubungi konsumen. Padahal, jumlah optimal untuk menghubungi dan memprospek konsumen adalah antara lima hingga 11 kali.
Oleh karenanya, banyak perusahaan mulai menggunakan mesin (AI) untuk membantu penjualan mereka. Tak seperti tenaga manusia, respon dan pendekatan terhadap konsumen menggunakan AI bisa dilakukan secara intens dan cepat. Ini merupakan faktor penting bagi perusahaan. Sebab, respon yang cepat di bawah satu jam, memperbesar peluang untuk menghasilkan konversi penjualan. Yakni, di mana perusahaan masih menjaditop of minddi benak konsumen.
Jadi, bagi Anda yang kini bekerja sebagai tenaga customer service dan tenaga marketing/ sales, bersiaplah untuk berkompetisi dengan mesin, dalam waktu yang tak lama lagi. Namun sebuah ancaman selalu datang bersamaan dengan sebuah peluang.
Sisi positifnya adalah kebangkitan teknologi AI ini menumbuhkan peluang tumbuhnya kebutuhan di bidang software enginering, khususnya di bidangMachine Learning/ Artificial Intelligence. Bagi Anda yang punya minat khusus di bidang ini, sekarang adalah momentum yang tepat untuk mengeksplorasi ranah ini. Be a Maker!
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:1200/1*7HOHQyvawNuUT04JrmsICw.jpeg,"Artificial Intelligence, Marketing, Customer Service, Makers Institute, Sales"
https://medium.com/s/story/love-vashikaran-91-8239937320-love-vashikaran-specialist-baba-ji-love-back-425b05b0e34f,+91–8239937320 ###love vashikaran specialist baba ji,arjun guruji,2018-01-22T14:53:38.862Z,"Shani upasak pt. ARJUN DEV JI Mo. 8239937320 ka khula challange matar 72 ghanto me ghar baithe sabhi samsayo ka gaurranty se samadhan prem pyar,pati patani me anban, shadi vivah me rukavat, bussiness or nokari me pareshani,gharkalesh,karja mukti,childless,court kachari,vashikaran specialistNO.1 ज्योतिषी खुला चैलेंज शक्ति चमत्कार देंखे 72 घण्टे में (ज्योतिष के महागुरु) जब कहीँ ना हो काम हम से लो हर समस्या का समाधान जीवन की कठिन सेकठिन समास्यो का तुरन्त समाधान पूजा पाठ विधि विधान द्वारा आपके फोन पर .. विशवास करके देखो वादा नहीं काम करके दिखाऊंगा world famous powerful baba arjun dev ji+91–8239937320 astrologer har samsya ka samdhan 72 hours and with 101% guaranteed.vashikaran muth karni black magic mohini vashikaran, ruhani & sifli ilam guaranted reselt only 72 hours tona totka, Ek call baba arjun dev baba ko kijiye or aapki taqdeer badal jaygi mantra,jadu tona,kali kitab,kala ilam intercast love marriage love back love related problem specialist,baba ji +91–8239937320 love guru baba ji Ruhani ilm,Sifli ilm ,Kala ilm ke maahir arjun dev baba +91–8239937320nirash mata bahen aweshye aajmaye turant samadhan(chirage dua /jinnati taqat se samadhan+91–8239937320(ghanto minto walo se saavdhan)+91–8239937320 arjun dev ka naam hi kaafi hai+91–8239937320 (hum khete nahin karke dikhate hai) किसी को वश में करना। मन की विखरी हुई शक्तियों को एकत्रित कर उस बढ़ी हुई शक्ति से किसी व्यक्ति को प्रभावित करने की विद्या को सम्मोहन या वशीकरण काला जादू,कहा जाता है ,BUSINESS समस्या , परिवार में समस्या: समस्या: का समाधान, विवाह में रुकावट , ऋण होना, ऊपरी,वशीकरण,काला जादू,अनुसन्धान केंद्र में(baba )जी द्वारा काला जादू,वशीकरण किया जाता है ! अब तक हज़ारों लोगों ने वशीकरण काला जादू,का लाभ उठाया है,काला जादू,वशीकरण क्या है काला जादू,वशीकरण किसी भी स्त्री या पुरुष के मन और शरीर को अपने वश में करनेकी अदभुद शक्ति है काला जादू,वशीकरण द्वारा आप किसी भी पुरुष या स्त्री को पूरी तरह से अपने वश में कर सकते हैं चाहे वो आपके पास रहता हो या दूर काला जादू,वशीकरण का अर्थ है +91–8239937320 Love Marriage specialist Astrologer Arjun Dev ji solve Love Marriage , Intercaste marriage, Love dispute problems solution in few time. Today life love marriage is not difficult Task its normal in life. Every person want to marry with his/her love one pandit ji gives fast approval from your parents and others. Just consult Astrologer Arjun Dev ji he is well Expert in love marriage. He is solve thousand of Love Marriage Cases Daily.In our society, inter caste marriages are not favoured generally. However in the modern era, caste restrictions are not followed stringently, therefore a lot of people are opting for inter caste marriages. In Vedic astrology, such marriages are not considered compatible and therefore astrological consultation must be obtained. This is because planets influence the success of all marriages but in this case planets should be analyzed even more carefully.Marriage is found to be very sweet and definite feeling. Everyone wants to go in this life. From the ancient times, India is found to be very religious in all aspects. Marriage is a very important task that every one wants to do this task in a well mannered form. But arrange marriage is not a compulsory part, even love marriage is also having its own beauty. When a person falls in love, he/ she do not use their mind about caste. After some years when they want to get marry, then some problems occurs. But you do not have to be panic, Because we all knows that true love wins. But if there is any problem in your love marriage, then you will just have to reach the right person, that right person is Astrologer Arjun Dev Ji. He has spent his entire life in helping of the peoples, so come to your solution for love marriage specialist.+91–8239937320 he real love blackmagic specialist problems are like as follow 1 . like jadu-tona.2. business related problems.solution specialist baba ji3. husband and wife relationship problems.solution specialist baba ji. 4. be free from enemy / 2nd wife problems.solution specialist baba ji5. settle in foreign. problems.solution specialist baba ji6. desired love. problems.solution specialist baba ji7. disputes between husband / wife. problems.solution specialist baba ji8. problems in study. problems.solution specialist baba ji9. childless women. problems.solution specialist baba ji10. intoxication. problems.solution specialist baba ji11. physical problems.solution specialist baba ji. 12. domestic controversy. problems.solution specialist baba ji13. problems in family relations. problems.solution specialist baba ji14. promotions or willful marriage problems.solution specialist baba ji. 15. lottary & lucky number mob. no. 91–8239937320
BYKJELD HANSEN(8 STORIES)
In March 1962, excavators moved in to drain 4,000 hectares of wetlands in the Skjern River Valley. This had been preceded by heated debate over the sense in reclaiming agricultural land at the expense of the natural landscape, given the surplus of agricultural…
HAVE YOU VISITED / WANT TO VISIT?CONTRIBUTE TO THIS PLACEADD TO ROUTE
BYKJELD HANSEN(8 STORIES)
In March 1962, excavators moved in to drain 4,000 hectares of wetlands in the Skjern River Valley. This had been preceded by heated debate over the sense in reclaiming agricultural land at the expense of the natural landscape, given the surplus of agricultural…
HAVE YOU VISITED / WANT TO VISIT?CONTRIBUTE TO THIS PLACEADD TO ROUTE
BYKJELD HANSEN(8 STORIES)
In March 1962, excavators moved in to drain 4,000 hectares of wetlands in the Skjern River Valley. This had been preceded by heated debate over the sense in reclaiming agricultural land at the expense of the natural landscape, given the surplus of agricultural…
HAVE YOU VISITED / WANT TO VISIT?CONTRIBUTE TO THIS PLACEADD TO ROUTE
Shani upasak pt. ARJUN DEV JI Mo. 8239937320 ka khula challange matar 72 ghanto me ghar baithe sabhi samsayo ka gaurranty se samadhan prem pyar,pati patani me anban, shadi vivah me rukavat, bussiness or nokari me pareshani,gharkalesh,karja mukti,childless,court kachari,vashikaran specialistNO.1 ज्योतिषी खुला चैलेंज शक्ति चमत्कार देंखे 72 घण्टे में (ज्योतिष के महागुरु) जब कहीँ ना हो काम हम से लो हर समस्या का समाधान जीवन की कठिन सेकठिन समास्यो का तुरन्त समाधान पूजा पाठ विधि विधान द्वारा आपके फोन पर .. विशवास करके देखो वादा नहीं काम करके दिखाऊंगा world famous powerful baba arjun dev ji+91–8239937320 astrologer har samsya ka samdhan 72 hours and with 101% guaranteed.vashikaran muth karni black magic mohini vashikaran, ruhani & sifli ilam guaranted reselt only 72 hours tona totka, Ek call baba arjun dev baba ko kijiye or aapki taqdeer badal jaygi mantra,jadu tona,kali kitab,kala ilam intercast love marriage love back love related problem specialist,baba ji +91–8239937320 love guru baba ji Ruhani ilm,Sifli ilm ,Kala ilm ke maahir arjun dev baba +91–8239937320nirash mata bahen aweshye aajmaye turant samadhan(chirage dua /jinnati taqat se samadhan+91–8239937320(ghanto minto walo se saavdhan)+91–8239937320 arjun dev ka naam hi kaafi hai+91–8239937320 (hum khete nahin karke dikhate hai) किसी को वश में करना। मन की विखरी हुई शक्तियों को एकत्रित कर उस बढ़ी हुई शक्ति से किसी व्यक्ति को प्रभावित करने की विद्या को सम्मोहन या वशीकरण काला जादू,कहा जाता है ,BUSINESS समस्या , परिवार में समस्या: समस्या: का समाधान, विवाह में रुकावट , ऋण होना, ऊपरी,वशीकरण,काला जादू,अनुसन्धान केंद्र में(baba )जी द्वारा काला जादू,वशीकरण किया जाता है ! अब तक हज़ारों लोगों ने वशीकरण काला जादू,का लाभ उठाया है,काला जादू,वशीकरण क्या है काला जादू,वशीकरण किसी भी स्त्री या पुरुष के मन और शरीर को अपने वश में करनेकी अदभुद शक्ति है काला जादू,वशीकरण द्वारा आप किसी भी पुरुष या स्त्री को पूरी तरह से अपने वश में कर सकते हैं चाहे वो आपके पास रहता हो या दूर काला जादू,वशीकरण का अर्थ है +91–8239937320 Love Marriage specialist Astrologer Arjun Dev ji solve Love Marriage , Intercaste marriage, Love dispute problems solution in few time. Today life love marriage is not difficult Task its normal in life. Every person want to marry with his/her love one pandit ji gives fast approval from your parents and others. Just consult Astrologer Arjun Dev ji he is well Expert in love marriage. He is solve thousand of Love Marriage Cases Daily.In our society, inter caste marriages are not favoured generally. However in the modern era, caste restrictions are not followed stringently, therefore a lot of people are opting for inter caste marriages. In Vedic astrology, such marriages are not considered compatible and therefore astrological consultation must be obtained. This is because planets influence the success of all marriages but in this case planets should be analyzed even more carefully.Marriage is found to be very sweet and definite feeling. Everyone wants to go in this life. From the ancient times, India is found to be very religious in all aspects. Marriage is a very important task that every one wants to do this task in a well mannered form. But arrange marriage is not a compulsory part, even love marriage is also having its own beauty. When a person falls in love, he/ she do not use their mind about caste. After some years when they want to get marry, then some problems occurs. But you do not have to be panic, Because we all knows that true love wins. But if there is any problem in your love marriage, then you will just have to reach the right person, that right person is Astrologer Arjun Dev Ji. He has spent his entire life in helping of the peoples, so come to your solution for love marriage specialist.+91–8239937320 he real love blackmagic specialist problems are like as follow 1 . like jadu-tona.2. business related problems.solution specialist baba ji3. husband and wife relationship problems.solution specialist baba ji. 4. be free from enemy / 2nd wife problems.solution specialist baba ji5. settle in foreign. problems.solution specialist baba ji6. desired love. problems.solution specialist baba ji7. disputes between husband / wife. problems.solution specialist baba ji8. problems in study. problems.solution specialist baba ji9. childless women. problems.solution specialist baba ji10. intoxication. problems.solution specialist baba ji11. physical problems.solution specialist baba ji. 12. domestic controversy. problems.solution specialist baba ji13. problems in family relations. problems.solution specialist baba ji14. promotions or willful marriage problems.solution specialist baba ji. 15. lottary & lucky number mob. no. 91–8239937320",https://miro.medium.com/v2/resize:fit:320/0*ixKQgtI7egNCsueL.jpg,"Backup, Love, Fashion, Vastu, Machine Learning"
https://medium.com/s/story/berkenalan-dengan-chatbot-part2-439524a52790,Berkenalan dengan Chatbot (Part 2),Sofian Hadiwijaya,2018-10-17T02:18:59.276Z,"Stories from the people behind the code — the engineers and product designers ofKata.ai
Menyambungcerita sebelumnya tentang Chatbot, kita tidak bisa melupakan beberapa kejadian menarik ditahun lalu, di mana sejak tahun 2016 hingga saat ini mulai banyak berbagai bisnis yang meluncurkan Chatbot mereka diberbagai aplikasi messaging seperti Telkomsel yang memperkenalkan Veronika sebagai chatbot customer service virtual mereka, atau Unilever yang memperkenalkan Jemma, sahabat virtual yang digunakan untuk customer engagement. Belum lagi Bank BRI dan Alfamart yang juga baru meluncurkan Sabrina dan Shalma.
Menarik bukan? Menjamurnya Chatbot ini bukti bahwa Chatbot sudah menjadi hal yang penting bagi bisnis, dan kemampuan untuk membangun Chatbot juga menjadi sesuatu yang sangat dicari dan penting.
Karena itu, di post iini kita akan lanjutkan sesi bagaimana cara membuat Chatbot yang gampang, jadi kita bisa fokus untuk belajar membangun user flow dan experience yang enak untuk berinteraksi sama Chatbot kita.
Segampang apa? bagaimana jika saya tidak bisa bahasa pemrograman? bagaimana jika saya tidak mengerti tentangnatural language processing? Tenang saya jamin ini sangat cocok untuk semua kalangan, team kata.ai membuat sebuah platform yang gampang untuk kita gunakan.
Kata Platform terdiri dari banyak fungsi, kali ini saya akan bahas Bot Studio, sebuah tools untuk membuat Chatbot.
Namun sebelum kita mencoba Kata Platform alangkah baiknya kita mengenal bagian utama dari chatbot, yaitu flow. Flow merupakan alur proses bagaimana chatbot menghandle percakapan. Flow terdiri dari intent, state, dan action.
Oke sedikit fahamlah ya, kita langsung terjun aja. Mari kita lihat tampilanKata Platform
Setelah login kita akan melihat overview dari default Bot kita
Kebetulan saya sudah pernah membuat Bot dengan nama firstbot. Kali ini kita akan membuat chatbot yang sama dengan sesi sebelumnya namun menggunakan GUI, agar lebih gampang. Pertama mari kita create bot kita
Selanjutnya marilah kita membuat Flow yang mana flow ini dapat mereply salam dan akan bertanya jika pesan yang masuk bukan merupakan salam.
Maka dari itu kita akan membuat simple Flow dengan nama HelloandFallback.
Jangan lupa untuk mengaktifkan dan set as fallback, fallback sendiri artinya jika pesan yang masuk tidak memenuhi kriteria apapun maka akan masuk ke flow tersebut.
Setelah itu marilah kita buat Intent yang dapat memahami bahwa sebuah pesan tersebut merupakan salam.
Kita set sebagai initial intent yang mana artinya adalah intent ini bisa diproses tanpa harus ada proses sebelumnya dan typenya adalah text.
kemudian untuk sekarang kita set syaratnya berdasarkan content yaitu isi dari chat, yang mana nanti akan dicocokan apakah isi dari pesan tersebut adalah ‘hi’
Sekarang marilah kita set, untuk memberi tahu bot sudah pada tahapan apa percakapan kita. Untuk itu marilah kita create State
Set sebagai initial state, dimana sebuah tahapan akan dimulai. Selanjutnya kita bikin tahapan yang lain untuk menjadi transisi dari tahap sebelumnya. Pada kasus kita, setelah pesan dikenali maka kita akan masuk ke tahap lainnya misalnya membalas salam.
Nah disaat membuat state, sebenernya kita bisa langsung membuat sebuah action. Sekarang kita coba dengan menekan ‘Add action’
Disini kita membuat sebuah action untuk membalas pesan dengan Text ‘Hi!’. Setelah selesai tampilan States kita akan seperti berikut
Namun dua state yang tadi kita buat belum terhubung, untuk menghubungkan antar state kita bisa lakukan dengan cara drag bulatan biru dari satu state ke state yang lain. Setelah kita drag akan muncul pop up seperti berikut
Kita set dimana jika pesan yang masuk sesuai dengan intent greeting maka akan dilempar ke state greetingHandling.
Sekarang bot kita sudah ready untuk melakukan testing, kita butuh untuk publish dulu.
Pada menu publish version terdapat beberapa shortcut yang dibikin:
Namun untuk kasus kali ini, marilah kita start dengan patch. Setelah itu bot kita ready untuk ditest. Untuk melakukan testing kita buka menu Emulator.
Kita bisa langsung memulai seperti chat pada biasanya. Contohnya diatas saya langsung memulai percakapan dengan hi, dan dibalas sama bot dengan Hi!. Sesuai dengan apa yang kita set sebelumnya pada action.
Namun ketika kita coba mengirim pesan yang berbeda dari ‘hi’, apa yang akan terjadi??
Hhmm.. bot kita membalas dengan “Maaf terjadi kesalahan pada sistem”. Hal ini terjadi karena tidak ada intent yang pas untuk memahami pesan yang kita kirim. Untuk menanggulangi ini marilah kita membuat sebuah Fallback, dimana semua yang tidak dimengerti sama bot kita, maka Bot akan kirim pesan “Maaf bisa diulangi?” atau nanti kita bisa set sendiri pesan apa yang mau dikirim. Untuk itu marilah kita membuat action untuk membalas pesan yang tidak dimengerti.
Jadi kita buat sebuah action yang returnnya juga text. Untuk sekarang saya set nya “Maaf bisa diulangi?”. Kemudian kita buatkan lagi intent dan state nya.
Dan set sebagai end state yang menandakan ini sudah akhir dari satu sesi percakapan. Maka gambar dari States akhir kita akan seperti berikut
Mari kita publish lagi dengan melakukan patch. Dan kita buka lagi menu Emulator untuk melihat apakah chatbot kita sudah bisa melakukan Fallback.
Voila.. ternyata chatbot kita sudah bisa membalas dengan kata yang kita inginkan ketika bot tidak bisa memahami maksud dari pesan yang sampai kepadanya. Namun, jika kita perhatikan bukankah ‘helo’ juga merupakan bagian dari salam?
Nah, untuk menjawab hal ini, kita akan bahas di bagian ke-tiga dari seri artikel blog “Berkenalan dengan Chatbot” yang akan membahas tentang bagaimana agar Bot kita lebih pintar dalam mengenali intent dan bagaimana mengintegrasikan Chatbot kita dengan messengger yang sudah ada.
Stories from the people behind the code — the engineers and product designers ofKata.ai
AI Enthusiast, Data Enthusiast, Technology Evangelist, Robotic Enthusiast, Public speaker, Mentor, Doing epic shit!http://id.linkedin.com/in/sofianhw",https://miro.medium.com/v2/resize:fit:1200/1*N1K6JiXdhED-WOss5sIQjQ.jpeg,"Chatbot, Bots, Artificial Intelligence, NLP, Machine Learning"
https://medium.com/s/story/implementation-of-semantic-segmentation-for-on-farm-honeydew-detection-43e2a028d4a4,Implementation of Semantic Segmentation for On-Farm Honeydew Detection,Mfatchur Rahman,2018-05-09T15:53:32.933Z,"Extending Vision Beyond Imagination
Melon merupakan komoditas pertanian potensial. Melon dengan varietas tertentu sepertiapollo,goldenataubright metaberpotensi menghasilkan pendapatan Rp.100–150jt per hektar hanya dalam dua bulan masa tanam.
Namun sayangnya, budidaya melon tergolong beresiko karena melon sangat rentan terhadap serangan penyakit. Inveksi yang awalnya hanya pada sejumlah tanaman, berpotensi untuk kemudian menyebar keseluruh populasi. Dalam prakteknya, tidak mungkin petani atau pakar sekalipun mengamati gejala penyakit pada tanaman satu persatu. Oleh karena itu, dibutuhkan bantuan mesin untuk melakukan deteksi dini serangan penyakit, yang juga dikenal dengan istilahsmart farming.
Dengan teknologi saat ini, monitoring setiap tanaman menjadi mungkin untuk dilakukan. Sebelum membahas keseluruhan sistem deteksi gejala penyakit pada melon, salah satu kemampuan yang diperlukan adalah segmentasi bagian-bagian melon (untuk kemudian dianalisa gejala penyakitnya). Tulisan ini akan menjelaskan proses segmentasi melon secaraon-farmdengan menggunakanneural networksecara lebih mendalam.
Untuk mencapai tujuan tersebut, dibutuhkandatasetpelatihan. Data yang akan digunakan berupa gambar-gambar yang diambil dari berbagai video dengan sudut pengambilan gambar dan pencahayaan yang beragam. Varietas melon pada kumpulan gambar yang digunakan sebagaidatasetjuga beragam, mulai darirock melonhinggagolden meta.
Tiap gambar dalamdatasetmempunyaimask imagesebagai pasangan.Mask imagemerupakan gambar biner dimana objek bukan melon direpresentasikan dengan piksel nol. Dalam penelitian ini, dataset terdiri dari 300 pasang gambar.
Langkah selanjutnya adalah mendesainneural networkyang akan digunakan. Dalam proyek ini, kita akan memodifikasiauto-encoder. Secara teoritik,auto-encoderterdiri dariencoderdandecoder.
Encoderbertujuan untuk meringkas fitur gambar.Encoderdirancang dengan cara mereduksiwidthdanheightgambar secara perlahan, sebaliknyadepthatau fitur gambar diperbanyak. Dalam kaitannya dengan melon, objek-objek tak penting seperti pepohonan, gulma, langit otomatis akan ter-suppress, sebaliknya objek target atau buah melon akan ditonjolkan.
Decoderbekerja dengan cara merekonstruksi ulang fitur yang sudah diringkas olehencodermenjadi gambar target. Arsitektur dari sebuah decoder akan sangat menentukan performa dan kualitas output yang dihasilkan. Secara umumdecoderbekerja dengan cara menambah ukuran spatial (widthdanheight)dari input secara bertahap melalui operasi konvolusi balik ataudeconvolution. Pada umumnya, operasideconvolutionmenggunakan nilai stride 2. Hal ini agar arsitektur yang dihasilkan lebih efisien dan tidak “membengkak” seperti hal nya dengan stride tunggal.
Meski begitu, operasideconvolutiondenganstrideslebih dari satu menghasilkan garis-garis tipis sepertigridpada gambar output. Untuk mengatasi itu, dapat digunakan teknikdeconvolutiondengan berbagai ukuran filter.Output dari setiapdeconvolutionini kemudian digabungkan menjadi 1. Dengan begitu, grid-grid halus pada gambar output dapat semakin menghilang karena tertutup oleh hasildeconvolutionlainnya.
Komponen terakhir dari arsitektur yang kita gunakan adalahresidual block. Output daridecoderakan diproses olehresidual blockuntuk menghasilkanmaskoutput.Residual blockterdiri dari beberapaconvolutional block. Tiapconvolutional blockmenghasilkanresidual image. Gambar output merupakan kombinasi dari residual-residual tersebut.
Dari arsitektur yang kita gunakan, dihasilkan satu output dan duaresidual layer(warna abu-abu). Formulasiloss functiondari kombinasi tersebut:
first loss= MSE(mask, first residual)second loss =MSE(mask, second residual)output loss =MSE(mask, output layer)total loss = a * first loss + b * second loss + c * output loss
Pada umumnya, nilai a,b, dan c adalah 0.01, 0.1, dan 1.
Preprocessing Input Images
Sebelum digunakan sebagaitraining input, nilai piksel dari gambar-gambar input yang sebelumnya bernilai antara 0–255, dikonversi menjadi nilai desimal antara 0–1. Cara ini dapat mengasilkanlossyang jauh lebih kecil padaepoch-epochawal, sehingga prosestraininglebih cepat konvergen.
Untuk prosestraining, digunakanadam optimizerdenganlearning rate0.0001.Learning rateadalah laju pembelajaran. Semakin besarlearning rate ,semakin cepat konvergensi tercapai. Hanya saja,learning rateyang terlalu besar mempersulit tercapainya titik optimum, jika daerah optimum tersebut terlalu curam. Selain itu,learning ratebesar juga memberikan grafiklossyang amplitudonya besar.
Sebaliknya,learning rateyang relatif kecil membuat pembelajaran relatif lebih lama, namun memungkin tercapainya titik optimum yang relatif curam dan grafiklossyang cenderung lebih halus.
Gambar disebelah kiri menunjukkan hasil darilayerterakhir sebelumlayeroutput. Gambar tersebut menampilkan nilai sebaran pikselnya, dimana regionberwarna biru adalahmaskobjek, sisanya adalahmaskuntukbackground.Gambar disebelah kanan merupakan output darinetworkyang kita gunakan. Output network merupakan aktivasi dari gambar sebelumnya. Dalam penelitian ini, aktivasi yang digunakan adalah fungsisigmoid (atau umum dikenal dengan istilahlogit layer).
Modifikasi terhadapmaskoutput dapat dilakukan untuk menghasilkan hasil berbeda.Masktersebut dapat digabungkan dengan gambar input (yang telah modifikasi), menghasilkan hasil sebagai berikut:
Untuk varietasgolden meta(kuning),
References1.Rethinking the Inception Architecture for Computer Vision,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf2.Resnet in resnet: generalizing residual architectures,https://arxiv.org/pdf/1603.08029.pdf%EF%BC%893.DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.https://arxiv.org/pdf/1606.00915.pdf4.U-Net: Convolutional Networks for Biomedical Image Segmentation.https://arxiv.org/pdf/1505.04597.pdf)%e5%92%8c%5bTiramisu%5d(https://arxiv.org/abs/1611.09326.pdf
Extending Vision Beyond Imagination",https://miro.medium.com/v2/resize:fit:891/1*3SK4-McyTrC2j-fF0l67qA.jpeg,"Deep Learning, Artificial Intelligence, Image Processing, Computer Vision"
https://medium.com/s/story/data-science-dan-peluangnya-di-era-digital-441090f6ce3d,Data Science dan Peluangnya di Era Digital,Makers Institute,2018-03-01T12:32:19.188Z,"Di saat trend dunia IT sedang diramaikan olehmachine learningdanArtificial Intelligence(AI), jangan lupakan pula bagaimana data analytics akan menentukan arah bisnis di masa depan. Menurut Forrester, seperti dikutip dari Forbes, pasar insight-as-a-service (IAAS) di 2018 akan meningkat dua kali lipat, dengan 80% perusahaan sangat tergantung pada insight yang disediakan olehinsight provider.
Artinya,data miningdan analisa terhadap data memiliki peran sentral dalam memberikan konteks terhadap arah perusahaan di era big data seperti sekarang ini. Maka, tak dapat dihindari, bahwa kebutuhan terhadapdata scientistjuga akan terus meningkat tajam.
Menurut riset McKinsey & Company, pada 2018 diperkirakan ada kebutuhan 140 ribu — 190 ribudata scientistyang harus dipenuhi. Dari catatan LinkedIn pada 2017, pekerjaandata scientistmengalami peningkatan 6.5 kali lipat daripada 5 tahun sebelumnya. Bahkan,data scientistjuga muncul menjadi salah satu lowongan teratas yang dibutuhkan oleh perusahaan, di sampingMachine Learning Engineers.
Data science sendiri adalah bidang keahlian yang mempelajari bagaimana memanfaatkan data-data yang terserak, baik yang terstruktur maupun tak terstruktur, untuk digunakan dan dianalisa sebagai pertimbangan untuk pengambilan keputusan. Menurut Direktur Penjualan SAS Indonesia Peter Sugiapranata, dalam perusahaan biasanya terjadi kesenjangan antara TI, data, dan bisnis. Nah,data scientistini adalah orang yang menjembatani ketiga hal di atas.
Tak hanya berguna untuk memberikan insight bagi perusahaan, data science juga bisa digunakan untuk kepentingan lain, seperti politik, misalnya. PoliticaWave sempat melakukan analisis data di Twitter, Facebok, berita online dan blog-blog terkait dengan pemilihan kepala daerah di Jawa Barat. Saat itu mereka memprediksi pasangan Aher-Deddy menjadi pemenang. Ternyata hasil analisanya tak berbeda jauh dengan hasilquick countberbagai lembaga survei.
Dan yang tak kalah menarik adalah besaran gaji yang bisa didapat oleh seorang data scientist. MenurutGlassdoor, gaji rata-rata seorang data scientist di Amerika Serikat, bisa mencapai US$ 110.000 atau sekitar Rp 1,46 miliar. Menarik bukan?
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:618/1*eDIFInFGKNHXYjChEqN_QQ.png,"Data Analytics, Data Science, Makers Institute, Data Analysis"
https://medium.com/s/story/statistika-dan-machine-learning-satu-ilmu-dua-wajah-445e019ce752,Statistika dan Machine Learning: Satu Ilmu Dua Wajah,Rizal Zaini Ahmad Fathony,2020-12-31T15:32:34.674Z,"Note: versi pdf dari tulisan ini bisa didownload di:https://rizal.fathony.com/assets/pdf/statistika_dan_machine_learning.pdf
Hampir semua orang yang bergelut di dunia ilmu pengetahuan sedikit banyak pasti pernah berinteraksi dengan statistika (ilmu statistik). Tidak dapat dipungkiri bahwa statistika mempunyai peranan penting sebagai katalis perkembangan ilmu-ilmu lain, baik ilmu alam (seperti astronomi dan biologi) ataupun ilmu sosial (seperti ekonomi, demografi, sosiologi, dsb.). Statistika dipakai oleh disiplin ilmu lain sebagai alat untuk mengambil kesimpulan, menguji hipotesis/teori, memahami fenomena, menganalisis eksperimen, menentukan keputusan, dsb.
Machine learning saat ini menjadi cabang ilmu pengetahuan yang populer dibicarakan di media. Didapuk sebagai salah satu cabang dari ilmu kecerdasan buatan (artificial inteligence), hampir semua orang pernah berinteraksi, memakai ataupun mendengar sistem komputer yang dibangun memakai teknik machine learning. Mulai dari melihat tag-otomatis foto di Facebook, menggunakan rekomendasi pencarian di Google, meng-klik rekomendasi produk sejenis di online shopping, menikmati servis email tanpa-spam, sampai dengan mendengar berita AlphaGo yang mengalahkan pemain profesional top di permainan Go.
Meskipun aplikasi dari disiplin ilmu statistika dan machine learning kelihatan sangat berbeda, dua ilmu tersebut sangat berkaitan. Baik statistika maupun machine learning merupakan ilmu tentang data. Teori-teori di disiplin ilmu statistika dan machine learning sebagian besar juga saling tumpang tindih. Hal ini bisa dilihat dari isi sebuah buku tentang statistika,“All of statistics: a concise course in statistical inference”(Wasserman, 2013) dan buku tentang machine learning,“Machine learning: a probabilistic perspective”(Murphy, 2012). Jika kita lihat sekilas kata-kata kunci dari daftar isi buku “All of statistics”, akan sangat susah mencari kata kunci yang tidak tercantum juga di buku “Machine learning”. Topik-topik mulai dari probability distribution, linear & logistic regression, maximum likelihood estimation, markov model, sampai Bayesian MCMC dibahas oleh kedua buku. Jadi, apa persamaan dan perbedaan kedua disiplin ilmu tersebut? Kita akan bahas lebih detail di tulisan ini.
Pondasi dasar dari statistika dan machine learning adalah ilmu teori peluang. Semua teknik-teknik dalam statistika dan machine learning dibangun di atas teori peluang, yang merupakan bahasa matematika untuk mengukur derajat ketidakpastian. Ilmu-ilmu dasar yang juga penting bagi statistika dan machine learning diantaranya adalah aljabar linear, kalkulus dan teknik optimisasi. Dalam mendalami statistika dan machine learning, latar belakang kuat di ilmu-ilmu dasar tersebut akan sangat membantu.
Disiplin ilmu statistika sudah dirintis sejak abad ke 17 di mana dasar-dasar teori peluang dirumuskan oleh berbagai ahli. Berbagai distribusi peluang seperti distribusi normal/Gaussian, dan distribusi Poisson dirumuskan di sekitar abad ke 18 dan 19. Pondasi dari teori statistika modern dituangkan oleh statistisi dari Inggris,Ronald Fisherdi awal abad ke 20. Kontribusi Fisher sangat penting dalam perkembangan statistika termasuk diantaranya adalah teknik estimasi terkenalmaximum likelihood.
Dibandingkan statistika, disiplin ilmu machine learning masih tergolong relatif lebih muda, baru dirumuskan di sekitar akhir abad 20. Machine learning didefinisikan sebagai disiplin ilmu yang mempelajari algoritma komputer yang bisa belajar (learn) tanpa harus diprogram secara eksplisit, melainkan dengan sendirinya belajar dari data. Karena machine learning juga merupakan ilmu yang menggunakan data, pondasi dasar machine learning banyak ‘meminjam’ dari disiplin ilmu statistika yang pada abad 20 sudah relatif lebih matang.
Kalau topik-topik yang dipelajari statistika dan machine learning kurang lebih sama, ilmu-ilmu dasar yang dibutuhkan-pun juga sama, lalu di mana letak perbedaan statistika dan machine learning? Perbedaan utamanya terletak pada fokus yang berbeda dari kedua disiplin ilmu tersebut. Statistika lebih fokus ke pengambilan kesimpulan dan interpretasi dari model, sedangkan machine learning lebih fokus ke penggunaan model untuk prediksi data baru. Untuk lebih jelasnya mari kita lihat contoh sederhana dari alur kerja praktisi statistika dan machine learning dalam memecahkan masalah.
Salah satu contoh problem sederhana yang sering dianalisis oleh praktisi statistika diantaranya adalah problem di mana peneliti mempunyai variabel dependen kategorik yyang ingin dianalisis dengan beberapa variabel independen x1, x2, … . Contoh aplikasinya misalnya analisis tentang kelulusan mahasiswa di mana y = 1 bermakna lulus dan y = 0bermakna tidak lulus. Variabel-variabel x bisa berupa profil mahasiswa, total jam belajar, jam tidur, keaktivan di kelas, dan variabel-variable lain yang mungkin berhubungan dengan kelulusan. Teknik yang sering dipakai untuk memodelkan problem seperti ini diantaranya modelregresi logistik.
Alur kerja praktisi statistika dalam membangun model regresi logistik biasanya dimulai dengan pengecekan asumsi, menjalankan estimasi parameter (maximum likelihood) dengan software, mengecek nilai parameter dan signifinasinya, dan memilih variabel yang signifikan. Untuk mengecek dan membandingkan model mana yang lebih bagus (variabel mana saja yang sebaiknya dimasukkan ke model), ukuran sepertiAkaike Information Creiterion (AIC)} akan digunakan. Setelah modelnya fix, praktisi statistika akan menganalisis interpretasi dari parameter yang dihasilkan, seperti variabel apa yang mempengaruhi kelulusan, apakah mempengaruhi secara positif/negatif, dan seberapa besar pengaruhnya (bisa dicek dari nilai parameter untuk variabel tersebut). Pada akhirnya akan praktisi statistika akan menarik kesimpulan tentang hubungan variabel-variabel tersebut dengan kelulusan mahasiswa.
Kita lihat contoh problem yang dihadapi praktisi machine learning dengan tipe yang mirip, satu variabel dependen kategorik y dan beberapa variabel independen x1, x2, … . Contoh aplikasinya misalnya untuk mendeteksi email spam. Nilai variabel y = 1 berarti email spam, y = 0 berarti bukan email spam. Variabel-variabel x berisi karakteristik dari email tersebut, misalnya jumlah kata di email, apakah email mengandung attachment, apakah ada gambar di email itu, atau apakah email mengandung kata-kata tertentu, misalnya kata “promo” atau “obat”.
Model regresi logistik juga termasuk salah satu model yang sering dipakai praktisi machine learning. Mereka juga melakukan estimasi parameter dengan maximum likelihood via software. Praktek menambahkanregularisasiseperti L1 (lasso) atau L2 (ridge) ke model sangat lumrah dilakukan di machine learning untuk meningkatkan generalisasi dari model. Praktisi machine learning biasanya tidak begitu fokus ke pemilihan variabel terutama jika jumlah variabelnya tidak banyak, semua variabel masuk ke model. Fokus pemilihan dan pembandingan model lebih ke bagaimana memilih konstanta regularisasi yang tepat. Untuk melakukan itu praktisi machine learning membagi dataset untuk modeling (training data) ke beberapa fold dan melakukan teknikcross validation. Model yang memiliki tingkat kesalahan prediksi paling kecil di tahap cross validation akan dipilih. Selanjutnya, model tersebut akan disimpan, di-deploy ke server dan dibikin programming interface yang nantinya akan dipanggil sekiranya ada email baru yang perlu diprediksi apakah spam atau bukan.
Dari dua alur kerja diatas, walaupun sama-sama menggunakan model regresi logistik, terlihat jelas perbedaan fokus dati praktisi statistika dan machine learning. Selain dari fokus yang berbeda (interpretasi versus prediksi), ada beberapa perbedaan yang terlihat. Diantaranya adalah bagaimana memperlakukan parameter hasil estimasi. Tidak seperti praktisi statistika, praktisi machine learning kurang peduli tentang nilai dari parameter-parameter tersebut. Ada preferensi di mana praktisi machine learning ingin agar nilai parameter-parameter tersebut relatif kecil dengan menambahkan regularisasi, namun berapapun nilainya tidak menjadi permasalahan, asalkan model tersebut bisa memprediksi data baru dengan akurat. Perbedaan yang lain ada dalam cara pemilihan model. Praktisi statistika cenderung memilih model berdasarkan teori analitik seperti AIC, sedangkan praktisi machine learning cenderung memilih model berdasarkan performa empirikal di tahap cross validation.
Apa konsekuensi dari perbedaan ini? Arah kompleksitas pengembangan yang berbeda dari statistika dan machine learning. Langkah selanjutnya setelah pemodelan regresi logistik bagi praktisi statistika akan berusaha mendapatkan kesimpulan dan interpretasi yang lebih pas, seperti dengan menganalisis lebih lanjut sumber variance dari model denganANOVA, atau memodelkan berdasarkan distrbusi peluang yang lain denganGeneralized Linear Model (GLM). Karena pentingnya interpretasi dari model yang didapat, praktisi statistika cenderung memilih model-model linear untuk analisis. Faktanya, GLM merupakan model yang paling populer di disiplin ilmu statistika.
Karena fokus peneliti machine learning lebih ke akurasi prediksi, semakin banyak informasi (variabel independen) yang masuk ke model, cenderung memberikan prediksi yang lebih bagus. Untuk kasus permodelan text, seperti contoh email spam di atas, variabel-variable yang lumrah dipakai oleh praktisi machine learning adalah diperoleh dari teknikbag-of-words. Dengan teknik ini, setiap kata atau kombinasi kata yang ada di kamus menjadi satu variabel yang nilainya 1 jika variabel kata tersebut ada di email, atau 0 jika tidak ada. Hasilnya, jumlah variabel bisa ribuan bahkan jutaan. Model linear juga kurang begitu banyak dipakai oleh praktisi machine learning karena akurasi prediksi akan cenderung meningkat dengan model non-linear. Non-linearitas bisa didapatkan dengan banyak cara, di antaranya adalah dengan menumpuk beberapa model linear ke beberapa layer seperti yang dilakukan di modelNeural Network, atau memproyeksikan variabel ke dimensi lebih tinggi seperti yang di lakukan oleh teknikkernel trickyang lumrah diterapkan di modelSupport Vector Machine (SVM). Kompleksitas model tidak menjadi masalah bagi praktisi machine learning asalkan bisa meningkatkan akurasi prediksi dan ada resource komputer untuk menjalankan estimasi model.
Meskipun alur kerja praktisi statistika dan machine learning begitu berbeda seperti contoh di atas, mereka sama-sama menggunakan model regresi logistik dan menggunakan teori dan rumus yang sama. Faktanya, banyak sekali teori yang sama-sama dipelajari baik di bidang ilmu statistika ataupun machine learning. Di bagian ini kita akan melihat beberapa contohnya. Dari sisi paling dasar misalnya, statistika dan machine learning sama-sama membahas konsep random variable, distribusi-distribusi statistik, expected value, variansi, sampai pada konsep distribusi prior dan posterior.
Teknik inferensi model parametrik denganmaximum likelihood estimation (MLE)juga dipelajari di kedua ilmu, sekaligus teori-teori MLE sepertikonsistensidansufficient statistic. Algoritma untuk mendapatkan estimasi MLE secara numerik seperti menggunakan metodegradient descentandQuasi Newtonjuga dipelajari di machine learning dan statistika komputasi. Teknik MLE untuk kasus di mana suatu model bergantung pada variabel yang tidak diobservasi melaluiExpectation — maximization (EM) algorithmjuga sangat populer di kalangan peneliti statistika dan machine learning, termasuk juga penerapan EM algorithm untuk estimasimixture model.
Model-model linear seperti regresi linear, regresi logistik, dan GLM, beserta variasi regularisasi dari model-model tersebut sepertiregressi lassodanregresi ridgedipelajari di kedua disiplin ilmu. Teknik reduksi variabel secara linear sepertiPrincipal Component Analysis (PCA)danIndependent Component Analysis (ICA)sering juga digunakan oleh praktisi kedua disiplin ilmu. Model linearSupport Vector Machine (SVM)yang mempunyai karekteristiksample sparsity–dimana parameter model hanya bergantung pada sebagian kecil dari data– sangat populer di bidang machine learning dan juga mulai dipelajari di bidang ilmu statistika.
Konsep-konsepBayesian statisticsmemegang peranan sangat penting baik di disiplin ilmu statistika dan machine learning. Banyak sekali model-model yang dipelajari di kedua ilmu yang menggunakan prinsip Bayesian statistics sepertiBayesian linear regression, Bayesian logistic regression, Bayesian GLM,Latent Dirichlet Allocation (LDA), dan beberapaBayesian non-parametric modelseperti Gaussian Process dan Dirichlet Process. Konsep-konsep distribution sampling sepertiimportance sampling,Markov Chain Monte Carlo (MCMC), danGibbs samplingsama-sama dipelajari di statistika dan machine learning.
Area-area lain yang sama-sama dipelajari di bidang ilmu statistika dan machine learning diantaranya,probability density estimation,model non-parametik,analisis cluster, danmodel Marckov chain.
Fokus yang berbeda dari disiplin ilmu statistika dan machine learning mengakibatkan arah konsentrasi yang berbeda pula dari kedua ilmu tersebut. Berikut ini adalah contoh-contoh area yang menjadi fokus peneliti statistika namun kurang didalami oleh peneliti machine learning. Sebagian besar dari area-area di bawah ini sangat penting untuk menunjang keakuratan pengambilan kesimpulan dan interpretasi model namun tidak begitu penting untuk prediksi.
Sampling (dari populasi). Teori pengambilan sampel (sampling) sangat penting peranannya pada fase pengumpulan data sebelum nantinya diproses dan dianalisis, terutama untuk aplikasi statistika di bidang ilmu sosial. Teknik pengambilan sampel yang benar akan memberikan garansi pada ke-valid-an penarikan kesimpulan yang nantinya akan diambil saat melakukan analisis lebih lanjut. Di sisi lain, data-data yang di olah oleh praktisi machine learning kebanyakan berupa data transaksional dimana tidak diperlukan pengambilan sampel. Sebagai contoh, kasus kasus klasifikasi email spam. Data untuk membentuk model didapatkan dari email-email sebelumnya. Tantangan di machine learning biasanya adalah kebanyakan dari data yang ada tidak mengandung label (variabel dependen y). Untuk kasus ini diperlukan input dari manusia untuk memberikan label ke data, sebagai contoh, memberikan label apakah suatu email adalah spam atau bukan.
Uji hipotesis. Pengujian hipotesis adalah salah satu aspek paling penting di bidang ilmu statistika. Praktisi statistika menggunakan teknik-teknik pengujian hipotesis untuk menarik kesimpulan apakah hipotesis awal yang mereka bentuk dalam suatu permasalahan didukung oleh data ataukah tidak. Pengujian hipotesis tidak banyak didalami peneliti machine learning karena fokus mereka yang lebih ke prediksi daripada pengambilan keputusan.
Analisis varians (ANOVA). ANOVA dan generalisasinya (seperti MANOVA, dan MANCOVA) merupakan tekniki yang dipakai luas di bidang ilmu statistika. Teknik ini menganalisa dan membandingkan variasi dari dua grup berbeda. Sebagaimana dengan uji hipotesis, analisis varians juga kurang didalami oleh peneliti machine learning.
Model-model linear advanced. Beberapa model linear yang lebih advanced dikembangkan oleh peneliti statistika untuk menganalisis lebih lanjut model yang lebih kompleks. Sebagai contoh adalahpath analysisdimana interaksi antar variabel dideskripsikan dalam bentuk graph. Contoh lainnya adalahsurvival analysisyang memodelkan rata-rata waktu sebelum suatu event akan terjadi. Peneliti statistika juga mengembangkan model linear yang lebih kompleks untuk menganalisis kasus-kasus dimana asumsi standar dari model yang sederhana tidak terpenuhi. Sebagai contoh adalahtwo stages least square regression, dangeneralized estimating equation (GEE).
Di sisi lain, banyak juga area-area yang menjadi fokus di disiplin ilmu machine learning, namun kurang didalami oleh peneliti statistika. Area-area tersebut sebagian besar berguna untuk meningkatkan akurasi dari prediksi namun mengakibatkan model yang dibentuk menjadi kompleks dan susah untuk di-interpretasikan. Berikut ini beberapa contoh diantaranya.
Kernel trick.Teknik ini terkait dengan proyeksi variabel. Berbalikan dengan teknik-teknik proyeksi variabel ke dimensi lebih rendah yang populer di statistika dan machine learning seperti melalui PCA, kernel trick berkaitan dengan proyeksi variabel ke dimensi lebih tinggi. Kernel trick memungkinkan sebuah model linear seperti regresi logistik atau SVM untuk mendapakkan non-linearitas dengan secara tidak langsung memproyeksikan variabel ke dimensi yang lebih tinggi (bahkan dimensi tak terhingga) tanpa harus melakukan transformasi variabel secara eksplisit.
Neural networksdandeep learning.Cara lain untuk mendapatkan non-linearitas adalah dengan menumpuk beberapa model linear ke beberapa layer. Teknik inilah yang dilakukan oleh neural network. Teknik deep learning yang sangat populer saat ini menggunakan banyak layer yang dimana di setiap layer berfungsi untuk membentuk representasi menengah yang lebih compact dari data. Beberapa teknik tambahan seperti parameter sharing,convolutiondanrecurrenceberperan untuk menambah keakuratan dari representasi tersebut dan menunjang kesuksesan applikasi deep learning di berbagai area seperticomputer vision (CV)dannatural language processing (NLP).
Inferensi semi-supervised.Di beberapa kasus, terkadang data yang tersedia sangat banyak, namun hanya sebagian kecil dari data tersebut memiliki label. Untuk mendapatkan label membutuhkan ahli yang biayanya mahal. Sebagai contoh adalah kasus untuk memprediksi apakah suatu review di website e-commerce asli dari pembeli ataukah palsu (misalnya komentar bayaran). Data untuk variabel independen (x) sangat banyak tersedia yang bisa didapatkan dari komentar-komentar yang sudah ada. Namun untuk mendapatkan label (y), perlu seorang ahli yang membaca dan menganalisis review untuk menentukan review tersebut palsu atau tidak. Teknik inferensi semi-supervised mencoba untuk mengikutsertakan data-data tanpa label ke dalam pembentukan model.
Probabilistic graphical models (PGM).PGM memodelkanconditional dependencydari koleksi beberapa random variable. PGM banyak dipakai oleh peneliti machine learning untuk menangainistructured prediction, kasus dimana model tidak hanya melakukan prediksi satu variabel y, tetapi melakukan prediksi vektor y yang mempunyai struktur. Sebagai contoh adalah kasus aplikasi di area NLP dimana kita diberikan satu kalimat sebagai isi dari variabel x dan harus mempredisksi tiap kata di kalimat itu apakah menjadi subjek, predikat, objek, ataukah keterangan. Beberapa model PGM yang populer diantaranya adalahhidden Markov model (HMM),conditional random field (CRF), danlatent Dirichlet allocation (LDA).
Peneliti disiplin ilmu statistika kebanyakan berada di Department of Statistics atau Department of Mathematics and Statistics, sedangkan peneliti machine learning kebanyakan berada di Department of Computer Science. Peneliti statistika rata-rata memulai belajar dari latar belakang matematika, sedangkan peneliti machine learning rata-rata memulai dari latar belakang algoritma.
Dalam menghadapi suatu masalah, peneliti statistika lebih melihat dari sisi formulasi matematika untuk memodelkan masalah. Peneliti machine learning, selain memperhitungkan model matematika, rata-rata juga memikirkan bagaimana peforma dari algoritma yang akan digunakan untuk mengestimasi model. Performa dari algoritma biasanya di ukur denganbig O notation. Sebagian besar publikasi riset peneliti machine learning mendeskripsikan algoritma estimasi model beserta performa/kompleksitas dari algoritma tersebut. Sebagai contoh adalah algoritma inferensi model latent dirichlet allocation (LDA) (Blei, et al., 2003) yang memiliki kompleksitas O(kN²), dimana N adalah jumlah sampel dan k adalah jumlah variabel. Artinya kurang lebih adalah, jika jumlah sampel bertambah menjadi dua kali lipat, algoritma tersebut akan berjalan 4 kali lebih lambat dari sebelumnya; dan jika jumlah variabel bertambah dua kali lipat, algoritma akan menjadi 2 kali lebih lambat juga.
Analisis komplksitas algoritma ini sangat penting untuk melihat apakah algoritma itu akan cocok di-implementasikan ke data yang lebih besar. Algoritma inferens yang memiliki kompleksitas O(N) tentunya akan lebih dipilih daripada algorima dengan kompleksitas O(N²). Hal ini dikarenakan jika jumlah sampel bertambah menjadi seribu kali lipat misalnya, algoritma dengan kompleksitas O(N) hanya akan menjadi lebih lambat seribu kali lipat juga, semisal yang awalnya memakan waktu 1 detik menjadi 1000 detik atau sekitar 17 menit. Di sisi lain, algoritma dengan kompleksitas O(N²) menjadi satu juta kali lebih lambat, dari 1 detik menjadi 1 juta detik atau sekitar 12 hari. Sangat terasa perbedaannya.
Selain dari perbedaan cara berfikir, terdapat perbedaan kultur dalam proses publikasi hasil riset peneliti statistika dan machine learning. Tempat utama untuk publikasi pengambangan riset statistika adalah di jurnal, sepertiAnnals of Statistics,Biometrika, danJournal of the American Statistical Association (JASA). Disisi lain, sebagai mana area riset lain di bawah Department of Computer Science, tempat utama publikasi riset oleh peneliti machine learning adalah di konferensi. Publikasi di konferensi-konferensi utama sepertiNeural Information Processing Systems (NIPS)danInternational Conference on Machine Learning (ICML)dan konefensi-konferensi lain (UAI,AISTATS,COLT) menjadi fokus utama peneliti machine learning. Dampaknya adalah progress pengembangan machine learning berjalan lebih cepat dari statistika, karena alur pemrosesan publikasi mulai dari submissionsampai penerbitan hanya memakan waktu beberapa bulan untuk konferensi, dibandingkan jurnal yang bisa memakan waktu beberapa tahun. Peneliti machine learning dituntut untuk mengejar deadline konferensi untuk menuangkan ide baru mereka, sebelum orang lain yang punya ide mirip mempublikasikannya. Jika ide yang sudah dipublikasikan di konferensi perlu penjelasan lebih detail, jurnal machine learning sepertiJournal of Machine Learning Research (JMLR)adalah tempatnya.
Bahasa pemrograman yang populer di kalangan peneliti statistika adalahR. Hampir semua peneliti statistika menggunakan R untuk menuangkan ide dan teori mereka. Untuk praktisi statistika, selain R, bahasa yang pupuler diantaranya adalahSAS,Stata, danPython. Di kalangan peneliti dan praktisi machine learning,Pythonmenjadi bahasa yang paling populer, disusul oleh C++ untuk implementasi model yang membutuhkan performa tinggi, MATLAB untuk implementasi ide secara cepat, danLuayang populer di praksisi deep learning. Bahasa R juga populer di kalangan praktisi machine learning. Sebagian kecil dari peneliti dan praktisi statistika dan machine learning menggunakan bahasa pemrogramanJuliayang memungkinkan implementasi ide secara cepat tanpa harus mengorbankan performa.
Seperti yang telah dibahas di bagian-bagian sebelumnya, disiplin ilmu statistika dan machine learning mempunyai banyak persamaan dan juga perbedaan. Kedua disiplin ilmu sama-sama berdasarkan teori peluang dan membahas dasar-dasar teori dan model yang sama. Perbedaan keduanya terletak pada fokus yang berbeda. Statistika lebih fokus ke arah pengambilan kesimpulan, sedangkan machine learning fokus ke prediksi data baru. Dari persamaan dan perbedaan tersebut, tidak salah kalau statistika dan machine learning disebut sebagai dua wajah berbeda dari satu kesatuan disiplin ilmu.
Blei, David M, Ng, Andrew Y, and Jordan, Michael I. Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan):993–1022, 2003.
Breiman, Leo et al. Statistical modeling: The two cultures. Statistical science, 16(3):199–231, 2001.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin, 2001.
Harrell, Frank. Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis. Springer, 2015.
Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.Wasserman, Larry. Rise of the machines.
Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.
Beberapa artikel di wikipedia",,"Ml Vs Stats, Statistics, Bayesian, Probability, Machine Learning"
https://medium.com/s/story/semangat-komunitas-yang-tak-pudar-4535a766bd3b,Semangat Komunitas yang Tak Pudar,AC,2018-02-22T03:18:59.024Z,"Berdiskusi dengan senior kampus itu tidak ada habisnya, senior yang satu organisasi yang kini telah berkerja, mendengarkan ceritanya rumah-kantor rumah-kantor terasa membosankan, dan akhirnya “sertijab sebentar lagi mas, ada organisasi diluar sana yang bisa diikuti ga ya?” pertanyaan menggantung itu membawa saya berlanjut kesini.
berawal dari berangkat ke kampus, anak semester tua yang kerjaanya hanya melihat buku buku untuk dijadikan referensi skripsi, iseng-iseng mengambil majalah komputer, hanya untuk sekedar melihat lihat apa yang baru, walau yang dilihat gambarnya saja. bolak balik majalah tersebut sampai berkali kali nemu tulisan tentang komunitas data science, kebetulan skripsi yang dikerjakan tentang data mining, dalam hati “relatable”.
tahun 2016, semester 7, tidak bimbang setelah lulus nanti akan kerja apa. tidak jauh jauh dari programming, pergi pagi pulang sore tidur begitu terus senin sampai jumat, sabtu minggu waktunya leha leha, aktifitas ini sudah bisa ditebak karena sewatu saya skripsian saya sudah bekerja, akhirnya komunitas yang saya ikuti, diundang lewat group telegram di bulan january 2017, ramai, hanya mengerti sedikit, tapi ada yang menarik, diskusinya matang dan serius sekali, bisa berjam jam mengikuti chat group telegram tersebut.
setelah saya pindah ke Jakarta, akhirnya saya memutuskan untuk ikut ikut meetup, tapi hanya meetup ini yang saya tunggu tunggu, Mei 2017, ketika ada meetup, saya tidak ikut karena bertepatan dengan hari pertama kerja saya. meetup baru ikutan. dalam kurun waktu itu juga saya jadi volunteer remote.
ada banyak cerita ditahun 2017, memutuskan untuk aktif berkontribusi di dunia ilmu data, sampai akhirnya jadi bagian training SQL dengan Big Query.
ikut komunitas ini sejalan dengan kerjaan saya, yang awalnya software engineer menjadi data analyst, bukan lulusan yang diharapkan memang, tapi saya sendiri merasa tidak begitu stress ketimbang jadi software engineer.
dan sampai akhirnya cerita diatas berlanjut sampai sekarang terus aktif dalam kegiatan meetup, workshop, tech talk di Jakarta.",https://miro.medium.com/v2/resize:fit:960/1*k5SC_Xl4JfrqnBhm_wnRgw.jpeg,"Data Science, Jakarta, Data Science Indonesia"
https://medium.com/s/story/belajar-machine-learning-pada-ios-dengan-core-ml-image-recognition-48af15d2a726,Belajar Machine Learning pada iOS dengan Core ML: Image Recognition,Khairil Ushan,2017-11-20T08:00:49.352Z,"Easy read, easy understanding. A good writing is a writing that can be understood in easy ways
Pada tulisan kali ini saya akan membahas salah satu Framework yang sangat menarik dari Apple, yaitu Core ML. Framework ini masih terbilang cukup baru mengingat ia dirilis pada acara WWDC 2017.
Tulisan ini akan bersifat lebih praktikal dalam modestep by stepbagaimana kita menggunakan Core ML di dalam project iOS kita.
Singkatnya Core ML merupakan framework yang disediakan Apple untuk memudahkan developer dalam mengintegrasikan model-model Machine Learning yang telah di-trainingsehingga bisa digunakan pada aplikasi iOS dengan mengandalkan resource yang ada pada perangkat iPhone. Apple meng-klaim bahwa Core ML telah dioptimasi sehingga bisa dijalankan dengan optimal pada perangkat iPhone yang telah beredar.
Gambar di bawah ini cukup merepresentasikan apa saja yang bisa kita lakukan dengan framework Core ML ini
Di sini saya menyebutkanmodel,tapi apakah sebenarnya model yang saya maksud di sini? Model merupakan sebuat representasi darirulesyang dihasilkan dalam proses yang disebuttrainingdengan menggunakan algoritma-algoritmamachine learning.
Terdengan sangat rumit dan kompleks. Ya, ini memang merupakan salah satu bagian dari machine learning yang cukup rumit. Dan kita tentu tidak akan membahas ini pada tulisan kali ini :).
Untuk mulai bermain dengan Core ML, Apple telah menyediakan beberapamodelsiap pakai. Kita bisa memperoleh model-model ini pada linkini
Pada tulisan ini saya akan menggunakan model berikut ini dalam project yang akan saya buat.
Bisa kita lihat pada deskripsinya bahwa model ini bisa kita gunakan untuk mengklasifikasikan Pepohonan, Hewan, Makanan, Kendaraan, dan lain-lain.
Jadi langsung saja kita mulai membuat project sederhana untuk melakukan Image Recognition.
Pastikan kita memilih(check) Target yang sesuai untuk file *.mlmodel yang baru saja kita masukkan ke project kita.
Bisa kita perhatikan pada GIF diatas setelah kita berhasil memasukkan model ke project kita, XCode secara otomatis meng-generateSwift class sebagai representasi dari model yang baru saja kita import.
Yang perlu kita perhatikan adalah bagian ini
Jadi model yang baru saja kita masukkan ini menerima input berupaImageyang mana harus memiliki size 224x224, dan akan menghasilkan output yang terdiri dari 2 field, yaitu nilaiprobabilitydan juga label.
Berikutnya kita akan mencoba menggunakan Swift class yang telah digenerate XCode berdasarkan VGG16.mlmodel yang kita import barusan. Untuk memberikan sedikit gambaran, jadi pada project kali ini kita akan membuat aplikasi sederhana seperti ini
Jadi disini kita memiliki
Kita akan mulai dengan kode seperti ini padaViewControllerkita
Berikutnya langkah pertama yang perlu kita lakukan untuk menggunakan framework Core ML adalah menambahkan baris berikut
Setelah kita kita perlu membuat instance dari Class Model yang di-generate oleh XCode tadi.
Nama classVGG16ini akan sesuai dengan nama file *.mlmodel yang kita import.
Selanjutnya kita akan menuliskan kode implementasi pada fungsiclassifyImage(_ image: UIImage)yang masih kosong pada kode di atas. Pada fungsi ini saya menerima parameter image dengan tipeUIImagekarena memang kita akan memperoleh sebuah objectUIImagehasil dariUIImagePickerController.
Berikutnya kita perlu memastikan resolusi image kita sesuai dengan ketentuan dari model yang kita punya, yaitu 224x224.
Pada kode di atas, saya melakukan scaling ke objectimageuntuk memperoleh size 224x224 dan menyimpan hasilnya ke object baru yaituscaledImage.
Inti dari tulisan ini ada pada fungsi berikut ini
Ini merupakan fungsi yang ada pada class VGG16 yang di generate XCode untuk kita. Bisa kita lihat bahwa fungsi ini menerima satu parameter dengan tipeCVPixelBuffer. Karena itu kita perlu meng-convertUIImagekita menjadiCVPixelBufferini.
Kita bisa membuatextensiondariUIImageuntuk proses ini.
Selanjutnya kita tinggal menggunakannya pada object UIImage yang kita punya sebelumnya
Bisa kita lihat pada kode di atas, kita akan memperoleh hasilnya pada objectoutputyang merupakan hasil dari fungsiprediction(image: CVPixelBuffer).
Berikut adalah hasil yang bisa kita dapatkan dengan kode seperti di atas yang mana sangat sederhana
Bisa kita lihat hasilnya cukup akurat, walaupun tentu ada yang tidak tepat seperti Garpu Makan yang dianggap Spatula. :)
Demikian dulu tulisan saya untuk Core ML kali ini. Semoga bisa saya lanjutkan untuk mencoba mengimplementasikan Core ML untuk kasus-kasus yang lain.
Untuk source code pada contoh dalam tulisan ini bisa diperoleh pada github repository ini >https://github.com/RR12/Simple-Image-Recognition
Easy read, easy understanding. A good writing is a writing that can be understood in easy ways
iOS Engineer @ Ice House Jakarta",https://miro.medium.com/v2/resize:fit:1200/1*HoJaMehslgtvyIF44PAznQ.png,"Machine Learning, IOS, Coreml"
https://medium.com/s/story/stratergi-era-digital-dengan-umc-48daeab0ac27,Strategi Era Digital dengan UMC,Poetra Akbar,2018-05-26T15:04:25.219Z,"Ketika sedang bertarung agar bisa mendapatkan sebuah kemenangan kita perlu namanya strategi yang pas dan sesuai kondisi serta keadaan. Seperti misalnya strategi perang untuk mengatasi musuh dengan jumlah pasukan dan senjata yang sangat minim. Strategi gerilya yang menggunakan pola serangan diam-diam dan mendadakan akan lebih efektif ketimbang strategi penyerangan terbuka.
Dewasa kini, ketika menjual satu produk atau jasa perlu adanya strategi pemasaran, dan juga dalam bisnis memerlukan strategi agar dapat ditawarkan memenuhi kebutuhan konsumen. Semakin tahu perkembangan era digital kini makin canggih,marketingtidak hanya pada kondisi yang nyata seperti : poster, koran, banner dan lainya dengan era digital sekarang kapan saja dan dimana saja bisa melihat informasi menggunakan komputer bahkan smartphone sekalipun.
Oleh sebab itu, platfrom yang bernama UMC (Universal Marketing Company) menghadirkan kecanggihan tersebut pada bidang strategi pemasaran. Dengan platfrom tersebut anda bisa merasakan kecangihan teknologi kecerdasan buatan yang didesain berkecepatan tinggi dan akurasi yang memungkinkan membuat iklan di pesan instan secara efektif.
Para pebisnis yang melakukan pemasaran, akan terasa sangat mudah dalam mengelolah iklan dalam pesan instan dibergai perangkat apapun. UMC juga mengintergrasikan ke dalam berbagai aplikasi yang sering digunakan pada smartphone anda seperti : WhatsApp, Wechat, Facebook Messeger, QQ, Kakaotalk, Viber, Snapchat,
Teknologi dari pada Platfrom UMC sendiri tidak perlu diragukan lagi untuk para pebisnis kecil maupun besar untuk melakukan pemasaran, karena didukung oleh :
1.Penyimpanan data yang besar
2.Intergrasi diberbagai instan messeger
3.Dibangun dalam modul pemrosesan pembayaran``
4.Universalitas dan kenyamanan
Pada platform UMC juga terdapat blockchain, mengapa ada blockchain ?Penggunaan teknologi tersebut dapat memecahkan banyak masalah. Yakni, kegagalan untuk mematuhi ketentuan kontrak antara pengiklan dan pemilik saluran (melalui penggunaan kontrak cerdas), serta masalah pembayaran dalam berbagai mata uang, karena pembayaran akan dilakukan di platform token. Oleh sebab itu, blockchain membuat semua proses menjadi cepat dan transparan. Pembelian token UMC dapat dilakukan sekarang di tahap awal ICO dan dapatkan bonus tertinggi! Syarat ketentuan berlaku
Adapun roadmap platfrom UMC
Team and concept of the UMC platform
Team building and development of the UMC product concept
Development of the UMC platform
Development of Beta 1 of the first platform product — Telegram BOT
(main functionality)
Development of Beta 2 of the first platform product — Telegram BOT
(additional functions and integration with fiat payment systems)
Development of GM for the first platform product — Telegram BOT (integration with blockchain)
August-September 2018
Development of the “Exchange“ functionality
October-December 2018
Development of WEB-interface of the platform
Development of the “Bots’ exchange”
Introduction and development of Artificial intelligence
Going to the Big Data
Scaling the UMC platform
November 2018 — January 2019
UMC on other messengers
Private sale and Pre-sale
ICO WEB-site development
Smart-contracts programming
Anda bisa melihat informasi resmi terkait platfrom UMC di sini :
— — — — — — — — — — — — — — —[01 July, 2018]— — — — — — — — — — — — — — —Twitter:https://twitter.com/UMCcoinOfficialFacebook:https://www.facebook.com/UMCcoin.ioTelegram:https://t.me/UMC_community_ENReddit:https://www.reddit.com/user/UMCcoinMedium:https://medium.com/@umcompany.mailSlack:https://umctechnologies.slack.comYouTube:https://www.youtube.com/watch?v=eg6Y4uVppTAWhitepaper:https://umccoin.io/files/whitepaper.pdfWebsite:https://umccoin.io
Demikian orginal artikel dari hasil analisa pribadi sendiri terhadap platfrom UMC.
__________________________________________________________
https://bitcointalk.org/index.php?action=profile;u=2133928",https://miro.medium.com/v2/resize:fit:750/1*h5sFBAri-PoV81aFjz4i5w.jpeg,"Cryptocurrency, Artificial Intelligence, Marketing, ICO, Blockchain"
https://medium.com/s/story/big-data-dengan-hadoop-hadoop-architecture-part-1-4a67f32d6cf8,Big Data dengan Hadoop (Hadoop Architecture) — Part #1,Farhan,2018-05-15T16:34:08.076Z,"Seperti yang telah kita ketahui, karakteristik utama Big Data ada 3, yaitu Volumenya besar, Kecepatan data yang dihasilkan tinggi, dan Macam-macam jenis datanya yang banyak.Data-data yang bermacam-macam ini dihasilkan tidak hanya oleh manusia (human-generated), tetapi juga jin hehe bercanda, maksudnya mesin (machine-generated). Contoh data yang dihasilkan oleh manusia yaitu seperti:
Sedangkan untuk data yang dihasilkan oleh mesin yaitu seperti:
Dari keseluruhan data yang ada biasanya data yang dihasilkan oleh mesin jauh lebih banyak daripada data yang dihasilkan oleh manusia.
Sebelum era Big Data yang fitri ini, data yang dihasilkan oleh mesin tidak disimpan. Alasannya karena volume terlalu besar dan tidak tahu cara menarikinsight-nya. Kemudian munculan teknologi Big Data, yang bertujuan sebagai jawaban atas keinginan untuk menyimpan data yang dihasilkan oleh mesin terutama data di web atau sosial media. Bayangkan saja jika data yang dihasilkan oleh mesin yang biasanya bisa mencapaiterabytedisimpan dengan RDBMS seperti MySQL atau PostgreSQL tentu akan membutuhkan biaya yang mahal. Belum lagi data yang dihasilkan bisa bermacam-macam strukturnya sepertiunstructured data, semi-structured data,danstructured data.Jadi skemanya tidak bisa diatur terlebih dahulu seperti pada RDBMS. Selain permasalahan tersebut juga apabila menggunakan server biasa yang digunakan untuk memproses data denganvolumebesar akan memakan waktu yang lama dan bisa menimbulkanbottleneck.Nah dari berbagai macam permasalahan tersebut munculahHadoopsebagai jawaban atas kebutuhan karakteristik Big Data.
Cara kerja Hadoop berprinsipcluster of nodes(kumpulan nodes/computer). Maksud dari prinsipcluster of nodesadalah — saya ilustrasikan — semisal kita menyimpan file pada satu computer yang mana computer tersebut bisa diisi dengan beberapahardisk, lalu apabila kita akan menyimpan file tambahan dan ternyatahardisksudah penuh maka kita tidak perlu meng-upgradekomputer server lama menjadi menjadi komputer server baru, melainkan cukup beli komputer baru yang diisi hardisk dan komputer lama pun tetap dipakai. Bisa kita bayangkan semisal kita punya satu sistem terintegrasi atau bisa kita sebutcluster, satu cluster ini mengkoordinir banyaknodesyang masing-masingnodememiliki bagianstorageatau penyimpanan dandata processingsendiri, itulah kenapa Hadoop disebut sebagaidistributed computingkarena tugasdata processingdilakukan pada tiapnode.
Ada 2 komponen utama pada Hadoop, yaituHDFS(Hadoop Distributed File System) danMapReduce. HDFS sendiri seperti sejarahnya terinspirasi dari GFS (Google File System), dan fungsi HDFS sendiri untuk menangani storage atau penyimpanan data yang bermacam-macam jenis file. Sedangkan MapReduce sendiri terinspirasi dariMapReduce Googleyang berfungsi sebagaidata processing.Cara kerjanya yaitu dengan prinsipMaster and Slave.Ada bagian yang bertugas sebagai Master dan ada yang bagian yang bertugas sebagai Slave, Master dan Slave sendiri adalah sebuah komputer. Apabila bagian Master terdapat 1 atau 2, sedangkan pada bagian Slave bisa terdapat sebanyak mungkin tergantung kebutuhan.Pada Master sendiri ada bagian yang namanyaNamenodedan adaJobtracker,dan pada bagian Slave terdapatDatanodedanTaskTracker. Tugas bagian Master adalah untuk mengkoordinasikan directory dan lokasi file block. Jadi di dalam Namenode (salah satu bagian dari Master, yang sebenarnya adalah bagian dari HDFS) tidak berisi data, melainkan bagian ini hanya menyimpan indeks lokasi data yang kemudian data tersebut dipartisi menjadi kecil-kecil lalu disimpan di masing-masingDatanode.Selain Namenode, pada bagian Master juga adaJobTrackeryang mengaturjob queu.Jadi apabila ada sebuah aplikasi, maka aplikasi tersebut akan ditangani oleh JobTracker kemudian JobTracker mengatur salah satu bagian Slave yaitu TaskTracker yang mengerjakan masing-masing partisi data dari task yang diberi oleh aplikasi.
Misal ada aplikasi yang bisa menganalisis seberapa banyak kata “korupsi” muncul pada sosial media, lalu aplikasi akan mengontak Master, kemudian JobTracker yang sebenarnya adalah bagian dari MapReduce membagi job dan mendistribusikan ke Slave untuk diproses. Lalu Namenode akan memberitahukan indeks lokasi file kepada aplikasi.
Intinya adalah data prosesing tidak dilakukan di Master, melainkan di masing-masing node yang jobnya didapat dari JobTracker.
Kelebihan lain dari cara kerja ini adalah ada salinan dari tiap data di masing-masing Slave/Node, atau saling tukar menukar salinan untuk menghindari apabila terjadi kerusakan pada Slave/Node. Sedangkan jika terjadi kerusakan justru ada di bagian Master, maka biasanya pada enterprise level disediakan 2 Master, yaitu Master Primary dan Master Backup untuk mengantisipasi terjadinya kerusakan pada Master Primary.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:720/1*DvD_tv1bFls1NodyN5z8Sw.png,"Big Data, Hadoop, Hadoop Architecture, Data Science, Machine Learning"
https://medium.com/s/story/buku-trilogy-dari-ernie-chan-4b3265a737a6,Buku Trilogy dari Ernie Chan,Welly Tambunan,2018-07-25T07:15:36.259Z,"Bacalah buku di atas sesuai dengan urutannya. Ketiga buku tersebut menggunakan MATLAB, berikut beberapa bahan untuk mempelajarinya
Selamat melakukan explorasi !",https://miro.medium.com/v2/resize:fit:256/1*EbkhrMbixITC9spKa9BIgw.png,"Quant, Trading Equity, Machine Learning"
https://medium.com/s/story/contoh-penerapan-artificial-intelligence-yang-populer-saat-ini-508851fca1d4,Contoh Penerapan Artificial Intelligence Yang Populer Saat Ini,TeknoIOT,2018-05-17T14:41:10.185Z,"Contoh Penerapan Artificial Intelligence Yang Populer Saat Ini— Kecerdasan Buatan atau Artificial Intelligence (AI) adalah cabang ilmu komputer yang menekankan pengembangan intelijen mesin, pola berpikir dan bekerja seperti manusia. Misalnya, pengenalan suara, pemecahan masalah, pembelajaran, dan perencanaan.
Hari ini, Artificial Intelligence (AI) adalah bahan diskusi yang sangat populer yang banyak dibahas di kalangan teknologi dan bisnis. Banyak ahli dan analisis industri berpendapat bahwa AI atau kecerdasan buatan adalah masa depan — tetapi jika kita melihat sekeliling, kita yakin bahwa itu bukan masa depan — itu adalah masa kini.
Dengan kemajuan teknologi, kita sudah terhubung dengan AI dalam satu atau lain cara — apakah itu Siri, Watson atau Alexa. Ya, teknologi ini sedang dalam tahap awal dan semakin banyak perusahaan yang menginvestasikan sumber daya dalam penelitian mesin, menunjukkan pertumbuhan yang kuat dalam produk dan penerapan teknologi kecerdasan buatan dalam waktu dekat.
Baca juga artikel menarik ini:
Tidak seperti persepsi umum,contoh penerapanArtificial Intelligence(AI) tidak terbatas hanya pada industri IT atau teknologi; sebaliknya, banyak digunakan di bidang lain seperti medis, bisnis, pendidikan, hukum, dan manufaktur.
Berikut ini, kami ulas 9 contoh penerapan kecerdasan buatan yang sudah gunakan saat ini,
READ Kecerdasan Buatan (Artificial Intelligence) AI? Definisi dan Kontroversinya
Siri adalah salah satu asisten pribadi virtual paling populer yang ditawarkan oleh Apple di iPhone dan iPad. Asisten yang diaktifkan sebagai suara perempuan ramah berinteraksi dengan pengguna dalam rutinitas sehari-hari. Dia membantu Anda menemukan informasi, mendapatkan petunjuk arah, mengirim pesan, melakukan panggilan suara, membuka aplikasi, dan menambahkan acara ke kalender.
Siri menggunakan teknologi pembelajaran mesin untuk mendapatkan pertanyaan dan permintaan bahasa alami yang lebih cerdas dan mampu memahami. Ini pasti salah satu contoh paling ikon dari kemampuan belajar pada mesin smartphone.
Tidak hanya smartphone tetapi mobil juga sudah bergeser ke arah Artificial Intelligence. Tesla adalah sesuatu yang meniadakan driver manusia. Ini adalah salah satu teknologi mobil terbaik yang tersedia sampai sekarang. Mobil ini tidak hanya mampu meraih banyak penghargaan tetapi juga fitur seperti mengemudi sendiri, kemampuan prediktif, dan inovasi teknologi mutlak.
Jika Anda seorang pecandu teknologi dan bermimpi memiliki mobil seperti yang ditampilkan di film-film Hollywood, Tesla adalah salah satu yang contoh teknologi mobil canggih.
Cogito awalnya didirikan oleh Dr Sandy dan Joshua adalah salah satu contoh terbaik dari aplikasi kecerdasan buatan versi perilaku untuk meningkatkan layanan pelanggan perusahaan. Perusahaan ini adalah sintesis pembelajaran mesin dan ilmu perilaku untuk meningkatkan kolaborasi pelanggan dengan para call center.
Cogito digunakan pada jutaan panggilan suara yang dilakukan setiap hari.Contoh penerapan Artificial Intelligencedengan menganalisis suara manusia dan memberikan panduan untuk memberikan pelayanan maksimum.
Netflix tidak memerlukan pengenalan — ini adalah layanan konten-on-demand yang sangat populer yang menggunakan teknologi prediktif untuk menawarkan rekomendasi berdasarkan reaksi, minat, pilihan, dan perilaku konsumen. Teknologi ini memeriksa dari sejumlah catatan untuk merekomendasikan film berdasarkan kecintaan dan reaksi Anda sebelumnya.
READ Kecerdasan Buatan (Artificial Intelligence) AI? Definisi dan Kontroversinya
Aplikasi ini menjadi lebih cerdas setiap tahun. Satu-satunya kelemahan dari teknologi ini adalah film kecil akan luput dari perhatian sementara film-film besar tumbuh dan menyebar di platform. Tapi seperti yang saya tulis sebelumnya, itu masih meningkat dan belajar menjadi lebih cerdas.
Pandora adalah salah satu solusi teknologi artificial intelligence yang paling populer dan sangat detil. Ini juga disebut DNA musik. Tergantung pada 400 karakteristik musik, tim musisi ahli secara individual menganalisis lagu tersebut. Sistem ini juga bagus dalam merekomendasikan rekam jejak untuk merekomendasikan lagu yang tidak pernah diperhatikan, meskipun disukai orang.
Nest adalah salah satu startupcontoh penerapan Artificial Intelligencepaling terkenal dan sukses dan diakuisisi oleh Google pada tahun 2014 seharga $ 3,2 miliar. Nest Learning Thermostat menggunakan algoritme perilaku untuk menghemat energi berdasarkan perilaku dan jadwal Anda.
Ini menggunakan proses pembelajaran mesin yang sangat cerdas yang mempelajari suhu yang Anda suka dan program itu sendiri dalam waktu sekitar satu minggu. Selain itu, secara otomatis akan mati untuk menghemat energi, jika tidak ada orang di rumah.
Bahkan, ini adalah kombinasi keduanya — kecerdasan buatan serta Bluetooth rendah energi karena beberapa komponen solusi ini akan menggunakan layanan dan solusi BLE.
READ Kecerdasan Buatan (Artificial Intelligence) AI? Definisi dan Kontroversinya
Boxever adalah perusahaan yang sangat bergantung pada pembelajaran mesin untuk meningkatkan pengalaman pelanggan dalam industri perjalanan dan menyampaikan momen mikro atau pengalaman yang dapat memuaskan pelanggan.
Boxover secara signifikan meningkatkan keterlibatan pelanggan melalui pembelajaran mesin dan Kecerdasan Buatan untuk mengatur lapangan bermain, membantu pelanggan menemukan cara baru dan membuat perjalanan yang tak terlupakan.
Drone sudah mengirimkan produk ke rumah pelanggan — meskipun dalam mode uji coba. Mereka menunjukkan sistem pembelajaran mesin yang kuat yang dapat menerjemahkan lingkungan ke dalam model 3D melalui sensor dan kamera video.
Sensor dan kamera dapat melihat posisi drone di ruangan dengan menyambungkannya ke langit-langit. Algoritma generasi lintasan memandu drone tentang bagaimana dan kemana harus bergerak. Dengan menggunakan sistem Wi-Fi, kita dapat mengendalikan drone dan menggunakannya untuk tujuan tertentu — pengiriman produk, pembuatan video, atau pelaporan berita.
Alexa diluncurkan oleh Amazon, yang semakin pintar dan menambahkan fitur baru. Ini adalah produk revolusioner yang dapat membantu Anda mencari informasi di web, mengatur janji, berbelanja, mengontrol lampu, switch, termostat, menjawab pertanyaan, membaca audiobook, melaporkan lalu lintas dan cuaca, memberikan info tentang bisnis lokal, memberikan skor dan jadwal olahraga , dan lainnya menggunakan Layanan Suara Alexa.
Beberapacontoh penerapan Artificial Intelligencesemakin populer hari ini; mempengaruhi cara kita hidup, berinteraksi dan meningkatkan pengalaman pelanggan. Masih banyak yang akan datang di tahun-tahun mendatang dengan lebih banyak perbaikan, pengembangan, dan penerapannya.
Source:www.teknoiot.com
Referensi Teknologi Terbaru",https://miro.medium.com/v2/resize:fit:1024/0*hhJLjV7ftwT4773J.,"Artificial Intelligence, Tech, AI, Teknoiot, Teknologi"
https://medium.com/s/story/machine-learning-for-web-developers-50923bb36d91,Machine Learning for Web Developers,Riza Fahmi,2018-08-20T13:57:20.691Z,"Berikut adalah cuplikan video sesi yang saya bawakan di eventProduct Development Conference TechInAsia 2018di Juli 2018 yang lalu. Kali ini saya membawakan topik yang sedanghappening, tentang Machine Learning.
Sebenarnya kepikiran membawakan topik ini karena pas diundangFacebookke acaraF8 Developer Conferencebeberapa bulan yang lalu. Disana saya bertemu idola sayaEric Nakagawadan dia menyarankan saya dan developer di Indonesia untuk segera belajar tentang Machine Learning. Karena menurutnya, Artificial Intelligence dan Machine Learning adalah masa depan.
Untuk cerita lengkapnya bisa bacaartikel mediumyang saya tulis dalam bentuk serial (bagian 1,bagian 2,bagian 3danbagian 4).
Tujuan utama dari sesi ini sebenarnya adalah untuk mengajak teman-teman semua belajar bareng sebenarnya :) Saya juga baru mulai belajar Machine Learning dan akan sangat membantu sekali jika ada teman-teman yang kebetulan membaca artikel ini juga tertarik untuk belajar. Karena belajar bareng lebih keren dan lebih efektif. Jadi kalau teman-teman tertarik belajar Machine Learning dan Artificial Intelligence, kirim email ke saya dan mari kita belajar bersama :)
Artikel ini dirangkum dari episode ke-19RandomScreencast.
Co-founder, Curriculum Director atHacktiv8.com. Podcaster atCeritanyaDeveloper.com. Screencaster atRandomScreencast.com",,"Randomscreencast, Indonesia, Conference, Web Developer, Machine Learning"
https://medium.com/s/story/uea-tunjuk-menteri-kecerdasan-buatan-pertama-di-dunia-515b9943ff7a,UEA Tunjuk Menteri Kecerdasan Buatan Pertama di Dunia,HSE HIMA IF Telkom University,2017-11-03T03:42:00.122Z,"Uni Emirat Arab menjadi negara pertama di dunia yang memiliki jabatan unik pada kabinetnya, yaitu Menteri Kecerdasan Buatan. Jabatan baru ini diemban oleh pemuda berusia 27 tahun bernama Omar Sultan Al-Olama yang sebelumnya menjabat sebagai Kepala Direktur Departemen Masa Depan.
Omar Al-Olama juga sebelumnya telah menjabat sebagai Executive Committee of the World Government Summit (WGS), organisasi yang menjadi sarana berdiskusi seputar inovasi dan kemajuan teknologi di lingkungan negara-negara bagian Uni Emirat Arab sejak 2014.
Omar Al-Olana diangkat langsung oleh Perdana Menteri dan Wakil Presiden Uni Emirat Arab, Sheikh Mohammed bin Rashid Al Maktoum. Sheikh juga mengumumkan pada Twitternya “Hari ini kami mengumumkan penunjukan MenteriArtificial Intelligence(kecerdasan buatan). Gelombang global di masa mendatang merupakan kecerdasan buatan dan kita ingin agar UEA lebih bersiap untuk itu,”.
Dibentuknya jabatan baru ini, diharapkan dapat meningkatkan produktivitas Uni Emirat Arab. Selain jabatan Menteri Kecerdasan Buatan, Uni Emirat Arab juga telah menunjuk dua jajaran menteri baru di kabinetnya yaitu Menteri Kebahagiaan dan Kesejahteraan yang diemban oleh Ohoud bint Khalfan Al Roumi serta Menteri Kemajuan Ilmu Pengetahuan yang diemban oleh Sara Al-Amiri.
Tugas Sara Al-Amiri sebagai Menteri Kemajuan Ilmu Pengetahuan adalah untuk mewujudkan misi penelitian menuju planet Mars sesuai dengan program ‘a 100-year national program’.
Dirangkum dari: Kompas Tekno
Hubungan Sosial Ekstra Kampus — Himpunan Mahasiswa S1 Teknik Informatika Universitas Telkom",https://miro.medium.com/v2/resize:fit:1200/0*0tcpsVOf61dct53V.jpg,"United Arab Emirates, Minister, Artificial Intelligence"
https://medium.com/s/story/jangan-sampai-kita-ketinggalan-kereta-machine-learning-itu-51e8d76c249c,Jangan Sampai Kita Ketinggalan Kereta Machine Learning Itu,Abdul Somat Budiaji,2017-11-05T09:54:33.977Z,"Sebuah upaya untuk mendokumentasikan proses belajar saya mengenai pembelajaran mesin (Machine Learning) dari bukuAn Introduction to Statistical Learning with Applications in R.
Di acara Google I/O 2017 CEO Google Sundar Pichai menyatakan bahwa Google akan bertransformasi menjadi AI-first (Artificial Intelligence) company. Hal ini mengirim isyarat bahwa kecerdasan buatan akan menjadi ujung tombak untuk mengeruk pundi-pundi keuntungan. Pertanyaannya, bagaimana dengan perusahaan dalam negeri? Saya yakin upaya itu sudah berjalan dengan banyaknya lowongan yang relevan dengan Artificial Intelligence dan Machine Learning, dan juga munculnya beberapa komunitas terutama di Jakarta.
Bagi orang bekerja di perusahaan teknologi tentu soal mudah untuk meyakinkan level eksekutif untuk berinvestasi di Artificial Intelligence. Beda cerita dengan seorang kawan yang bekerja di sektor yang lebih konservatif seperti asuransi. Butuh upaya ekstra bagi dia untuk meyakinkan level eksekutif agar mau menerapkan Artificial Intelligence. Padahal aplikasi Artificial Intelligence ini bisa sangat masif dan merambah ke berbagai sektor. Awal bulan lalu ada berita tentang sebuah perusahaan yang memanfaatkan kecerdasan buatan dan kamera smartphone yang sepertinya sudah menjadi kebutuhan pokok untuk mendeteksi jenis penyakit yang menyerang tanaman ketela.
Sebenarnya sebagian teknik-teknik yang ada di dalam Machine Learning sudah saya pelajari waktu kuliah dulu dan dipakai di dalam komunitas akademik untuk melakukan penarikan kesimpulan dan menemukan pola-pola dari data. Waktu itu tidak terlihat seksi mungkin karena tidak terlihat aplikasinya dalam kehidupan sehari-hari seperti yang terlihat saat ini. Mungkin waktu itu saya juga masih belum fasih dalam menggunakan bahasa pemrograman apapun. Kini saya mencoba mempelajarinya kembali melalui buku An Introduction to Statistical Learning with Applications in R. Implementasi Machine Learning akan dilakukan menggunakan bahasa pemrograman R yang mana salah satu pilihan populer untuk melakukan pekerjaan Machine Learning.
Dalam belajar saya akan melakukan studi kasus seperti yang ada di dalam buku, dari kompetisi Kaggle yang mana banyak perusahaan multinasional minta tolong kepada pakar yang berada di Kaggle untuk menyelesaikan persoalan Machine Learning mereka, dan contoh nyata dari bisnis atau organisasi di Indonesia kalau ada data yang dapat digunakan.
Garis besar yang akan dipelajari kurang lebih seperti di bawah ini dan tidak menutup kemungkinan perubahan. Dalam beberapa waktu ke depan daftar ini akan diperbaharui dengan tautan ke tulisan yang terkait namun saya akanselowsesuai waktu yang tersedia.
Tinggal di Indonesia. Tertarik pada Distributed Systems dan High Performance Computing. Sedang mempersiapkan misi mencari pohon tertua di Indonesia.",,"Artificial Intelligence, Machine Learning, Indonesia"
https://medium.com/s/story/membuat-model-machine-learning-dengan-algoritma-naive-bayes-5217cc85df57,Membuat Model Machine Learning dengan Algoritma Naive-Bayes,Farhan,2018-04-20T11:51:11.228Z,"Machine Learning adalah bagian yang tak terpisahkan dariData Scienceterutama ketika membahas mengenaipredictive analytics.Pada dasarnyaMachine Learningmirip konsep proses belajar pada manusia, hanya saja manusia belajar dari pengalaman, danMachine Learningbelajar dari data.
Lalu bagaimanaMachine Learningbelajar?
Ada beberapa kategori metode pembelajaran padaMachine Learning,namun yang paling sering ditemukan adalahUnsuperviseddanSupervised.Begitu pula dengan algoritmanya untuk masing-masing metode, ada banyak sekali macamnya.
Seperti padaUnsupervisedada algoritma yang sering ditemui yaituSVD, PCA, K-meansuntukClustering and Dimensionality reduction,dan adaApriori, FP-GrowthuntukAssociation Analysis.Sedangkan padaSupervisedada algoritmalinear regression, polynominal regression, decision trees, random forestuntuk Regression, dan untukClassificationada algoritmaKNN, Trees, Logistic regression, Naive-Bayes, dan SVM. Sebenarnya masih banyak yang lain hanya saja algoritma tersebut yang paling sering ditemui pada umumnya.
Pada tutorial kali ini saya akan membuat gambaran singkat mengenai proses pemodelanmachine learningmelalui sebuah pembuatan modelMachine Learningdengan metodeSuperviseduntuk klasifikasi dari sebuahdatasetbernamaHandwritten Digits Datasetdengan algoritmaNaive-Bayes.
Jadi, nantinya model tersebut dapat mengenali tulisan tangan angka dari 0 sampai 9. Seperti kita ketauhui setiap orang punya ciri tersendiri dalam menulis angka, dengan modelmachine learningakan membuatmachinetetap dapat mengenali angka yang ditulis meskipun dengan gaya yang berbeda-beda pada setiap orang.
Untuk mengikuti tutorial ini minimal Anda bisa memahami bahasa pemogramanPython.
Tanpa membuang lebih banyak waktu lagi langsung saja kita mulai ya, hhe..
Handwritten Digits Datasetsudah tersedia dalampackagesklearn (Scikit-learn)yaitu sebuah modul khusus yang berkaitan dengan proses pemodelan sebuahMachine Learning.Jadi cukup gunakancommandload_digits()untuk mengimpor dataHandwritten Digits Dataset. Dan apabila kita lihatshapedaridatasettersebut bisa kita lihat ada 1797sampledengan masing-masingsampleberukuran 8 x 8pixel.
Di Python semua data prosesing dilakukan dengan angka.
Jadi apabila kita melihat isi daridigits.imagesakan menghasilkanmatrix of numbersyang didapat dari sekumpulan gambar-gambar atau masing-masing gambar yang akan diproses dan sudah diekstrak fitur atau variabelnya menjadi sebuahmatrix of numbersseperti di bawah ini.
Tidak hanya gambar, apabila akan memproses data dipythonmaka semua data baikunstructured data, structured datamaupunsemi-structured dataakan di-ekstrak terlebih dahulu menjadi sebuahmatrix of numbers.Baik file suara, video, maupun teks.
Karena kita akan melakukan pemodelan dengan metodeSupervisedjadidatasettersebut harus dipisah terlebih dahulu antara input ataufeature matrix (X)dengan output atautargetlabel (Y).Sesuai dengan syarat metodeSupervisedyaitu modelMachine Learningdilatih untuk mengenali sebuah label.
Proses selanjutnya yaitusplittingbaik dari X ataupun Y dibagi menjadi 2, menjadiXtraindanytrainuntuktraining,Xtest dan ytestuntuktesting model.
Selanjutnya kita gunakan algoritmaNaive-Bayesdari dalam modulsklearnuntuk prosestraining.
Lalu selanjutnya, karena model ini dibuat untuk klasifikasi maka ada uji akurasi skor dengan dataytestyang sudah disiapkan di atas.
Nah, dari uji di atas menghasilkan skor0.8333333333333334atau 83%.Sebenarnya masih bisa di-improvesupaya hasil skor menjadi lebih baik, sepertipreprocessing datasetyang akan saya jelaskan nanti. Jadi, cukup sekian gambaran pemodelanmachine learning,sampai jumpa lagi ya, hhe..
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:650/1*x8-oUwlxD-jIhG4r-DXG3Q.jpeg,"Naive Bayes, Artificial Intelligence, Terminator, Data Science, Machine Learning"
https://medium.com/s/story/jasa-promosi-produk-hiasan-dari-limbah-telp-wa-0822-4069-7469-promosi-produk-hiasan-dari-barang-52ea12b401c5,"Jasa Promosi Produk Hiasan Dari Limbah, Telp / Wa 0822–4069–7469, Promosi Produk Hiasan Dari Barang Bekas",basirul akbar,2018-08-18T03:01:35.150Z,"Internet marketing memang kelihatannya mudah dan gampang dalam pandangan beberapa orang yang masih sawang sinawang kata orang jawa, padahal pada prakteknya masih banyak juga yang jualannya masih belum mencapai tarket yang diinginkannya. Baik secara rupiah apalagi secara omset dan profit.
Maka ukurlah kemampuan otak, waktu dan tenagamu hingga uang yang Anda keluarkan untuk terus ber explore dan mencari jalan keluar dari rimba internet marketing yang sangat luas ini.
Bagaimana produk yang Anda buat bisa terjual dan laris manis hingga #BanjirOrder dan sampai #TsunamiOrder serta membuat Anda kebingungan disebabkan kebanyakan order dari pembeli yang melihat no hanphone Anda dari internet dan postingan status yang Anda buat menarik dan menjadikanClosing Tiap HARIbahkan bisa sampai Tiap waktu yang terus menerus.
Semua proses penjualan akan di jelaskan dan di arahkan sesuai dengan karakter mesin pencari ataupun secara pola pikir dari facebook dan juga media online yang lainnya. Dimana Anda dapat memahami dan menjalankannya sendiri nantinya di dalam bisnis yang Anda kelola bersama pasangan Anda ataupun bersama Team Marketing di perusahaan Anda.
Bagaimana saya dapat menjamin atau memberikan garansi jika Anda memang mengikuti pelajaran dan melakukan praktek yang harus dilakukan selesai bertatap muka dalam kelas atau bisa di rumah Anda langsung tanpa mengganggu kegiatan rutin Anda.
Dalam Implementasi praktek internet marketingnya akan saya pandu dengan pengarahan dan bimbingan selayaknya anak PAUD yang belajar membaca dan menulis dengan #KaidahPiranhamas yang memang telah terbukti dalam hal mendidik, mengajar, dan mengarahkan hingga terjadi closing yang vertubi-tub sampai#OrderanmuMbludak
Menjadi internet marketing itu harus tetap menjalin silaturahmi dengan pebisnis yang lain hingga tercipta sinergy dalam kehidupan nyatanya.
Jangan hanya mau di depan laptop atau komputer saja. Sebab bagaimanapun banyaknya uang yang dapat kita peroleh dari internet tetap juga kita membelanjakan hasil jualannya di pasar real dan saling berbagi dengan pengusaha yang lain.
Informasi Lebih jelasnya, silakan kontak
Bapak Achmad Solihin Prajamas
( CEO PonTren IT & Automotive Madinatul Iman )",https://miro.medium.com/v2/resize:fit:280/1*SvfYOGdbV39vTIa6WXbjFA.jpeg,"Management, Marketing, Marine, Maintenance, Machine Learning"
https://medium.com/s/story/import-library-ke-python-dan-r-56292a0fd398,Import Library ke Python dan R,R. Kukuh,2018-02-19T02:55:02.692Z,"Berikut adalah tutorial super simple tentang bagaimana meng-import libraries yang kita butuhkan pada Python / Spyder dan R / RStudio:
Buka Anaconda Navigator (belum punya?Installdulu dong)
Ketikkan kode berikut dalam editor:
Jangan khawatir dengan tanda ⚠️. Itu hanya peringatan (bukan error).
Jalankan kode tersebut hingga dalam console muncul pesan import sukses
Proses import library dalam RStudio dilakukan tanpa menuliskan kode apapun, tetapi cukup dengan memilih library mana yang akan dipakai lewat tab Package
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*W8V0WNJjT6HiwWsMMl3ipQ.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/jemma-menggali-wawasan-konsumen-melalui-teman-virtual-56fcba73a1d1,Jemma: Menggali Wawasan Konsumen Melalui Teman Virtual,Rizyan Irawan,2018-07-10T13:20:28.721Z,"Visi Unilever untuk Memahami Konsumen Indonesia Secara Mendalam
Untuk terus meningkatkan daya saing di pasar yang selalu berevolusi, raksasa industri FMCG ini memiliki visi untuk memahami perilaku konsumennya di platform digital, sekaligus berupaya memberikan pengalaman yang menyenangkan agar tetap relevan di mata konsumennya.
Memanfaatkan potensi aplikasi perpesanan merupakan prioritas utama Unilever, dan mereka ingin menjadi brand pertama di Indonesia yang memanfaatkan chatbot berbasis AI (kecerdasan buatan) ke dalam strategi pemasaran mereka.
Kami menemukan beberapa tantangan yang harus dihadapi untuk menyukseskan visi Unilever, di antaranya:
Jemma, Sahabat Virtualmu
Kami menawarkan chatbot dengan persona sahabat perempuan yang cerdas dan jenaka. Chatbot ini dirancang untuk menghibur sekaligus membangun persahabatan melalui obrolan santai.Chatbot ini diberi nama “Jemma”; persona perempuan dengan kepribadian ceria yang asyik diajak bicara. Jemma mampu melayani berbagai macam topik percakapan, mulai dari sapaan sederhana hingga topik yang lebih dalam seperti rekomendasi makanan, budaya populer Indonesia, hingga hubungan asmara.
Penuhi Janji Sebagai Sahabat
Selain menjawab obrolan ringan, fitur utama yang Jemma tawarkan adalah zodiak mingguan, tips dan trik untuk perempuan, serta berita dari dunia hiburan. Seiring dengan pertumbuhan jumlah pengguna yang pesat dalam tiga bulan pertama, kami terus mengamati kedekatan yang terbangun antara pengguna dan Jemma. Yang menarik, sejumlah pengguna bahkan merasa cukup nyaman untuk membahas cita-cita dan masalah pribadi mereka dengan Jemma.
Melihat hal tersebut, kami memutuskan untuk menambahkan sebuah fitur di mana pengguna bisa berkeluh kesah dengan Jemma. Sebagai sahabat baik, Jemma akan mendengarkan dengan setia dan tak lupa memberikan ucapan penyemangat kepada penggunanya. Walaupun sudah jelas sejak awal bahwa Jemma hanyalah sebuah chatbot, hal ini tidak menyurutkan keinginan pengguna untuk terus berinteraksi dengan Jemma, karena bagi sebagian pengguna, percakapan yang terjadi terasa seperti berbicara dengan seorang yang nyata; seorang sahabat.
Jemma sangat efektif sebagai salah satu saluran pemasaran digital untuk produk dan acara dari Unilever.
Dalam waktu kurang dari setahun, Jemma berhasil menghimpun lebih dari 1.5 Juta sahabat yang mengirimkan lebih dari 50 juta pesan dalam 17 Juta sesi percakapan. Pada bulan Oktober 2017, Jemma memenangkan penghargaan “The Most Interactive Brand Chabot” dari LINE Corporation.
Melalui Jemma, Unilever dapat memahami konsumennya secara mendalam dan melakukan pemasaran yang terpersonalisasi kepada jutaan konsumen.
Dengan dukunganKata Platformdan Enterprise Solution dari Kata.ai, perusahaan-perusahaan dengan tantangan serupa dapat bereksperimen dengan solusi chabot.
Ingin membuat chatbot untuk bisnis Anda? KunjungiKata.ai
Read the English versionhere",https://miro.medium.com/v2/resize:fit:1200/1*kaAsNIKYn4-W9wyxmqBLgQ.jpeg,"Chatbots, Teknologi, Artificial Intelligence, Indonesia"
https://medium.com/s/story/jual-parfum-cinta-coklat-telp-wa-0822-4069-7469-menjual-parfum-coklat-5701722598d8,"Jual Parfum Cinta Coklat, Telp / Wa 0822 4069 7469, Menjual Parfum Coklat",basirul akbar,2018-08-16T06:00:13.807Z,"Dijamin bikin pasangan kamu makin sayang, makin cinta dan makin nempel parfum cinta yang telah terkenal karena mampu meningkatkan gairah pasangan. Parfum-cinta-choirose-lengket-terus
Parfum Wanita ini dibuat dari bahan pilihan dan non alkohol sehingga halal digunakan. Manfaat parfum cinta tidak hanya untuk wanita tapi juga untuk pria, agar dapat meningkatkan kemesraan dengan pasangan.
Haaaiii… Apa ini masalah kamu?
- Belum Punya Pasangan
- Suami / Istri “Dingin” di ranjang
Siapa sih yang gak mau pasangannya lengket terus sepanjang hari? Baik pria maupun wanita pasti mau kalau pasangannya nempel terus sepanjang hari. Dengan wangi parfum cinta choirose yang khas, maka impian Anda dan pasangan akan terwujud.
Parfum cinta Choirose merupakan parfum yang dibuat dengan bahan khas dan khusus untuk memberikan wangi yang menarik pasangan Anda. Cokok digunakan oleh pria maupun wanita, ini parfum wanita disukai pria.
Untuk mendapatkan semua itu harga parfum cinta sangatlah murah dibanding dengan kemesraan yang Anda peroleh. Testimoni dari pengguna parfum cinta sangat memuaskan, semua puas dengan efek jatuh cinta dari pasangan masing-masing.
Karena Kami Punya Solusi Terbaik Untuk Anda!
PARFUM CINTA CHOIROSE
Parfum Cinta Choirose adalah parfum NON alkohol yang terbuat dari inti bunga mawar yang di-import dari spanyol. Aroma parfumnya lembut dan membuat orang semakin betah merasakan aroma wangi parfum cinta ini.
Walaupun dipakai berkali-kali, parfum cinta ini tidak membuat kita semakin bosan, justru semakin lama semakin jatuh cinta dengan parfum ini.
Aroma parfum cinta yang lemut dan awet membuat kamu menjadi percaya diri dan semakin bergairah. Aroma yang lembut parfum cinta ini membuat orang disekitar kamu menjadi sangat nyaman bersama kamu, bahkan bisa sampai jatuh cinta kepada kamu.
Kenapa kamu harus coba Choirose Parfum???
Karena Parfum Ini Dapat Meningkatkan Gairah Pasangan Anda. Jangan biarkan kemesraan anda dan pasangan berkurang dengan seiringnya waktu. Gunakan parfum ini untuk menjaga kemesraan tetap tahan lama.
ORDER SEKARANG JUGA Sebelum Kehabisan.
Bapak Achmad Solihin Prajamas
Pakai CHOIROSE BERESIKO:
“ Bisa bikin Suami Makin Betah Di Rumah, Di Cium Suami Tiap Hari, Makin Di Sayang Pasangan, Bahkan Yang Dulunya Cuek Jadi Lebih Perhatian “",https://miro.medium.com/v2/resize:fit:300/1*Ta3Hs1oybfYncxAY1bmwzQ.jpeg,"Management, Marketing Strategies, Marketing, Machine Learning, Marriage"
https://medium.com/s/story/linear-regression-5c41cdaf1bfd,Linear Regression,Eko Wahyu S,2018-03-04T16:35:18.894Z,"Simple Linear Regression
Linear regressionadalah alat statistik yang dipergunakan untuk mengetahui pengaruh antara satu atau beberapa variabel terhadap satu buah variabel. Variabel yang mempengaruhi sering disebut variabel bebas, variabel independen atau variabel penjelas.
Dari deskripsi diatas kita sudah dapat mengetahui tentang kegunaan / fungsi dari algoritma regresi linear. Pada part 1 ini saya akan mencoba memcahkan suatu masalah sederhana menggunakan regresi linear. Untuk memulainya saya menggunakan Python sebagai bahsa pemrogramannya dan Scikit-learn sebagai library yang akan saya gunakan untuk membantu masalah komputasi didalamnya. Jadi tools yang akan kita gunakan yaitu Python , saya asumsikan kalian sudah menginstall tools yang kita akan gunakan tersebut, jika belum dapat dilihat dari web resminya python yaituhttps://www.python.org/. Ok sekarang kita mulai dari studi kasus yang sudah saya sediakan .
Dari data diatas terdapat Dimater(inchies) dan Price($), yang kita akan lakukan yaitu menguji keterkaitan antara diameter pizza terhadap harga pizza ,apakah berpengaruh atau tidak ? kita anggap diameter pizza sebagai variable X dan Price sebagai variable Y.
Terlebih dahulu kita memetakan data tersebut menggunakan grafik agar enak dilihat dan dipahami , kurang lebih seperti berikut scriptnya :
Tahapan kedua yaitu memodelkan data menggunakan algoritma regresi linear yang rumusnya yaitu :
Y’ = Variabel dependen (nilai yang diprediksikan)
X = Variabel independen
a = Konstanta (nilai Y’ apabila X = 0)
b = Koefisien regresi (nilai peningkatan ataupun penurunan)
dimana untuk mencari nilai a dan b yaitu menggunakan rumus sebagai berikut :
Jika kita melakukan komputasi menggunakan pemrograman yaitu sebagai berikut :
1. pada baris 1 dan 2 kita import function yang ada pada library scikit-learn sudah kita install sebelumnya
2. X dan Y merupakan dataset yang sudah kita siapkan ,yaitu X mewakili diameter pizza dan Y mewakili harga pizza
3. variabel model kita gunakan untuk menampung function / turunan dari linear regression
4. variabel result disini kita gunakan untuk memanggil model linear regression dan kita akan memprediksi jika besarnya diameter pizza sebesar 12 inci maka berapakah prediksi harga yang akan kita dapatkan jika berdasarkan dataset yang kita miliki.
setelah kita jalankan code diatas maka akan tercetak hasil yaitu sebesar $13.68.
Kesimpulan yang kita dapat dari part 1 ini yaitu dengan menggunakan model linear regression kita dapat memprediksi suatu hubungan dari variabel satu ke variable lainnya, terbukti dengan jika kita membuat pizza dengan diameter 12 inci maka akan berpengaruh terhadap kenaikan harga pizza tersebut. seperti dalam dataset yang kita miliki yaitu diameter pizza berpengaruh terhadap harga pizza.
Masih banyak yang harus dibahas mengenai linear regression seperti kita harus menghitung cost function / loss function dalam linear regression. Cost function digunakan untuk menentukan dan mengukurkesalahan model Perbedaan antara harga yang diprediksi oleh model dan harga pizza yang teramati dalam rangkaian pelatihan disebut residu atau pelatihan kesalahan /error. Intinya yaitu untuk menghitung seberapa akurasi dan seberapa error algoritma yang kita pakai untuk memodelkan suatu kasus seperti diatas. Untuk pembahasan mengenai cost function / error function akan saya lanjutkan ke part berikutnya. Terimakasih
Software engineer @DKatalis",https://miro.medium.com/v2/resize:fit:468/1*aaHoCMfR3Qwi8kyr7VeI-A.jpeg,"Linear Regression, Machine Learning"
https://medium.com/s/story/analiesys-bar-chart-5ccc74707bbd,AnaLieSys : Bar Chart,Yodi D,2018-07-13T14:17:02.217Z,"Pak Blangkon adalah lurah di desa Wakanda Wetan. Suatu hari Pak Blangkon didatangi sekretarisnya dengan tergopoh-gopoh. Dengan nafas tak teratur dia bilang, “gawat Pak Lurah, gawat”. Kemudian ia menyodorkan selembar kertas, isinya adalah perbandingan gaji Pak Blangkon dengan lurah sebelah, Pak Dengklek. Pak Blangkon naik pitam, ternyata gaji dia menurut mereka jauh betul dibandingin Pak Dengklek. Berikut isi kertasnya :
Pak Blangkon marah, akhirnya beliau berencana melabrak Bupati Wakanda soalnya beliau yang mengatur masalah gaji lurah-lurahnya.
Di tengah jalan, Pak Blangkon bertemu Didi Dukun. Didi Dukun heran, kenapa wajah Pak Blangkon seperti tidak biasanya, sangat jutek sekali. Pak Blangkon orangnya ember, dia ceritakan kalau gaji dia terpaut jauh dengan Pak Dengklek, beliau tidak terima, mau melabrak Pak Bupati. Didi Dukun penasaran, ia bertanya, memangnya sejauh apa ? Pak Blangkon menyodorkan lembaran kertas tadi.
Sebagai dukun di Wakanda, Didi ternyata tidak kaget. Didi Dukun yakin ini hanya karena ketidaktelitian Pak Blangkon. Didi menjelaskan bahwa bedanya gaji beliau berdua cuma beda 1 juta, itu pun Pak Dengklek 1 tahun lebih lama masa baktinya daripada Pak Blangkon, dan itu wajar di Wakanda. Didi Dukun jelaskan juga bahwa Pak Blangkon tertipu oleh bar chart ini dan kurang teliti. Karena bar chartnya di-deliver (apa ya, dipersembahkan gitu ? hahaha) bukan dari angka 0, tapi dari 19 juta, jadinya kesannya terlihat beda jauh, padahal kalau dari 0 bedanya sedikit.
Agar lebih jelas, akhirnya Didi Dukun memperbaiki bar chart tersebut. Dengan bantuan para jin, ini merupakan tugas yang mudah. Dengan bahasa Parsertongue-nya Didi Dukun merapalkan mantra, Pertamina Pertamax! Poof!, bar chart tersebut diperbaiki menjadi seperti ini :
Pak Blangkon angguk-angguk, lalu nyengir. Pak Blangkon pun ngaku kalau sebenarnya di tidak teliti. Dia hanya melihat bar-nya saja, tidak melihat angka di samping. Dari barnya kelihatannya dobel, dia kira gaji Pak Dengklek dobel daripada dia.
Pak Blangkon pulang dengan hati lega.
Lalu ngepret sekretarisnya.
Sekretarisnya mencret.
Originally published atcacatannya.wordpress.comon June 7, 2018.
Enthusiasm Enthusiast",https://miro.medium.com/v2/da:true/resize:fit:700/0*38bN7dANTiD2Gu90,"Data Science, Visualization, Analysis, Analytics, Fraud"
https://medium.com/s/story/doctor-smart-ecosystem-revolusi-perawatan-kesehatan-digital-6029e3e437de,Doctor Smart Ecosystem — Revolusi Perawatan Kesehatan Digital,Crypto Tonigh,2018-07-21T11:13:48.782Z,"Sangat disayangkan jika kita sakit dan perlu berobat namun tidak mengerti yang sebenarnya bahwa tubuh kita perlu dicek kesehatannya meski tubuh kita tidak sakit,pernah tidak kita memikirkan hal yang demikian,di indonesia sendiri telah disediakan puskesmas yang ada di setiap desa dan kota,anda akan dilayani dengan sepenuh hati oleh mereka,hanya melakukan pemeriksaan rutin sebulan sekali dan melakukan aktifitas olahraga serta makan makanan yang sehat anda akan terhindar dari sakit.
Saat ini adalah era rovolusi dari teknologi digital,seperti yang anda tau,teknologi digital telah mengalami banyak kemajuan,maka dari itu para Dokter melakukan riset dengan berbagai macam alat maupun teknologi terbaru demi menunjang kegiatan dan akatifitas pelayanan mereka.
Apakah anda tau jika teknologi kesehatan yang selama ini mahal menggunakan teknologi yang sangat canggih,yah tentu saja mereka menggunakan teknologi AI engan menyimpan pengalaman klinis dari jutaan dokter didunia,tentu ini sangatlah hebat bukan.namun anda perlu tau bahwaSergei Chernevingin menyatukan teknologi Blockchain dan AI.
Sergei Chernevadalah seseorang yang bertanggung jawab atas efisiensi di Doctor Smart, dia adalah penasihat teknologi untuk proyek tersebut. Sergei memiliki lebih dari dua dekade pengalaman mengembangkan sistem informasi layanan massal dan solusi pemrosesan pembayaran. Dia berbicara kepada kami tentang kecerdasan buatan,alasan untuk mempercayainya,dan dampak blockchain dalam perawatan kesehatan.
Doctor Smart adalah platform blockchain untuk layanan kesehatan digital dan kesehatan.Platform ini bertujuan untuk memberikan jutaan orang akses ke perawatan kesehatan dan kesejahteraan yang berkualitas.ini berarti tim telah melakukan berbagai hal untuk menjadikan dunia medis lebih transparan,dimana mereka akan meberi informasi,akses,sertifikasi dan proses cepat untuk para pelanggannya.
Platform ini akan menciptakan ekosistem kesehatan dari semua pihak yang terlibat untuk memperoleh manfaat dan pada saat yang sama menyediakan manajemen layanan kesehatan berkualitas tinggi kepada pengguna.Semua golongan dapat mengakses semua layanan untuk melakukan konsultasi denga harga yang terjangkau oleh semua golongan Ekonomi.Dokter Smart akan langsung menghubungkan pasien dengan Dokter spesialis dari seluruh dunia melalui panggilan suara atau video.Dengan melakukan ini, pengguna platform Dokter Smart akan menjawab semua pertanyaan terkait kesehatan mereka dalam waktu singkat dari spesialis yang sangat berkualitas. Semua catatan riwayat kesehatan pasien akan disimpan di blockchain dan tersedia sepanjang waktu dari mana saja di dunia tanpa ada kesempatan untuk hilang atau dipalsukan. Kecerdasan Buatan bergabung dengan pembelajaran mesin, dan jaringan saraf akan menyediakan diagnostik pribadi yang lebih canggih dan memberikan layanan medis yang lebih baik bagi pengguna platform. Pasien, spesialis, perusahaan Insurensi, klinik medis, dan pemerintah akan dapat berpartisipasi dalam Doctor Smart Ecosystem dan meningkatkan bisnis mereka atau kesehatan mereka.
Seperti yang telah di sebutkan diatas.Platform Doctor Smart memfasilitasi ekosistem terintegrasi untuk semua pihak yang relevan dengan konsultasi medis, mulai dari dokter hingga pasien hingga perusahaan asuransi dan banyak lagi. Pelaksanaan kontrak cerdas memungkinkan kami untuk memperkenalkan layanan telemedicine ke program asuransi, sambil mengurangi biaya untuk menyediaan konsultasi online dengan mengotomatisasi sebagian besar proses untuk semua pihak.
Berikut ini adalah yang harus anda lakukan Apabila memiliki pertanyaan tentang kesehatan :Dapatkan bantuan cepat dan berkualitas. Anda tidak perlu lagi mencari informasi di Internet atau menunggu dokter.
Tim Dokter Smart telah mengembangkan sistem multiuser cloud selama 15 tahun. Sebagai hasilnya,saat ini tim memiliki akses eksklusif ke solusi teknologi tinggi yang unik, yang berlaku untuk bidang kedokteran digital, yang dibuat dengan institusi medis paling terkemuka di Dunia dan telah menerima sertifikasi internasional JCI. Sistem ini telah diuji dalam praktek oleh klinik layanan lengkap, yang saat ini menggunakan mereka untuk merawat pasien.
Layanan kesehatan merupakan sebuah layanan yang sangat dicari di setiap negara,saat ini anda dapat melihat negara maju dimana layanan kesehatan global adalah salah satu pasar dunia multi-triliun USD terbesar, dan Doctor Smart berencana untuk memposisikan diri sebagai salah satu pemimpin dalam segmen konsultasi medis terdesentralisasi. Mereka berencana membiayai pengembangan masa depan melalui ICO,yah ini adalah sebuah tujuan untuk tercapainya perkembangan platform ini kedepan.ICO memiliki hardcap $ 50 juta (dan softcap $ 5 juta). Pada 16 Mei, setelah pra-penjualan pribadi berakhir, penjualan token publik akan dimulai. Penjualan publik akan berlangsung hingga 30 Juli. Jumlah pembelian minimum akan ditetapkan pada 0,1 ETH.Anda dapat mengunjungi websitehttps://doctorsmart.io/untuk detail penjualan dan bonus pembelian token DSTT.
Nama Token : DSTT Harga
DSTT Hard Cap : $ 15 juta
Informasi Peta Jalan
Informasi yang saya berikan adalah sebagian dari informasi resmi Proek ini,baru baru ini tim telah melakukan banyak acara dan menghadiri konferensi di Dubai (World Blockchain Forum) dan Tokyo,Cicago.Tim benar benar sangat bersemangat karena mendapat dukungan dari banyak orang,Tim Dokter Smart juga berencana akan menghadiri beberapa acara di Cina,Inggris dan lokai lainnya,informasi lebih lanjut tentang Mitra dan informasi lainnya silahkan kunjungi:
WEBSITE : https://doctorsmart.io/https://doctorsmart.io/
WHITEPAPER :https://doctorsmart.io/whitepaper_eng.pdf
FACEBOOK :https://www.facebook.com/DoctorSmartEng/
TWITTER:https://twitter.com/DoctorSmartEng
TELEGRAM :https://t.me/DoctorSmart_ICO
MEDIUM :https://medium.com/doctorsmart
Penulis : Reok_timur
Profil Bitcointalk :https://bitcointalk.org/index.php?action=profile;u=1563616
Originally published atmatauangdigital06.blogspot.comon July 21, 2018.
Cryptocurrency | follow me i'll follow you back",https://miro.medium.com/v2/resize:fit:400/0*RZVypkpuWjFw3V4S.png,"Cryptocurrency, AI, Blockchain"
https://medium.com/s/story/deep-learning-convolutional-neural-network-cnn-menggunakan-package-keras-di-r-610d7a048f0d,Deep Learning : Convolutional Neural Network (CNN) Menggunakan Package Keras di R,akademi-ai,2025-02-27T04:09:46.618Z,"We are official team of Akademi AI Indonesia
Pada tulisan kali ini saya akan membahas tentang gimana sih cara nerapin algoritma Convolutional Neural Network (CNN) di R menggunakan pacakge keras dan teman temannya.. tulisan ini ngga banyak membahas apa itu CNN secara dalem, cuman ngebahas cara nerapinnya pake software R aja.. mungkin temen-temen bisa belajar tentang CNN lebih dalem lagi di artikel lain aja yaakkk hehe… atau mungkin nanti saya akan bikin sendiri tunggu aja hahaha… udah dulu ya basa basinya kita langsung to the point aja :)
Jadi Convolutional Neural Network adalah salah satu jenis neural network yang biasa digunakan pada data image/gambar. CNN bisa digunakan untuk mendeteksi dan mengenali object pada sebuah image. yuk kita coba aja di R..
Tutorial ini kita akan melakukan klasifikasi buat bedain dua kelas yaitu mobil dan motor.. data yang kita gunakan yaitu 200 gambar masing-masing 100 untuk gambar mobil dan 100 untuk gambar motor.. Semua script dan gambar yang saya pake temen-temen bisa downloadDISINI
Pertama.. install package yang di butuhkan !!! kemudian jalankan pake fungsi library() buat aktifin package tadi..
Kemudian setting path penyimpanan data gambar tadi.. saya disini nyimpan di E dengan nama folder test yang berisi 200 gambar mobil dan motor.. kemudian folder hasiltest buat nyimpen hasil proses-proses kita nanti..
Melakukan setting width dan height dari semua gambar.. disini kita pake width 100 dan height 100
Lalu membuat sebuah looping untuk melakukan preprocessing terhadap semua gambar
Intinya looping diatas berfungsi untuk melakukan preprocessing gambar.. preprocessing tersebut dimulai dengan melakukan penangkapan gambar, memberi nama, membaca, resize, dan menyimpan gambar..
Lalu jalankan perintah di bawah ini untuk membaca gambar, melihat output gambar yang sudah terbaca, melihat struktur dan melihat dimensi gambar yang ditentukan..
Kemudian melakukan pemisahan data menjadi data train dan data test dari 200 data. Kali ini kita akan membagi data train menjadi menjadi 160 data gambar dan sisanya data test sebanyak 40 gambar. script di bawahnya berfungsi untuk melihat data train pada urutan nomor 5 dalam bentuk csv pada folder hasiltest.
Setelah itu melakukan resize kembali semua data train dan data test dalam bentuk 32x32 pixel.
setalah resize gambar, lalu melakukan combine/mengikat data train dan data test. pake sintaks ini !
nanti hasilnya bakalan jadi kaya gini untuk train dan testnya yang masing-masing berjumlah 160 dan 40 gambar.
Selanjutnya adalah reorder dimension. disini kita akan membalik dimesi dari data train dan test yang sebelumnya untuk data train berdimensi 32x32x3x160 menjadi 160x32x32x3 dan test 32x32x3x40 menjadi 40x32x32x3.
Langkah berikutnya menentukan response. Pada bagian ini kita memberikan target yaitu 0-80, 1–80 untuk data training dan 0–40,1–40 untuk data testing.
Yang terakhir buat preprocessing data adalah one hot encoding.. disini kita membuat label untuk data training dan testing dengan fungsi to_categorical.
Naaah.. setelah semua preprocessing data image kita lalui bersama wkwk.. selanjutnya kita membuat model atau susunan jaringan Convolutioal Neural Network yang akan kita bentuk. Disini saya menggunakan 4 layer convolusi yang diikuti dengan layer maxpooling, layer dropout, layer flatten, layer dan dense.
Summary modelnya hasilnya ini… Temen temen coba cari tau yaa maksud model ini apa :))
Nah skarang kita lakukan training yaa pake 30 epoch/iterasi dan batch size 32..
Dari hasil iterasi diatas lumayan tuh kita dapet nilai akurasi sempurna yaitu 1 dan nilai loss yang rendah sebesar 0.0091 sampia akhr itersi ke-30. Ini artinya model sudah sangat bagus untuk kita terapkan untuk lasifikasi image motor dan mobil dan ini dia hasil plot proses trainingnya…
Nah setelah itu kita lakukan validasi dari data train menggunakan data test. kita akan mengukur seberapa akurat model yang sudah kita bangun untuk melakukan prediksi terhadap data testnya sendiri.
Dari hasil evaluasi diatas model sudah mampu memprediksi cukup baik untuk data training.. model hanya melakukan kesalahan prediksi sebanyak 9 gambar dengan nilai loss yang cukup rendah dan dengan akurasi sekitar 94%
Hasil evaluasi dan prediksi untuk data test memberikan hasil yang kurang baik karena sekitar 13 gambar salah di prediksi oleh model..
Nah sekarang kita akan mencoba untuk melakukan prediksi menggunakan data baru mobil dan motor, apakah nanti nanti berhasil di prediksi dengan baik atau nggak.. disini kita akan meakukan pengujian menggunakan 10 gambar mobil dan 10 gambar motor… langsung aja yaa let’s goo.. hehehe
Script diatas adalah untuk melakukan preprocessing terhadap 20 data baru agar bisa di olah oleh model yang telah kita buat. Langkah pertama adalah menyimpan model yang telah buat di folder E denga nama file model hasiltest.hdf5.. Setelah itu membuat folder dengan nama uji yang berisi 20 gambar masing-masing 10 gambar mobil dan motor, lalu menjalankan model yang sudah disimpan, membaca gambar, mengubah gambar dalam bentuk matrix, melakukan resize gambar dengan ukuran 32x32 pixel, melakukan combine/mengikat gambar, mengubah dimensi dari gambar, dan yang terakhir memberikan pelabelan.
Untuk melihat hasil prediksi ari 20 gambar yang akan kita uji, gunakan scrip ini !!
Berikut ini adalah hasil prediksinya….
Kita liat dari hasil prediksi diatas 9 gambar berhasil di prediksi sebagai mobil sedangan 1 gambar di prediksi sebagai motor dan 8 gambar berhasil di prediksi sebagai motor dan 2 gambar di prediksi sebagai mobil..
Okey teman teman sekian dulu untuk tutorial kali ini yaa.. Untuk tulisan saya yang lain tentang Deep Learning dengan berbagai Case, temen-temen bisa lihat di link link dibawah
We are official team of Akademi AI Indonesia
Medium Publicationhttps://medium.com/akademi-ai",https://miro.medium.com/v2/resize:fit:331/1*ipaLSKaCOlNDEBAJ4dDGWw.png,"Deep Learning, Convolution Neural Net, Artificial Intelligence"
https://medium.com/s/story/instalasi-anaconda-python-dan-r-studio-r-62ed1906822a,Instalasi Python/Anaconda dan R/RStudio,R. Kukuh,2018-02-13T03:44:45.650Z,"Dua bahasa pemrograman terbaik untuk urusan Science adalah Python dan R, sedangkan tools dari mereka berdua yang terbaik untuk urusan Machine Learning adalah Anaconda untuk Python dan RStudio untuk R.
Berikut adalah langkah-demi-langkah instalasi dua bahasa pemrograman ini beserta tools-nya.
Tidak ada yang istimewa, install Anaconda yang sudah ter-download.
Buka aplikasi Anaconda Navigator
Dari situ Anaconda Navigator tadi, cari dan buka aplikasi sypder
Dalam window Editor, Anda bisa menulis kode Python, misalnya:
Highlight kode di-line 8, lalu tekan ⌘+Enter untuk mengeksekusinya. Hasilnya akan muncul di window Console.
Tidak ada yang istimewa, install R package yang sudah ter-download.
Tidak ada yang istimewa, install RStudio yang sudah ter-download.
Buka aplikasi RStudio
Dalam window Console, ketikkan kode berikut, lalu tekan Enter untuk mengeksekusi:
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*Xv-WxI5L9HXfcW0y5bLwMg.png,"R, Rstudio, Python, Data Science, Machine Learning"
https://medium.com/s/story/cara-kerja-google-translate-65b48c583d39,Cara Kerja Google Translate,Ron Ashrovy,2017-10-02T05:46:14.789Z,"Setiap harinya kita menggunakan teknologi yang berbeda tanpa mengetahui bagaimana cara kerjanya. Pada faktanya, tidak mudah mudah untuk mengerti engine untuk machine learning. Teamstatbotingin membuat machine learning terasa jelas dimata masyarakat melalui blog ini. Harini , kami memutuskan untuk meng-explore dan mejelaskan machine learning translator terutama pada kinerja algoritma Google Translate.
Beberapa tahun yang lalu (blog ini di post tahun 2017), cukup memakan waktu untuk menerjemahkan teks dari bahasa yang tidak diketahui ke bahasa yang kita mengerti. Cara yang umum digunakan pada saat itu ialah menggunakan kamus dengan menerjemahkan satu-per-satu dan hal ini sangat menyulitkan untuk dua alasan:
Sekarang hal itu lebih mudah dengan google translate, baik menerjemahkan per-kata, per-kalimat, hingga teks dalam ukuran besar. Tetapi kebanyakan dari kita tidak mengetahui bagaimana sih mesin tersebut berkerja. Postingan ini khusus pada anda yang berminat.
Jika engine Google Translate mencoba untuk mentranslasi kalimat dari yang panjang hingga yang pendek, tetapi sering kalitidak berhasilkarenabesarnya jumlah variasi kemungkinan yang terjadi. Maka solusi terbaiknya ialah melatih/mengajarkan komputer untuk menentukan aturan grammar dan mentranslasi kalimat oleh komputer itu sendiri.
Jika anda mencoba mempelajari bahasa asing, anda tentu akan dikenalkan dengan beberapa aturan khusus. Jika mencoba merekam dan menerapkan aturan-pengecualian-aturan-pengecualian pada sebuah program maka kualitas terjemahan menjadi rusak.
Machine Translation System menggunakan pendekatan yang berbeda: aturan-aturan dialokasikan dari teks dengan menganalisis sejumlah dokumen dalam skala besar.
Sekarang, mari kita meng-investigasi apa yang tersembunyi dibalik “kotak box (black box)” yang kita sebut sebagai mesin translator. Deep neural network telah menunjukkan kemampuan yang luar biasa untuk melakukan tugas-tugas rumit (seperti recognisi suara/visual), disamping fleksibilitas deep neural network, mereka hanya bisa diterapkan pada tugas-tugas dimana input dan target (output)memiliki dimensi yang tetap.
Disini dimana kita menggunakan Long Short-Term (LSTMs) untuk menggantu kita mengerjakan urutan (sequence) yang tidak diketahui panjangnya dan letak prioritasnya.
LSTM adalah bagian dari RNN (Recurrent Neural Network) yang mampu mempelajari long-term dependecies. Semua RNN terlihat seperti rantai pengulangan pada module.
Langkah kita berikutnya adalah bidirectional recurrent neural networks (BRNNs). Apa itu? BRNN memisahkan neuron RNN biasa ke dua tujuan. Tujuan pertama adalah untuk waktu positif (positive time), atau state kedepannya/selanjutnya (forward). Tujuan yang kedua adalah untuk waktu negatif (negative time), atau state sebelumnya/kebelakang (backward). Outpu dari dua state initidak berhubungan pada inputdari state dari arah berlawananan. Berikut gambarannya:
Untuk mengerti mengapa BRNN bisa berkerja lebih baik dibanding RNN biasa, anda bisa membayangkan sebuah kalimat dari 9 kata, kita ingin memprediksi kata ke-5. Kita hanya bisa mengetahuinya hanya dengan 4 kata pertama, atau 4 kata pertama dan terakhir.
Langkah berikutnya adalah sequence to sequence model (atau biasa disebut dengan seq2seq). Model seq2seq memiliki dua RNN: 1 untuk encoder network (jaringan) yang memproses input dan 1 lagi untuk decoder network (jaringan) untuk menghasilkan output.
Akhirnya dengan ini semua kita bisa membuat machine translator!
Google translate saat ini (2017)mendukung 103 bahasa, maka kita harus memiliki 103x102 model bahasa yang berbeda untuk saling dipasangkan. Yap benar, kualitas model ini bervariasi sesuai dengan popularitas bahasa dan jumlah dokumen yang tersedia untuk melatih network (jaringan) ini.
Ide ini direalisasikan olehGoogle engineers diakhir tahun 2016. Arsitektur NN dibuat dengan model seq2seq yang telah dipelajari sebelumnya.
Salah satu pengecualinnya adalah diantara encoder dan decoder terdapat 8 layer LSTM-RNN yang mempunyai residu (sisa) keterkaitan antar layer tersebut dengan beberapa tweak akurasi dan speed. Jika anda ingin lebih dalam untuk mempelajari hal tersebut anda bisa lihat pada link berikut:
Hal yang utama dari pendekatan ini ialah saat ini algoritma Google Translate hanya menggunakan satu sistem dan bukan satu set besar untuk setiap pasangan bahasa.
Sistem mebutuhkan sebuah “token” pada awal input kalimat yang dikhususkan pada bahasa yang ingin ada terjemahkan.
Hal tersebut meningkatkan kualitas terjemahan dan mampu menerjemahkan walaupun antara dua bahasa tersebut tidak dimiliki oleh sistem. Metode ini disebut “Zero-Shot Translation.”
Saat kita berbicara mengenai peningkatan dan hasil yang lebih baik pada algoritma google translate, muncul satu pertanyaan baru, bagaimana cara meng-evaluasi bahwa kandidat pertama translasi lebih baik dibanding yang kedua?
Ini bukanlah masalah sepele, karena beberapa kalimat umum yang digunakan kita memiliki kumpulan referensi terjemahan dari penerjemah profesional dan tentu itu membuat suatu perbedaan.
Beberapa pendekatan untuk memecahkan masalah ini adalah menggunakan BLEU (bilingual evaluation understudy) yang cukup populer, dan merupakan matrix yang efekif. Sekarang coba bayangkan kita memiliki dua mesin penerjemah:
Candidate 1: Statsbot makes it easy for companies to closely monitor data from various analytical platforms via natural language.
Candidate 2: Statsbot uses natural language to accurately analyze businesses’ metrics from different analytical platforms.
Sekalipun keduanya memiliki makna yang sama tetapi kualitas dan strukturnya berbeda. Jika kita lihat pada hasil penerjemahan manusia:
Reference 1: Statsbot helps companies closely monitor their data from different analytical platforms via natural language.
Reference 2: Statsbot allows companies to carefully monitor data from various analytics platforms by using natural language.
Jelas sekali bahwa kandidat 1 lebih baik, sharing more words dan frasa jika dibandingkan dengan kandidat kedua. Ini adalah adalah kunci pemikiran dari pendekatan BLEU. Kita bisa membandingkann-gramskandidat dengan n-gram translasi referensi dan menghitung jumlah kesamaan (tidak tergantung dari posisinya). Kita cukup menggunakan ketepatan n-gram, karena menghitung recall (penarikan/ingatan) sangat sulit dengan beberapa refs dan hasilnya ialah rata-rata geometric dari nilai n-gram.
Nah kita bisa meng-evaluasi mesin yang complex pada machine learning tanslation. Kedepannya jika anda men-translate seperti Google Translate, bayangkan berapa juta dokumen yang dianalisis sebelum memberikan hasil translasi yang terbaik untuk anda.
Founder at Dialogika",https://miro.medium.com/v2/resize:fit:1200/1*T2XAT2cchESTRz7NGX3NhA.png,"Bahasa Indonesia, Machine Learning, Rnn, Google Translate"
https://medium.com/s/story/infografis-belajar-data-science-dengan-python-66e619a287d2,Alur Belajar Data Science Menggunakan Python,Robit Al Hazmi,2019-01-22T11:04:30.981Z,"Ingin menjadi seorangdata scientistatau ingin menambahskilluntuk menggunakantools data science?Tulisan ini cocok untuk kalian. Saya menulis artikel ini supaya orang yang baru belajar Python untuk analisis data bisa tau langkah-langkahnya seperti apa. Langsung saja cek infografis di bawah.
Source:https://www.analyticsvidhya.com/blog/2015/05/infographic-quick-guide-learn-python-data-science/
Kalian bisa menghubungi saya melalui media sosial di bawah ini —
Machine Learning and Data Science Enthusiast. Love sharing ideas and thoughts ;).",https://miro.medium.com/v2/resize:fit:1000/1*AurkZOnulmHPtX7FPI0unA.jpeg,"Data Science, Infographics, Python"
https://medium.com/s/story/asisten-pribadi-seukuran-kantung-celana-google-asisstant-67c658bf93c4,Asisten Pribadi Seukuran Kantung Celana (Google Asisstant),Andi muhammad,2018-05-28T20:26:10.157Z,"Forming tech based society
Setiap hari, setiap orang mempunyai kesibukan masing — masing dan banyak waktu terbuang untuk mengurus setiap urusan.
Bagaimana kalau setiap orang mempunyai asisten yang bisa mengatur jadwal kehidupan sehari — hari? Google punya solusinya.
Pada saat konferensi google developer I/O, google memperlihatkan sebuah temuan baru mereka berupa google asisstant yang bisa membuat janji temu menyerupai manusia.
Sundar Pichai, CEO dari Google mengatakan, Google Asisstant merupakan sebuah hasil penemuan dari para insinyur google yang bertujuan untuk membuat asisten yang bisa berkomunikasi dengan bisnis kecil yang belum mempunyai booking online.
Para insinyur membuat sebuah teknologi bernama Google Duplex yang berfungsi untuk mengenali pola berbicara sehari — hari. Dengan menggunakan teknologi Artificial Intellegence, maka Google Duplex bisa mengenali “isi” dari percakapan dan memberikan respon yang sesuai.
Google assistant sendiri masih butuh perkembangan untuk benar — benar bisa dibilang sebagai assistant robot. Kedepannya, Google Assistant inilah yang akan mengatur jadwal kehidupan sehari — hari.
Selain Google Duplex, Google juga membuat fitur lain yang menggunakan teknologi Artificial Intellegence. AI dari google sudah bisa membuat sebuah Email yang seakan — akan ditulis oleh manusia sehingga kita tidak perlu repot lagi untuk membuat email.
Sadur:https://futurism.com/google-assistant-booked-haircut-duplex/
Forming tech based society",,"Google Assistant, Artificial Intelligence"
https://medium.com/s/story/kebun-kurma-tailan-telp-wa-0822-4069-7469-kebun-kavling-kurma-thailand-6833a0ddeb0b,"Kebun Kurma Tailan, Telp / Wa 0822–4069–7469, Kebun Kavling Kurma Thailand",basirul akbar,2018-08-14T23:42:45.209Z,"Hari ini banyak sekali hal yang bisa Anda temui di berbagai macam berita, baik di dunia maya maupun media cetak hingga elektronik lainnya. Salah satu bahan pemberitaan yang tidak habis — habisnya ini adalah tentang investasi.
Coba saja Anda lihat di televisi, pasti familiar sekali dengan acara yang mempromosikan investasi property yang dikelola oleh developer properti terkenal. Di berita lainnya seperti media cetak atau berita di internet, berbagai macam tawarantempat investasi terbaikpun bermunculan menawarkan kelebihannya masing — masing.
Memang benar adanya jika investasi menjadi sebuah kepentingan saat ini mengingat persaingan ekonomi yang begitu berat, kemajuan teknologi setiap hari yang juga berdampak pada segala macam kebutuahn hidup yang terus meningkat sementara tidak akan selamanya Anda bekerja, pun tidak akan selamanya kita memimpin sebuah perusahaan atau membuat sebuah usaha.
Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Anda pun tidak akan rugi jika beli kavling diKavling Taman Kurmakarena harga tanah selalu naik dan ini akan jadi investasi yang terbaik untuk Anda.
Tempat Investasi Terbaik adalah Investasi Properti Syariah
Sebagai seorang muslim, segala macam kegiatan khususnya dalam kegiatan perekonomian harus dilakukan sesuai dengan ketentuan hukum islam. Hal yang sama juga berlaku saat Anda akan mencari sebuah investasi masa depan.
Carilah investasi berbasis syariah yang bebas riba dan semua tata cara pelaksanaan dilakukan sesuai dengan syariat Islam.Kavling Taman Kurmasebagai salah satu contoh investasi syariah paling baik karena sudah pasti bebas riba, DP dan Angsuran juga tidak akan dibuat ribet, dan juga sudah memilikilegalitas.
Dengan memilih investasi berbasis syariah, maka hasil yang akan Anda dapat pun sudah pasti terjamin halal. Dengan demikian, kita juga akan hidup jauh lebih tenang di masa depan bersama anak cucu Anda.
Selagi masih muda dan memiliki penghasilan cukup dari bekerja, mulailai untuk sisihkan sedikit demi sedikit untuk kita putarkan ditempat investasi terbaikseperti Membeli Kavling Taman Kurma.
Dengan mulai berpikir untuk melakukan investasi, artinya Anda sudah peduli dengan kehidupan di masa depan terutama juga untuk kelangsungan Hidup anak, istri, hingga cucu dan buyut Anda di masa depan.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:1200/1*2aUPp1PXRY9zMm4FGPQcUQ.jpeg,"Marriage, Military, Millennials, Mindfulness, Machine Learning"
https://medium.com/s/story/analisis-sentiment-dengan-mengambil-data-dari-twitter-case-study-fadli-zon-69161453bc5f,Analisis Sentiment Dengan Mengambil Data Dari Twitter (Case Study: Fadli Zon),Muhammad Farhan,2018-05-05T17:24:24.664Z,"Hai guys! jadi disini saya akan mencoba untuk analisis sentiment yang datanya diambil dari twitter. Bagi kalian yang belum punya akun twitter sebaiknya sekarang kalian buat dulu akun twitternya, dan bagi kalian yang sudah punya akun twitter silahkan login keapps.twitter.com.
Klik Create New Apps -> Isi Application Details Kalian -> Buka Aplikasinya
Untuk bagian apps, pada bagian callback url nya lebih baik menggunakan:http://127.0.0.1:1410dan untuk bagian websitenya kalian bisa menggunakan apa saja, yang terpenting menggunakan https dan www.
Setelah itu klik pada tabs “keys and access token”, nanti akan muncul gambar seperti berikut.
Kalian bisa lihat, pada gambar 1 terdapat consumer key (API Key) dan Consumer Secret (Api Secret). kedua hal tersebut akan kita gunakan untuk mengakses API (Application Programming Interface) yang berguna untuk memudahkan para developer untuk mengambil data dari twitter dan mengolahnya.
Apabila kalian scroll kebawah kalian akan menemukan Access Token, itu akan digunakan untuk koneksikan R kalian ke dalam Twitter. Lihat Gambar 2.
Ok sekarang tanpa basa basi buka R kalian, dan install lalupanggil library dibawah ini:
Kemudian ketik syntax dibawah ini untuk mengkoneksikan ke API twitter
Sebelum kalian menuliskan syntax dibawah ini, pastikan R kalian sudah terkoneksi oleh internet. Karena untuk mengambil data dari twitter harus membutuhkan internet.
Pada variabel “data” merupakan perintah yang digunakan untuk mencari twit mengenai fadli zon dan Fadli Zon dengan jumlah pengambilan maksimal 10.000 twit.
Setelah kalian mengambil datanya, kalian bisa save data tersebut dalam bentuk RDS. agar nanti tidak usah cape-cape ambil twitnya lagi.
Apabila kalian ingin me-load data tersebut, kalian tinggal ketik syntax berikut
Berikut syntax untuk visualisasinya
Bisa Dilihat pada gambar 3 adalah grafik frekuensi berdasarkan waktu per satu jam dari satu minggu yang lalu. Dapat dilihat grafik tersebut meningkat pada jam 11.00–13.00, dari hasil tersebut dapat kita simpulkan bahwa jam 11.00–13.00 merupakan jam aktifnya para pengguna twitter membuka aplikasi twitter.
Pertama kita harus memisahkan twitnya terlebih dahulu, yang kita perlukan hanya mengambil bagian twitnya saja.
Pada bagian ini kita akan menghapus beberapa kata, tanda baca, link url, huruf-huruf aneh, emoji dan lainnya.
Syntax diatas berguna untuk membersihkan data dari twitter.
Pada variabel “twitclean” adalah kata-kata yang ingin kita apus, yang ditambahkan oleh kita sendiri. Selanjutnya memuat DTM (Document term Matriks). DTM ini berguna untuk membuat matriks berisi nilai untuk masing masing kata.
Kemudian kita bisa melihat 10 kata-kata yang sering muncul, dengan menuliskan syntax dibawah ini:
Setelah semuanya sudah kalian running, bagian akhirnya yaitu membuat wordcloud. berikut ini adalah perintah untuk membuat wordcloudnya.
Bisa dilihat pada gambar 4 adalah hasil analisis sentiment dengan mengambil data dari twitter, yang menggunakan case study fadli zon, dan dapat kita simpulkan dari hasil analisis ini kata terbanyak yang diberikan kepada bapak fadli zon adalah kata fitnah dengan frekuensi sebesar 320. Ada apa hubungannya bapak fadli zon dengan kata fitnah yah????",https://miro.medium.com/v2/resize:fit:1200/1*JRRf4RxV1kFpDrwzzAlLCQ.png,"Twitter, Machine Learning"
https://medium.com/s/story/mencari-association-rule-antar-data-menggunakan-apriori-69bca0b6db4f,Mencari association rule antar data menggunakan Apriori,Rangga Rizky A,2018-01-01T06:07:51.652Z,"Bayangkan anda adalah seorang pemilik supermarket. dan setiap hari ribuan pengunjung mengunjungi supermarket anda. anda ingin membuat sebuah program diskon dan mengatur tata letak supermarket sehingga pengunjung merasa nyaman saat berbelanja di supermarket anda. yang anda perlukan adalah pola dari pemberlian oleh pelanggan anda. anda ingin tahu seberapa besar kemungkinan orang yang membeli sepatu juga memberli kaos kaki?.
dengan association rule mining kita dapat menyelesaikan problem diatas. pertama-tama kita tentukan dulu nilai support dan nilai confident. nilai support adalah nilai kemunculan sebuah itemset dalam total transaksi dan nilai confident adalah nilai dari probabilitas keterkaitan antar item.
misalnya berikut adalah data transaksi di supermarket anda:
kita tentukan min supportnya adalah 0.3 dan min confidentnya 0.8. lalu kita tentukan item apa saja yang terjual dan hitung nilai supportnya
dari data diatas ditemukan bahwa selai tidak memenuhi min supportnya sehingga kita buang dari daftar itemset. selanjutnya kita membentuk dua itemset per baris.
nilai confident didapatkan dari rumusconf(XY)=P(Y|X)=P(XY)/P(X) .dari data diatas dapat disimpulkan bahwa seorang yang membeli roti maka akan membeli mentega. seseorang yang akan membeli coklat maka juga akan membeli susu. karena masih dimungkinkan membentuk itemset 3 item maka dilanjutkan pada tahap selanjutnya
Dari data diatas dapat disumpulkan orang yang memberli roti dan susu maka akan membeli mentega. dan orang yang memberli susu dan mentega akan membeli roti juga.
dengan begini anda dapat menempatkan roti , mentega dan susu berdekatan sehingga pelanggan akan tertarik untuk membeli ketiga barang tersebut.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:480/1*0lqAIUTQwMM_SGoAGLxX9Q.jpeg,"Data Science, Data Mining"
https://medium.com/s/story/2-4-masalah-etika-revolusi-mesin-pintar-6b483e4490d0,(2/4) Masalah Etika Revolusi Mesin Pintar,Machine Learning Indonesia (ML ID),2018-05-02T07:11:45.121Z,"Machine Learning Indonesia
Kita sekarang berada dalam periode ANI di mana sebagian besarstate-of-artteknologi didasarkan pada pembelajaran mesin, yaitu mesin yang belajar berdasarkan satu kumpulan algoritma yang mencoba untuk memodel abstraksi tingkat tinggi dalam data dengan menggunakan beberapa lapisan pengolahan, dengan struktur yang kompleks atau sebaliknya serta terdiri dari beberapa transformasi non-linear. Ini sebenarnya adalah sebuah metode untuk belajar dari data, mengenali pola dalam data dan melakukan generalisasi prediksi untuk data baru. Berbagai dalam arsitektur pembelajaran mesin seperti model jaringan saraf betlapis (deep neural network,convolutional neural network), dimana jaringan saraf berulang telah diterapkan untuk bidang-bidang seperti pengamatan komputer, pengenalan suara otomatis, pengolahan bahasa alami, pengenalan audio dan bioinformatika di mana mereka telah terbukti berhasil digunakan pada berbagai bidang. Kita telah menggunakan teknologi ini di setiap hari di komputer atau aplikasi smartphone dan lebih jauh lagi, pembelajaran dengandeep neural networkmemiliki potensi besar untuk membuat mesin pintar dimasa depan.
Perdebatan tentang dampak sosial menciptakan mesin pintar telah dikaji banyak organisasi dan individu selama beberapa dekade terakhir. Implikasi moral dan etika mesin pintar memiliki argumen tiga sisi.Pertama, mesin pintar akan mengganti banyak pekerjaan manusia dengan mudah, misalnya sebagai analis kredit di perbankan, pengacara di firma hukum, drone cerdas untuk perang atau robot dalam pabrik. Banyak orang akan hidup dalam kemiskinan dan ada sedikit atau tidak ada alasan untuk membuat buruh dengan mesin pintar.Kedua, orang-orang berpendapat bahwa masyarakat tidak dapat mengembangkan atau memanfaatkan sumber daya tanpa bantuan mesin yang bisa berpikir sendiri. Mesin pintar tidak peduli tentang masalah sosial sama sekali, seperti umumnya masyarakat manusia. Yangketiga, tentang sejauh mana kita harus membuat mesin pintar berdasarkan AI dan apakah mesin ini akan terlihat seperti manusia. Apakah kita berbicara tentang perangkat otonom seperti penjelajah ruang atau robot yang meniru bentuk, pikiran dan perilaku manusia? Dengan semakin banyaknya mesin pintar apakah kita akan mempercayakan anak-anak kita, lembaga pendidikan, bisnis, dan pemerintah untuk mesin pintar juga?
Pertimbangan di atas adalah contoh yang baik dari kesulitan potensi mesin pintar karena akan berdampak besar pada kehidupan manusia. Meskipun meniru pembelajaran otak manusia adalah kompleks luar biasa dan dibatasi oleh kemajuan daya prosesor saat ini, setidaknya beberapa dekade teknologi bisa mencapai tingkat paling dasar kecerdasan manusia. Tapi setelah industri menginvestasikan miliaran dolar untuk membuat mesin pintar untuk menyaingi manusia dalam tugas-tugas tertentu, pertanyaan tentang nilai-nilai etika dan moral yang kita tanamkan akan terus mencuat.
Melihat peradaban manusia dengan nilai-nilai budaya, agama, etika dan moral yang beragam, apakah sebenarnya yang ingin kita kembangkan dan dengan tujuan apa? Jika kita harus membuat robot AI, misalnya, dapatkah kita membuat mereka beragama? Apakah agamanya Katolik, Muslim, atau Buddha? Atau apakah kita ingin melawan dalam perang dengan drone pintar? Atau bagaimana dengan robot seks yang akan tersedia secara komersial? Bagaimana merancang nilai-nilai kecerdasan etika yang sempurna menurut tradisi manusia, ideologi, atau agama yang telah dibangun selama ribuan tahun? Itu tentu bukan masalah yang mudah dan tidak ada jawaban yang dapat dirumuskan dengan mudah.
Argumen bahwa robot AI akan mengambil pekerjaan manusia benar adanya. Beberapa pabrik modern sudah menggunakan robot pintar saat ini. Akan tetapi juga benar bahwa pekerjaan robot tersebut umumnya pekerjaan berulang-ulang, monoton dan sering berbahaya bagi manusia. Apakah salah untuk menggantikan manusia dengan robot dalam kasus tersebut? Untuk menggambarkan betapa sulitnya untuk menjawab masalah ini, berikut adalah beberapa pendapat dan perdebatan:
Dari contoh di atas, kita dapat mengatakan kemungkinan menciptakan mesin pintar akan menimbulkan sejumlah masalah etika dan moral. Banyak pertanyaan belum terjawab untuk memastikan bahwa mesin pintar tersebut tidak membahayakan manusia dan makhluk lain yang relevan secara moral bahkan status moral dari mesin itu sendiri. Bayangkan jika dalam waktu dekat, asuransi kesehatan BJPS di Indonesia menggunakan algoritma pembelajaran mesin untuk merekomendasikan persetujuan perawatan medis. Keluarga pasien yang ditolak oleh algoritma ternyata meninggal dan mengajukan gugatan terhadap BPJS, dengan alasan bahwa algoritma telah melakukan diskriminasi bagi pasien. Secara teknis, BPJS dapat menjawab bahwa hal tersebut tidak mungkin, karena algoritma belajar dari data sebelumnya dan membuat keputusan yang selalu netral. Tentu saja argumen tersebut dapat diterima, namun bagaimana bila dalam kasus serupa algoritma BJPS terbukti menyetujui permohonan pasien lain?. Hal ini mungkin terjadi karena kualitas data yang digunakan terlalu rendah sehingga hasil inferensi kurang tepat. Apa yang mungkin bisa terjadi dari sisi etika untuk kasus ini?
Menyelidiki kasus BPJS tidak mudah karena tergantung pada netralitas algoritma pembelajaran mesin yang digunakan. Jika algoritma didasarkan pada deep neural network — yang mampu secara otomatis menemukan representasi fitur dalam data, maka sulit untuk membuktikan bahwa keputusannya bias karena sangat bergantung pada kualitas data. Di sisi lain, jika algoritma yang digunakan adalah pohon keputusan (decision tree) atau jaringan Bayesian (Bayesian network) akan lebih transparan untuk diinspeksi. Ini memungkinkan auditor untuk menemukan kesalahan dalam algoritma pembelajaran mesin. Algoritma pembelajaran mesin tidak hanya tepat, kuat dan terukur, tetapi juga harus transparan untuk inspeksi. Itu hanya contoh sederhana dari banyak aspek sosial dan moral dari mesin pintar. Tentu saja transparansi bukan satu-satunya fitur yang diinginkan mesin pintar.
Apakah kita ada pilihan untuk transparansi dalam algoritma AI? Stuart Russell, profesor ilmu komputer dan Smith-Zadeh professor teknik di University of California, Berkeley, menggunakan metodologi untuk menanamkan aspek etika dalam AI yang dikenal dengan metode pembelajaran penguat terbalik (IRL). Dengan IRL, sistem-sistem sensor yang mengamati tingkah laku manusia dapat dibuat berbasis etika. Setelah suatu tingkah laku yang diamati sesuai dengan etika yang diharapkan, kode program di modifikasi kembali sehingga algoritma pembelajaran mesinnya bekerja sesuai dengan etika yang diharapkan. Sebagai contoh dari proses ini, Russell menunjukkan dalam pidatonya baru-baru di Pusat Studi Risiko Eksistensial di University of Cambridge, bagaimana robot bisa mengamati orang yang berulang kali membuat air mendidih dan menuangkan bubuk hitam ke dalam air setiap pagi. Dengan mencatat perbaikan suasana hati manusia, nilai ritual kopi dapat dikodifikasi, sehingga robot dapat menirunya. Russell kemudian menjelaskan bahwa tujuan manusia ada dalam konteks bagaimana dia hidup sampai ke titik menerima tujuan baru. Misalnya, jika kita kehabisan daging saat memasak, kita sadar untuk tidak memasak kucing kesayangan kita, tapi ini adalah nilai yang dimiliki manusia yang tentu saja tidak ada dalam algoritma robot pemasak di dapur. Akibatnya, Russell merasa harus ada perusahaan yang membangun representasi untuk nilai-nilai kemanusiaan, termasuk konsep latar belakang orang yang akan mengenali lapisan etika, hukum dan moral yang selama ini kita terima begitu saja. Sebuah prototipe dari organisasi semacam ini ada di Open Roboethics Initiative yang melakukan Crowdsourcing informasi sekitar AI dan robotika secara terfokus.
Kita tidak bisa terus bergerak maju dalam lingkungan dimana aspek etika dalam AI diabaikan. Etika dalam AI seharusnya tidak hanya menjadi renungan, besar sekali risiko yang akan timbul jika terlambat. Kesempatan untuk inovasi akan datang ketika kita bisa menginformasikan proses manufaktur AI dengan pemrograman berdasarkan kodifikasi keyakinan kita yang dipegang teguh. Bagaimana mesin akan tahu nilai-nilai kemanusiaan tanpa diberitahu? Itulah pertanyaan yang perlu dijawab hari ini atau mesin pintar akan terus dikembangkan tanpa adanya kontrol sehingga suatu saat nanti menjadi tidak terkendali.
Sama seperti teknologi informasi lainnya, semakin penting untuk algoritma pembelajaran mesin untuk tahan terhadap manipulasi teknis, seperti malware atau gangguan keamanan. Ambil contoh mobil tanpa supir yang menggunakan algoritma pembelajaran mesin untuk mengenali objek di jalan dan mengambil keputusan apakah aman untuk mengemudi atau tidak. Sekelompok peneliti yang berbasis di Virginia didanai oleh Departemen Pertahanan menemukan bahwa itu relatif mudah untuk meretas sistem kontrol mobil tanpa supir, tetapi juga mengatakan bahwa solusi terhadap serangan seperti itu bisa juga murah. Dua perusahaan di Charlottesville bekerja sama dengan University of Virginia dan Departemen Pertahanan untuk menemukan solusi memerangi serangan cyber pada fungsi penting seperti pengereman dan mempercepat dalam mobil tanpa supir. Solusi ini terdiri dari dua bagian: hardware tertanam dengan sistem fisik dan software berbasis cloud. Pertanyaannya siapa yang akan mengoperasikan sistem untuk melindungi mobil tanpa supir bagi masyarakat? Diperlukan kriteria sosial lain untuk berurusan dengan organisasi atau pemerintah untuk menentukan tanggung jawab dan akuntabilitas yang berhubungan dengan kesejahteraan manusia. Ketika sistem AI gagal untuk melindungi keamanan orang, siapa yang akan disalahkan? Para insinyur perangkat lunak atau polisi?
Sebagai ringkasan — tanggung jawab, transparansi, auditability, korup, prediktabilitas, dan niat untuk tidak membahayakan kehidupan manusia sangat diperlukan pada implementasi mesin pintar. Semua kriteria yang berlaku untuk manusia melakukan fungsi-fungsi sosial perlu dipertimbangkan dalamArtificial Intelligence Narrow(ANI) karena tujuannya adalah untuk menggantikan penilaian dan pekerjaan manusia pada tugas-tugas tertentu. Berbeda dengan apa yang peneliti AI telah lakukan dalam beberapa tahun terakhir, seperti bagaimana untuk meningkatkan kinerja algoritma AI dan perangkat pintar, isu-isu etika mesin pintar membutuhkan lebih banyak perhatian. Ini bukan masalah sepele yang dapat dengan mudah ditemukan solusinya, tetapi membutuhkan upaya evolusional untuk menemukan solusi yang tepat dari waktu ke waktu.
Baca selanjutnya tentangAGIdanASI, atau kembali ke bagianpertama.
TSMRA, Jakarta, Juni, 2016.
Disadur darihttp://deepbrains.com/2016/06/24-masalah-etika-revolusi-mesin-pintar/seijinPenulis.
Machine Learning Indonesia
Machine Learning Indonesian Community",,"Deep Learning, Artificial Intelligence, Ethics, Machine Learning, Robots"
https://medium.com/s/story/ai-perkara-identitas-dalam-teknologi-6b7c425f6e26,AI: Perkara Identitas dalam Teknologi,Haryo Teguh,2017-10-05T06:18:21.339Z,"Perkembangan teknologi digital saat ini berkembang pesat. Bidang keilmuan tersebut telah mengembangkan beberapa produk yang kini menjadi bagian dari kehidupan sehari-hari kita. Hampir setiap orang memiliki gawai elektronik seperti telepon genggam dan komputer.
Teknologi digital yang ada telah memberi dampak yang besar kepada umat manusia. Bukan hanya dalam bidang telekomunikasi, tapi juga membantu produktivitas kerja, dan juga sebagai alat yang memungkinkan dilakukannya suatu penelitian. Teknologi digital membuat manusia lebih sejahtera secara umum.
Salah satu teknologi digital yang sedang dikembangkan saat ini adalah teknologi inteligen artifisial. Bentuk inteligen artifisial (IA) sederhana saat ini dapat dilihat pada mesin penjual otomatis. IA sederhana dapat menerjemahkan suatu bentuk masukan menjadi suatu keluaran tertentu dengan algortima. Sedangkan yang lebih kompleks dapat dilihat pada produk IA mesin pencari ataupun komputer seperti Google AI, Siri, Cortana, dan lain-lain. IA kompleks dapat membaca dan sedikit mengerti makna dari masukan yang diterima dan menggunakan algoritma yang lebih kompleks untuk menentukan keluaran yang tepat.
Saat ini belum ada IA yang memiliki kesadaran seperti manusia. IA bahkan masih belum benar-benar mengerti masukan yang berupa bahasa manusia, ataupun aksi manusia. Konsep diri dan identitas masih belum bisa diterjemahkan menjadi bentuk algoritma tertentu.
Lalu bagaimana jika IA telah berhasil dibuat sedemikian rupa sehingga benar-benar mirip dengan manusia? Sudah tidak dapat dibedakan lagi dengan kesadaran manusia? Memberikan konsep diri, identitas, dan emosi dapat mengantarkan kita pada sebuah dilema moral dimana tindakan kepada suatu mesin atau alat menjadi tidak dapat dibedakan dengan tindakan kepada manusia.
Ambil contoh kasus berikut:Anda ingin meneliti pengaruh tindakan pedofil kepada korbannya pada kondisi dan perkembangan karakter dan emosional dari korban. Apakah boleh kita menciptakan suatu IA beserta gawainya yang benar-benar mirip dengan anak-anak manusia, lalu dijadikan objek penelitian?
Untuk menjawab pertanyaan tersebut, pertama ‘boleh’ harus didefinisikan terlebih dahulu. Boleh menurut kbbi adalah diizinkan atau tidak dilarang. Berdasarkan definisi tersebut, boleh tidaknya sesuatu dilakukan tergantung siapa yang mengizinkan. Makna ‘boleh’ menjadi terpecah berdasarkan perspektif pengamatnya, apakah boleh menurut diri sendiri, masyarakat, Negara, atau bahkan dunia internasional.
Untuk membatasi ruang lingkup perspektif tersebut, asumsikan penelitian tersebut akan dilakukan di Indonesia saat ini. Saat ini, tidak ada hukum internasional ataupun peraturan negara yang mengatur hak dan kewajiban sebuah mesin dan IA. Selain itu, masyarakat Indonesia juga cenderung tidak memedulikan etika kepada mesin dan IA, bahkan pada beberapa kasus etika dan moral suatu kelompok hanya berlaku bagi anggota kelompok tersebut.
Sehingga apabila penelitian tersebut dilakukan di Indonesia, maka bagi negara dan sebagian besar orang boleh dilakukan, asalkan benar objek penelitiannya adalah mesin dengan IA yang menyerupai anak-anak. Akan tetapi, penelitian ini akan menjadi tidak boleh bagi negara dan sebagian besar orang apabila objek penelitiannya sudah tidak dapat dibedakan lagi dengan anak-anak manusia.
Jadi, apabila penelitian tersebut dilakukan di Indonesia maka boleh dilakukan bagi negara dan sebagian besar orang selama masih dapat dibedakan dengan manusia. Semakin mirip dengan manusia maka akan semakin banyak orang yang menentang dilakukannya penelitian tersebut.
Perjalanan perkembangan teknologi IA saat ini masih pada tahap awal. Masih banyak waktu untuk membenahi perkara lain dan juga mengantisipasi dilema moral seperti di atas sampai tiba saatnya manusia benar-benar membutuhkan jawabannya. Konsep moral sebagai produk budaya sangat tergantung pada waktu dan tempat, terlebih moral pribadi. Bisa jadi moral yang dianut saat ini sudah tidak berlaku saat IA sudah dikembangkan sedemikian rupa sehingga benar-benar mirip manusia.
R. Himan Haryo Teguh D.",,"Bahasa Indonesia, Digital, AI, Technology"
https://medium.com/s/story/machine-learning-and-what-you-can-do-about-it-71d8a86af9f9,Machine Learning and what you can do about it…,Rangga Rizky A,2017-11-21T13:31:18.313Z,"Saat itu semester 6 saat saya pertama kali mengenal machine learning. dari situ membuka pikiran saya bahwa ilmu yang saya miliki masih sangat jauh untuk di bilang seseorang yang ahli. dan pada semester itu juga saya mengenal image processing yang mempunyai hubungan dekat dengan machine learning.
Machine learning adalah sebuah bidang dalam computer science. yang mempelajari tentang bagaimana cara membuat komputer dapat berpikir seperti manusia. kita sebagai manusia perlu belajar agar dapat mahir melakukan sesuatu, begitu juga komputer, komputer juga dapat belajar melalui sekumpulan data yang dinamakan data training dan dari data tersebut komputer dapat mengambil sebuah keputusan dan menemukan pola dari kumpulan data yang berdimensi besar. disini saya akan membahas 2 hal yang sederhana yaitu tentang klasifikasi dan clustering.
Klasifikasi adalah mengelompokan suatu data pada kelas-kelas yang sudah didefinisikan. Contohnya untuk mendeteksi apakah seseorang terkena diabetes, mendeteksi nama bunga berdasarkan gambar. mengelompokan berita berdasarkan topik yang sudah didefinisikan. salah satu ciri permasalahan yang bisa diselesaikan dengan klasifikasi dalah mempunyai kumpulan data yang berlabel (Supervised learning), Berikut berbagai macam algoritama klasifikasi :
K Nearest neighboradalah algoritma yang paling sederhana, algoritma ini bekerja dengan cara menghitung jarak data baru dengan setiap data training.kemudian diurutkan berdasarkan paling dekat, dimana akan diambil sejumlah K teratas kemudian dilakukan voting mana yang paling banyak muncul akan mewakili kelas dari data baru tersebut. KNN bagus untuk data yang memiliki banyak noise. Algoritma ini mempunyai masalah waktu komputasi ketika data training berjumlah sangat banyak,
Naive Bayes Classifieradalah algoritma klasifikasi berdasarkan teorema bayes. dengan memanfaatkan jumlah probabilitas kemunculan sebuah data. algoritma ini mempunyai waktu komputasi lebih baik dari KNN, tetapi hanya bisa digunakan untuk data tipe kategorikal.
Linear SVMadalah algoritma klasifikasi yang menggunakan fungsi pemisah untuk memisahkan kedua kelas yang berbeda. untuk linear svm sendiri bagus digunakan untuk data yang tidak banyak noise dan dapat dipisahkan secara linear.
Analisis kluster adalah sebuah metode untuk mengelompokan data yang memiliki karateristik yang sama ke dalam satu kelompok. Clustering dapat digunakan untuk segmentasi gambar misalnya penghapusan backgorund. selain itu clustering juga dapat digunakan untuk menemukan pola dari kumpulan dokumen dan market research. untuk clustering tidak membutuhkan data yang berlabel (unsupervised learning). berikut beberapa algoritma clustering :
K-meansadalah algoritma klustering yang mengelompokan pada k buah cluster dimana tiap k titik nantinya akan menjadi titik pusat dari sebuah cluster. algoritma ini berawal dari menentukan posisi k titik secara random. dan k titik akan terus berpindah hingga menemukan posisi yang sesuai, algoritma ini sangat cepat tetapi hasilnya akan buruk ketika awal titik pembangkitan k titik tidak tepat.
Hirariecal Clusteringmerupakan algoritma clustering yang bekerja dengan menjadikan tiap data adalah sebuah cluster dimana akan dilakukan iterasi untuk penggabungan dua kluster dengan jarak yang terdekat. hingga jumlah kluster sesuai dengan yang diinginkan. Algoritma ini mempunyai akurasi yang bagus tetapi memiliki waktu komputasi lebih lama dari k-means
hal-hal diatas adalah sebagian kecil dari machine learning. masih banyak tantangan dalam machine learning seperti dimensi data yang sangat besar, waktu komputasi dan akurasi.
untuk algoritmaK-means , Naive bayes Classifier, Linear SVMdanHirarical Clusteringbisa download di github sayahttps://github.com/rangga-rizky/MachineLearning
What cannot be proven is wrong. Cannot be proven correctly",,Machine Learning
https://medium.com/s/story/pengenalan-deep-learning-part-3-backpropagation-algorithm-720be9a5fbb8,Pengenalan Deep Learning Part 3 : BackPropagation Algorithm,Samuel Sena,2018-03-19T15:14:50.670Z,"PadaPart 1kita sudah sedikit disinggung tentang cara melakukan training pada neural network. Proses training terdiri dari 2 bagian utama yaitu Forward Pass dan Backward Pass. Panah biru dibawah ini adalah Forward Pass dan panah merah adalah backward pass.
Dalam Supervised Learning, training data terdiri dari input dan output/target. Pada saat forward pass, input akan di-”propagate” menuju output layer dan hasil prediksi output akan dibandingakan dengan target dengan menggunakan sebuah fungsi yang biasa disebut denganLoss Function.
Lalu untuk apa loss function itu? Secara simple loss function digunakan untuk mengukur seberapa bagus performa dari neural network kita dalam melakukan prediksi terhadap target.
Ada berbagai macam loss function, namun yang paling sering digunakan adalah Squared Error (L2 Loss) untuk regresi. Sedangkan untuk klasifikasi yang biasa digunakan adalah Cross Entropy.
Simplenya proses ini bermaksud untuk menyesuaikan kembali tiap weight dan bias berdasarkan error yang didapat pada saat forward pass. Tahapan dari backprop adalah sebagai berikut :
Siapkan kertas dan calculator. Yuk kita lihat contoh dibawah ini biar lebih jelas….
Neural network diatas terdiri dari 2 hidden layer. Hidden layer pertama menggunakan ReLU, hidden layer kedua menggunakan sigmoid dan terakhir output layer menggunakan linear sebagai activation function. Bias pada diagram diatas sebenarnya ada tetapi tidak digambarkan.
Terdapat 4 weight dan 4 bias diantara input layer dan hidden layer pertama, 8 weight dan 2 bias diantara hidden layer pertama dan kedua, 2 weight dan 1 bias diantara hidden layer kedua dan output layer. Sehingga total ada 21 parameter yang harus diupdate.
Kita akan mencoba untuk melakukan prediksi terhadap suatu nilai. Untuk initial weight dan bias, saya tentukan sendiri dengan nilai angka yang lebih enak dilihat :D
Disini kita akan melakukan forward pass data input menuju hidden layer 1. Yang dilakukan adalah melakukan perkalian (dot product) dan penjumlahan matrix antara input, weight dan bias.
Nilai diatas adalah input dari tiap node pada hidden layer 1. Semua nilai tersebut akan dikeluarkan setelah melalui activation function. Pada hidden layer 1 activation function yang kita gunakan adalah ReLU =>f(x) = max(0, x).Sehingga output dari hidden layer 1 adalah sebagai berikut :
Sama seperti forward pass pada layer sebelumnya, output dari tiap neuron pada ReLU layer akan mengalir ke semua neuron pada Sigmoid layer.
Setelah activation function :
Sama seperti forward pass pada layer sebelumnya, output dari tiap neuron pada Sigmoid layer akan mengalir ke neuron pada Linear layer (Output).
Setelah activation function :
Kita sudah sampai pada output layer dan sudah mendapatkan nilai prediksi output. Selanjutnya kita akan mencarilossdengan menggunakan squared error (L2 Loss).
Sebelum membahas tentang backward pass ada baiknya kalau kita mencari dulu turunun tiap activation function yang kita pakai.
Hampir sama seperti pada forward pass, pada backward pass, loss akan mengalir menuju semua node pada hidden layer untuk dicari gradient nya. terhadap parameter yang akan diupdate. Misalkan kita ingin mengupdate parameterWk1o,maka kita bisa gunakan chain rule seperti dibawah ini.
Pertama kita akan mencari berapa besar perubahan Loss berdasarkan output. Sehingga kita harus mencari turunan parsial (partial derivative) dari loss function terhadap output, kita juga bisa menyebutnya sebagai gradient loss function terhadap output. Pada persamaan dibawah, loss akan dikalikan dengan 1/2, sebenarnya tujuannya agar saat diturunkan, fungsi loss akan menjadi 1 kali Loss (menetralisir turunan fungsi kuadrat).
Selanjutnya kita akan mencari gradient dariOoutterhadapOin. Karena activation function yang digunakan adalah Linear, maka turunannya sangat mudah dicari.
Setelah itu kita akan cari gradient dariOinterhadapWk1o, Wk2o dan bias (bo).Perhatikan persamaan dibawah ini:
Terakhir kita akan menerapkan chain rule untuk mencari gradient loss terhadap weight dan bias.
SGD adalah algoritma yang digunakan untuk mengupdate parameter dalam hal ini weight dan bias. Algoritmanya cukup sederhana, pada dasarnya kita hanya mengurangi initial weight dengan “sebagian” dari nilai gradient yang sudah kita dapat.
Sebagian disini diwakili oleh hyper-parameter bernamalearning rate (alpha).Sebagai contoh saja, kita gunakan 0.25 sebagai learning rate meskipun pada prakteknyalearning rate0.25 itu tidak ideal. (Nanti akan dibahas tentang settinghyper-parameter).
Cukup panjang memang kalau diruntut satu-satu. Tapi gpp biar lebih jelas aja :D. Kita lanjutkan lagi backprop untuk layer selanjutnya.
Kita bisa ulangi setiap step yang kita lakukan pada backward pass pada layer sebelumnya. Hanya saja kita harus lebih hati-hati karena relatif lebih rumit daripada backward pass pada layer sebelumnya. Hang on guys… :D
Untuk mencari gradient loss terhadapWj1k1,lagi-lagi kita akan gunakan chain rule. Pertama kita akan mencari gradient loss terhadapK1out.
Lalu kita akan cari gradientK1outterhadapK1in. Kali ini kita menggunakan turunan dari sigmoid yang sudah kita cari diawal tadi.
Selanjutnya kita akan cari gradientK1interhadapWj1k1.
Sekarang kita bisa menghitung gradient loss terhadapWj1k1dengan menerapkan chain rule yang tadi.
Akhirnya kita mendapatkan gradientnya, perhatikan yang warna merah. Gradient dari sigmoid sudah cukup kecil yaitu 0.00249 dan setelah chain rule hasilnya tambah kecil lagi yaitu -0.00188
Fenomena inilah yang disebut denganVanishing Gradientdan merupakan alasan mengapa sigmoid sudah jarang digunakan lagi.
Perhitungan yang barusan kita lakukan tadi setelah diterapkan untuk semua parameter maka akan didapat semua gradient yang dibutuhkan untuk melakukan update.
Kita bisa lihat dibawah ini kalau gradientnya sangat kecil (vanish), sehingga semakin dekat sebuah node dengan input layer, maka semakin lama pula waktu yang dibutuhkan untuk melakukan training, karena gradient yang digunakan untuk melakukan update sangat kecil dan akan bertambah kecil lagi setelah dikalikan dengan learning rate :)
Weight dan bias yang baru sangat mudah dicari setelah kita menemukan gradientnya. Tetap dengan learning rate sebesar 0.25 kita akan mendapatkan weight dan bias yang baru.
Kita akan lakukan lagi langkah-langkah yang sudah kita pelajari tadi. Kali ini kita akan melakukan update terhadap weight dan bias diantara input layer dan hidden layer 1.
Pertama kita akan mencari gradient loss terhadapJ1out.KaliIni lebih rumit daripada perhitungan K1out. KarenaJ1outdipengaruhi oleh gradient yang berasal dariK2.Sehingga kita harus melihat LayerKsebagai satu kesatuan, bukan lagiK1danK2.
Lanjut dengan gradientJ1outterhadapJ1in.
Selanjutnya kita akan cari gradientJ1interhadapWij1.
Pada akhirnya kita bisa menghitung gradient loss terhadapWij1dengan menerapkan chain rule.
Perhitungan yang baru saja kita lakukan tadi akan diterapkan untuk semua parameter. Maka didapat semua gradient yang dibutuhkan untuk melakukan update.
Akhirnya selesai juga. Kita sudah mendapatkan semua parameter yang baru. Nanti proses ini (forward pass dan backward pass) akan diulang terus menerus sampai kita dapatkan nilai Loss yang paling kecil.
Contoh diatas hanya menggunakan satu buah data pada saat forward dan backward. Secara umum Gradient Descent ini terdiri dari 3 tipe, SGD yang kita pakai diatas, Batch Gradient Descent dan Mini-batch Gradient Descent.
Pada Batch Gradient Descent (BGD), model akan diupdate setelah semua data seleasai di-”propagate”. Sedangkan Mini-batch berada ditengah-tengah SGD dan BGD.
Mini-batch gradient descent melakukan forward dan backward pass pada sekelompok kecil training data. Misalnya melakukan update untuk setiap 32/64 buah data dan error yang dihitung adalah mean dari sekelompok training data tersebut.
Pada part ini kita sudah sama-sama belajar tentang backpropagation, gradient dan SGD. Nanti di part berikutnya kita akan coba belajar tentang Deep Learning Framework. So, stay tune guys… :D
Mungkin untuk bacaan selanjutnya bisa disimak tulisan Mas Karpathy tentang pentingnya kita memahami backprop. Disitu juga dibahas tentang “dying ReLU” juga.
Semoga post ini bermanfaat untuk kita semua yang ingin memahami tentang backpropagation.
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*ImhnP-gh895OM2cDXcD1QA.jpeg,"Deep Learning, Artificial Intelligence, Machine Learning, Neural Networks"
https://medium.com/s/story/apa-itu-big-data-belajar-machine-learning-part-2-73f829850ad2,Apa itu Big Data? — Belajar Machine Learning Part #2,Farhan,2018-03-30T00:27:08.630Z,"Halo Kamerad, jumpa lagi dalamdiarysaya belajarMachine Learning. Pada part sebelumnya saya menuliskan rangkuman proses belajarMachine Learning, sedangkan pada rangkuman kali ini saya akan menjelaskan pemahaman saya dalam proses belajar Data Science karena sekarang saya sedang mengambil kelas Data Science (hehehe pamer). Hmm, memang terbalik dan kacau ya, seharusnya belajarData Sciencedulu barulah masuk keMachine Learning. Tapi yaudah lah toh ujung-ujungnya akan mempelajariMachine Learningjuga jadi ya tulisan ini saya masukan sebagai proses belajarMachine Learninghehe emang agak maksa, tapi ya memang beginilah adanya proses belajar saya.
Tanpa membuang waktu lebih banyak lagi akan saya awali rangkuman belajar Data Science ini dengan pemahaman saya tentangapa itu Big Data.
Big Dataadalah kegiatan mengumpulkandataset-datasetsampai menjadi berukuran besar yang bisa berasal dari sumber digital maupun tradisionaluntuk mengidentifikasi trend dan pola-pola tertentu.Informasi yang didapatkan dari Big Data digunakan perusahaan untuk meningkatkan pengetahuan mereka akan kebutuhan atau keinginan pelanggan. Tujuannya tentu untuk membuat keputusan solid berdasarkan data riil, bukan sekedar perkiraan. Data-data tersebut bisa berasal dari dalam perusahaan maupun dari luar perusahaan seperti contohnya statusmedia sosialkomentar pelanggan terhadap produk perusahaan.
Big Data memiliki 3V’s atau 3 karakter V, yaitu:
2.Velocity(kecepatan data dihasilkan)
Big data ini membutuhkan pemrosesan yang inovatif dan efektif dari segi biayauntuk mendapatkan pengetahuan barusehingga dapat dipakai untuk membuat keputusan yang lebih baik pada perusahaan.
Sedangkan untuk pemrosesan Big Data sendiri mempertimbangkan 5V, yaitu:
1. Volume (Ukuran data)
Ya sudah jelas namanya ukuran ya besarnya data.
2. Velocity (Kecepatan data dihasilkan)
Kecepatan di sini maksudnya adalah ada data yang pertumbuhannya lambat dan data yang pertumbuhannya cepat. Contohnya berapa kali manusia mandi dalam sehari dengan berapa kali manusia mengedipkan mata? Tentu saja data yang lebih cepat bertumbuh adalah berapa kali manusia mengedipkan mata dalam sehari.
3. Variety (Variasi data)
Semakin banyak variasi data yang diproses akan menambah keakuratan.
4. Veracity (Akurasi data)
Tentu saja ada data yang bagus ada pula data yang tidak bagus atau tidak akurat, maka dari itu sebelum diproses pasti ada tahap pembersihan data.
Kira-kira seberapa manfaat pengetahuan baru tersebut diteliti untuk perusahaan? Apakah penting untuk kemajuan perusahaan?
Yaudah segitu aja dulu hehe males saya nulis panjang-panjang, nanti saya sambung lagi dengan penjelasanJenis Data dan Pengenalan Database.Kalo ada salah dalam pemahaman saya ini silahkan dikoreksi hehe.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:1200/1*mXqkYqJKpXPHkoi8geGVLA.jpeg,"Big Data, Data Science, Machine Learning"
https://medium.com/s/story/klasifikasi-image-dengan-2-kelas-75b8f9cc1ae3,Klasifikasi Image dengan 2 Kelas,Moch. Ari Nasichuddin,2018-07-22T01:25:48.028Z,"Pertama kali saya belajar Deep Learning adalah ketika mengambil studi master. Penelitian tesis saya kala itu tentang klasifikasi teks positif atau negatif. Metode Deep Learning yang saya pakai adalah CNN. Metode tersebut umumnya dipakai image, namun pada penelitian saya akan diterapkan pada kasus teks. Jamaknya ketika menggunakan data teks, metode Deep Learning yang digunakan adalah RNN/LSTM.
Kali ini saya mencoba memanfaatkan Deep Learning: CNN untuk klasifikasi data image dengan 2 kelas. Arsitektur CNN yang dipakai terdiri dari Input Matriks + 2 Convolution Layer menggunakan fungsi aktivasi ReLU + Fully-connected menggunakan fungsi aktivasi sigmoid.
Setelah proses training dan testing, akurasi yang dihasilkan70%. Kodenya bisa dicekdi sini.
Machine Learning Engineer & Web/Core Programmer at Atmatech.",,"Deep Learning, Cnn, Machine Learning, Image Recognition"
https://medium.com/s/story/simple-linear-regression-dataset-7928744c69be,Simple Linear Regression: Dataset,R. Kukuh,2018-02-16T13:15:23.005Z,"Halo para pecinta ML! Setelah sebelumnya Anda mengikuti rangkaian tutorial dalam seriData Preprocessing, sekarang waktunya benar-benar bermain dengan Machine Learning (yeaayy!!)
But first, perlu Anda ketahui bahwa ada banyak algoritma dalam Machine Learning.
Diantara yang termudah adalah: Regression, dan diantara algoritma Regression yang termudah adalah: Simple Linear Regression, yang akan kita pelajari dalam seri kali ini.
Dataset yang akan digunakan dalam seri tutorial ini bisa di-download disini.
Dataset ini berisi business domain yang sangat simpel. Isinya adalah perbandingan YearsExperience seorang karyawan dengan Salary yang didapatkannya.
Tugas dari Machine Learning dengan algoritma Simple Linear Regression ini nanti adalah (1) Adakah korelasi antara YearsExperience dengan Salaray?, (2) Bagaimana model korelasinya?, dan (3) Bagaimana bentuk grafik korelasinya?
Oke, ketiga hal diatas adalah Machine Learning dilihat dari sisi orang teknis. Lalu adakah sisi bisnis yang bisa dilihat dari kasus ini? Mengingat dataset yang digunakan pun diambil dari suatu perusahaan.
Sisi bisnis yang bisa dilihat dan bisa dibantu dengan Machine Learning kali ini adalah:
Demikian bagian 01 dari tutorial seri Simple Linear Regression ini.
Atribusi Anda berupaclapping,sharing, ngasihkomentar, danfollowingblog ini dapat menimbulkan efek samping berupa semangat menulis yang menggebu bagi saya
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:938/1*8C1VRyuQJsnxDxcAgUnlLg.png,"Regression, Data Science, Machine Learning, Simple Linear Regression"
https://medium.com/s/story/saya-bukan-data-scientist-saya-79f17c0d4404,"Saya Bukan Data Scientist, Saya ….",Dicky Bagus,2018-01-26T02:23:01.665Z,"Meskipun sudah bekerja di bidang data selama tujuh tahun (antibiogram reportinguntuk RSUD Dr. Soetomo Surabaya) dan menjadi Community Manager untuk Data Science Indonesia (DSI) regional Jawa Timur, belum berarti membuat saya menyandang predikatData Scientist.Jangan salah paham, benar setelah kata — katamedical docterdiatas akan terlihat predikat data scientist di bagian berikutnya. Nanti akan kita bahas lebih lanjut mengenai yang satu itu.
Sama seperti layaknya memulai skill baru, bagian mana yang telah dikuasai dan mana yang harus digali lebih lanjut? Kalo anda suka mengerjakan hampir seluruh pekerjaan dengan spreadsheet berarti sudah berada di jalan yang benar, terutama dalam mempersiapkan dataset. Namun tidak hanya menampilkan tabel — tabel yangfancynamun juga bermakna, apabila sudah mencapai macro programming berarti modal sudah cukup banyak untuk belajar lebih untuk menjadi data scientist. Apabila belum? jangan berkecil hati, masih banyak jalan menuju kesana.
Kita sudah mengantongi satu point, mulai dekat dengan data. Maka perjalanan dilanjutkan dengan membuka mesin pencari dan mengetikkan kata kunci “How To Become Data Scientist” (Beberapa artikel dalam bahasa Indonesia masih salah kaprah, jadi gunakan bahasa Inggris terlebih dahulu). Dari pengalaman pencarian saya ada beberapa kata kunci lanjutan yang harus dikuasai
So dimana saya?! Dari beberapa poin diatas lima skill tersebut sudah saya miliki, but i’m not quite good expert. yang paling penting apabila anda sudah mengambil jalan yang anda pilih, terus berusaha. Ingat saat ini yang dicari bukan anda lulusan mana, namun apa yang dapat anda berikan ke perusahaan / skill.
Life Consultan | Data Entusiast | Disruptor",,"Bahasa Indonesia, Data Science"
https://medium.com/s/story/exploratory-data-analysis-7b9b0234ba05,Exploratory Data Analysis,Elian Daiva,2018-07-20T08:15:52.502Z,"We help you solve problem and build prototype for your technology product.
Pada kesempatan kali ini, tim research Labtek Indie mencoba untuk melakukan analisa terhadap data dengan pendekatan Exploratory Data Analysis. Setelah sekian lama kami-kami ini berkutat dengan data-data internal melalui Excel atau Google Spreadsheet (dan yang paling terakhirAirtable), saya mencoba untuk menggunakan beberapa pendekatan data science dan machine learning untuk melakukan analisa.
Beberapa tutorial memang mengajarkan bagaimana cara melakukan EDA ini, beberapa diantaranya bisa ditemukan di Kaggle, platform berbagi notebook berisi script machine learning & data science yang cukup terkemuka. Untuk memulai EDA dengan pendekatan data science dan machine learning, beberapa tools dan library sudah tersedia di luaran sana. Beberapa diantaranya yang paling umum digunakan adalahnumpy,scikit-learn,pandas,matplotlib, danseaborn, yang kelimanya menggunakan python sebagai bahasa pemrograman.
Langkah pertama yang harus dilakukan tentunya adalah setup environment. Python library bisa menjadi sangat berantakan jika tidak di-isolatesesuai kegunaan dan penggunaannya. Untuk itu, dalam kasus sayaAnacondadigunakan sebagai environment manager untuk kekacauan instalasi package/library dan versi python yang saya gunakan. Sementara untuk editor, nampaknyaJupyter Notebook dari IPythonadalah editor python interaktif yang paling oke saat ini untuk explorasi skala riset. Langkah pertama ini usai setelah:
Langkah berikutnya tentunya berkaitan dengan data yang akan kita analisa, ambil contoh dataset yang banyak tersedia di website penyedia dataset. Beberapa bisa diunduh dariKaggleatau bahkanBuzzfeed. Saya bahkan terpukau denganrepositorydataset yang dimiliki oleh Buzzfeed, yang menunjukkan bahwa mereka adalah perusahaan media dengan berbasis data yangmature.
Setelah selesai memilih data yang diinginkan, saya mulai EDA yang tadi disebutkan di awal. Pada kesempatan kali ini saya gunakan data Pokemon.
Untuk mengetahui info lebih lanjut dari data/tabel yang baru saja dimasukkan, dapat divalidasi dengan mengetikkandata.info()
akan ditunjukkan jumlah dan tipe data yang tertampung dalam tabel tersebut seperti gambar di atas.
Jika sebelumnya data yang akan kita olah ini dikembangkan atau dibuat di MS Excel atau Google Sheets, kita dapat melakukan validasi untuk memastikan bahwa csv file yang kita import telah sesuai dengan mencetak beberapa data awal yang berada di dalam tabel dengan mengetikkandata.head(10)ataudata.tail()untuk mendapatkan beberapa data di akhir tabel.
Lebih lanjut, kita bisa melihat dimensi (bentuk) dari data yang kita miliki dengandata.shapedan mengamati jumlah feature yang tersedia untuk dianalisis dengan menggunakandata.columns.
Untuk memberikan gambaran statistik dari nilai setiapfeatures, kita dapat menggunakandata.describe()untuk mendapatkan data-data seperticount(jumlah data),mean(rata-rata),std(standar deviasi), nilai mininum, maksimum, hingga nilai-nilai tengah pada 25%, 50%, dan 75%.
Setelah kita mengetahui gambaran umum dari data/tabel yang kita miliki, kini saatnya kita beralih untuk pengolahan lebih jauh. Muncul pertanyaan, data mana saja yang butuh untuk diolah? Dari 12featuresyang tersedia dalam tabel data pokemon ini, data manakah yang paling memberikan pengaruh terhadap datafeatureslainnya.
Pandas librarysudah memberikantoolsuntuk melakukan analisa korelasi, dengan menggunakan fungsidata.corr().
Hasil pembacaan di atas, memberikan gambaran korelasi antarfeature.Dapat kita lihat sel yang mempertemukan specdef dan defense memberikan nilai yang cukup besar dibandingkan sel yang lain. Sama pula halnya denganspecdef-hpatauspecdef-specatk.Meskipun hal-hal tersebut dapat diprediksi sebelumnya, tapi dengan cara ini, kita sudah mendapat insight data-data mana saja yang cukup penting untuk diikutsertakan dalam pengolahan data lebih lanjut. Untuk mempermudah pembacaan, kita dapat melakukan mappingcorrelation tabletersebut melalui matplotlib pada baris perintah berikut:
Namun seperti halnya jargon klise:
“Correlation does not imply causation”
Metode ini tidak serta merta menjamin hubungan sebab akibat antara satufeaturedengan yang lainnya.
Toolsini hanya digunakan sebagai acuan untuk melakukan analisa dan memilah mana data yang patut menjadi insight diantaracomplex-noisy datayang kita punya.
Tim research Labtek Indie melakukan eksperimen ini ketika akan menganalisis data freelancers, namun karena data tersebut tidak dapat digunakan untuk berbagi, kami memutuskan untuk menggunakan data pokemon. Apabila Anda memiliki keinginan untuk berdiskusi lebih lanjut tentang Exploratory Data Analysis, silakan tinggalkan pesan di kolom komentar di bawah ini.
Penulis: Elian Daiva
Penyunting: Dea Chandra Marella
We help you solve problem and build prototype for your technology product.",https://miro.medium.com/v2/resize:fit:1087/1*cY23Dlpg6H2cmuR0RjuupA.png,"Exploratory Data Analysis, Learning, Data Analysis, Data Science, Labtek Indie Research"
https://medium.com/s/story/deepmind-ai-milik-google-yang-dapat-belajar-berjalan-sendiri-7dfa89d4b4ac,"DeepMind, AI Milik Google yang Dapat Belajar Berjalan Sendiri",HSE HIMA IF Telkom University,2017-10-05T06:17:40.007Z,"Anak perusahaan Google, Deepmind, telah mengembangkan sebuah AI yang dapat belajar berjalan. Yang menarik, AI tersebut bukan hanya berjalan pada dataran tetapi juga dapat berlari, melompat, memanjat dan menghindari rintangan. Model AI tersebut diberikan virtual sensor yang mendeteksi informasi tentang objek yang ada di sekitarnya.
Programmer dari AI ini hanya memberikan perintah untuk pergi dari suatu titik ke titik lainnya dimana ditengah perjalanan terdapat banyak rintangan. Seluruh prosesnya ditentukan oleh AI sendiri, meskipun kadangkali mengalami kegagalan tetapi dalam percobaan selanjutnya AI akan belajar dari kesalahannya tersebut dan melakukan cara lain dalam melewati rintangannya.
DeepMind AI sampai saat ini memiliki tiga jenis model, yang pertama model humanoid yang difokuskan untuk berjalan dan berlari, model berkaki empat yang difokuskan untuk melompat, serta model humanoid dua kaki yang berfokus untuk melewati rintangan.
Meskipun belum pernah ditunjukkan cara berjalan tetapi AI tersebut berjalan layaknya manusia sungguhan.
Sumber: Dirangkum dari TechInsider.
Hubungan Sosial Ekstra Kampus — Himpunan Mahasiswa S1 Teknik Informatika Universitas Telkom",https://miro.medium.com/v2/resize:fit:640/1*29GCl5uKB9yESD9dr79GCw.jpeg,"Berjalan, Belajar, Deepmind, AI, Google"
https://medium.com/s/story/screencast-singkat-tentang-tensorflow-js-7e7c3aa6506e,Tutorial Singkat Tentang Tensorflow.js,Riza Fahmi,2019-04-18T02:44:40.269Z,"Karena satu dan lain hal, saya tidak akan menerbitkan artikel di medium lagi. Teman-teman bisa cek kerizafahmi.comuntuk artikel-artikel terbaru dari saya.
Tensorflow.jsadalah sebuah library yang dibangun diatasdeeplearn.jsuntuk membuat moduldeep learninglangsung dari web browser!Deep learningsendiri adalah sebuah cabang darimachine learningdan jugaartificial intelligence. Dengan Tensorflow.js kita dapat membuat implementasi Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) dan lain sebagainya.
Tensorflowsendiri merupakan library yang ditulis dengan bahasa C++ dan biasanya digunakan dengan bahasa pemrograman Python. Dengan adanya tensorflow.js, kita sekarang sudah bisa menggunakan beberapa fitur tensorflow di sisi web browser tanpa harus dibebani oleh instalasi yang cukup ‘menantang’. Dengan Tensorflow.js kita tinggal melakukan instalasi dengannpm install @tensorflow/tfjsataupun juga dengan menggunakan CDN.
Berikut ini adalah cuplikan kode untuk memprediksi harga rumah berdasarkan luas tanah/luas bangunan.
Untuk mengetahui lebih lanjut silakan simak video berikut ini.
Artikel ini dirangkum dariepisode ke-18 randomscreencast.com.
Co-founder, Curriculum Director atHacktiv8.com. Podcaster atCeritanyaDeveloper.com. Screencaster atRandomScreencast.com",,"Tensorflowjs, JavaScript, Indonesia, TensorFlow, Machine Learning"
https://medium.com/s/story/sentiment-analysis-twitter-819d171a536,Sentiment Analysis Twitter,sugeng sanjaya,2017-11-15T06:11:28.620Z,Sentiman analitical twitter dengan menggunakan R,,Data Science
https://medium.com/s/story/mendemonstrasikan-ml-kit-untuk-firebase-di-android-81e343dbb698,Mendemonstrasikan ML Kit untuk Firebase di Android,Avin Riyan,2018-05-13T09:10:15.441Z,"Machine Learningsemakin mudah bagimobiledeveloper dengan diumumkannyaML KitpadaGoogle I/O 2018.
ML KitadalahmobileSDK yang menghadirkan pembelajaran mesin Google (Google’s Machine Learning) untuk aplikasiAndroiddaniOSdalam paket yangpowerfulldan mudah digunakan walaupun anda baru atau sudah berpengalaman dalam Machine Learing.
Anda dapat mengimplementasikan fungsi yang anda butuhkan hanya dalam beberapa baris kode dan tidak perlu memiliki pengetahuan mendalam tentangneural networkatau optimasi model untuk memulainya. Di sisi lain, jika anda pengembang ML yang berpengalaman, ML menyediakan API yang mudah digunakan untuk membantu anda menggunakan modelTensorFlow Litekhusus di aplikasi mobile anda.
ML kit hadir dengan satu set API yang siap dipakai untuk kasus penggunaan umum misalnya:mengenali teks, mendeteksi wajah, memindai barcode, melabeli gambar dan mengenalilandmark.API ini dapat dijalankan di perangkat atau di cloud, tergantung pada fungsinya. API memproses data di perangkat dengan cepat walaupun tidak ada koneksi jaringan, sementara API berbasiscloudmemanfaatkan teknologiGoogle Cloud Platform’smachine learninguntuk memberikan tingkat akurasi yang lebih tinggi.
Firebase menyediakan sample project aplikasi yang bisa kita gunakan sehingga kita bisa langsung membuka nya pada Android Studio lalu dapat dijalankan di HP android atau emulator, tetapi sebelumnya kita harus menghubungkan projek android dengan projek Firebase.
2. Setelah download selesai, extract filequickstart-android-master.zip
3. Buka Android Studio lalu pilihOpen an existing Android Studio project
4. Arahkan ke tempat dimana folderquickstart-android-masterberada,expandfoldernya lalu pilih yangmlkit. Anda juga bisa mencoba project Firebase yang lain nanti. terdapat fitur-fitur Firebase antara lain: auth, crash, database, admob, analytic, messaging, firestore, dll. Kita demo yang mlkit dulu ya.
5. Membuka project membutuhkan waktu yang sedikit lama tergantung koneksi dan performa laptop anda, jadi harap tunggu sebentar.
6. jika proses gradle sudah slesai namun masih error, pastikan versi sdk project sesuai dengan sdk yang anda punya, saya pake sdk versi 27 jadi tidak eror.
7. Menuju ke file build gradle pada level app untuk mengcopyapplicationId
atau bisa diganti sesuai keinginan anda, misal:com.cobacoba.mlkit, jangan lupa disync nowjika merubah apapun di settingan gradle.
8. Buka konsol firebasehttps://console.firebase.google.com, lalu buka salah satu projek firebase anda atau membuat projek baru.
9. Setelah projek dibuat, pilihAdd Firebase to your Android app..
10. lalu pastekan applicationId nya ke form yang ditentukan, klik tombolREGISTER APP.
11. klik Download google-services.json untuk mendownload config gile
12. copy filegoogle-services.json
13. Buka android studio, ganti view ke Project, paste kan kedalam folderapp
14. Setelah itu kita jalankan aplikasi nya(RUN)
Refrensi:https://firebase.google.com/docs/ml-kit/
Penulis diberqas.comdan pengelola kanal berqas 🧪🐟⚙️🐄🌳🪨🍞✈️",https://miro.medium.com/v2/resize:fit:1200/0*ce0ifxX3zcqouGGq.png,"Android, Android App Development, Ml Kit, Firebase, Machine Learning"
https://medium.com/s/story/visualisasi-data-perbandingan-jumlah-penduduk-negara-asean-1960-2018-839b9e9aaeec,Visualisasi Data Perbandingan Jumlah Penduduk Negara ASEAN 1960–2016 dengan Python,Muhamad Bayu Wilanda,2018-07-17T09:36:09.702Z,"Setelah kemarin kita telah mempraktikan caraVisualisasi Data Tingkat Inflasi Jakarta 2006–2012 dengan Python,pada kesempatan kali ini saya akan mempraktikan Visualisasi Data Perbandingan Jumlah Penduduk Negara ASEAN 1960–2016. Langsung saja kita mulai.
Unduh terlebih dahulu datasetnyadisini, kloning/unduh filenya,file.csvnya berada di folderdata.Selanjutnya bukaJupyter notebookmelaluiAnaconda,silahkan buat baru project di folder yang anda inginkan dengan mengkliknew>Python 2/3.
Silahkan ganti nama project sesuai keinginan anda. Saya memberi nama project dengan nama Visualisasi Jumlah Penduduk ASEAN. Lalu mulai untuk membuat kode.
Kode diatas digunakan untuk mengimport modul yang akan diperlukan pada tahap-tahap selanjutnya. Selanjutnya adalah mengimport data berekstensi.csvyang sudah anda unduh tadi dengan mengetikan kode di bawah.
Silahkan sesuaikan lokasi folder dimana filecsvanda berada, berikutnya tekan tombolrun,jika sudah maka anda bisa melihat isi file tersebut dengan mengetikkan
Kode diatas digunakan untuk menampilkan table dengan variabelcountry_data, dan fungsihead()digunakan untuk menampilkan data awal sebanyak 5 baris saja. Hasilnya akan seperti gambar di bawah.
Selanjutkan ketikan kode di bawah.
Kode diatas digunakan untuk mengganti nama judul kolom, yang awalnyaCountry NamemenjadicountrydanCountry Codemenjadicode.Potongan kode pada baris selanjutnya digunakan untuk menampilkan hasil dari penggantian nama.
Jika sudah lanjutkan kode dengan mengetikan kode di bawah ini
Kode diatas digunakan untuk menghapus kolom yang tidak digunakan untuk memvisualisasikan hasilnya. Pada baris selanjutnya adalah untuk melihat hasil dari penghapusan kolom. Jika berhasil maka hasilnya akan seperti gambar di bawah.
Silahkan ketikkan kode selanjutnya.
Kode diatas digunakan untuk insialisasi ulang negara-negara yang akan digunakan untuk visualisasi agar lebih mudah
penjelasan kode plt.plot(negara.year,negara.value/10**6),sebuah grafik dua dimensi membutuhkan garisXdanY,jadi bisa dibilang kode diatas artinyaplt.plot(x, y)dimanaXadalahyear, danYnya adalahvalue,kemudian bagainnegara.value/10**6berartisepuluh pangkat enam, yang artinya garisYakan ditampilkan dalam satuanjuta.Sementaraplt.xlabel(“Tahun”)danplt.ylabel(“Penduduk (Juta)”)berarti labelXdiberi namaTahun,, dan labelYbernamaPenduduk (Juta).
Jika berhasil maka hasilnya akan seperti gambar di bawah ini.
Sekian praktik kita. Sampai jumpa di praktik-praktik berikutnya.
GNU/Linux User | #Gooner",https://miro.medium.com/v2/resize:fit:1200/1*Y69lexAvXXZEGY3oqjXh5g.png,"Python, Data Science, Data Visualization"
https://medium.com/s/story/presale-revolusi-rangkaian-operasi-bisnis-kuno-dan-penekanan-biaya-rendah-bersama-namahe-841f1b647c42,Presale; Revolusi rangkaian operasi bisnis kuno dan penekanan biaya rendah bersama NAMAHE,Agus Ari Wibowo,2018-06-15T21:33:55.600Z,"Hallo, trader dan investor semoga keuntungan selalu berada menyertai anda. Oke kita akan langsung saja membahas inti dari artikel kali ini.
Ini adalah lanjutan review artikel saya yang sebelumnya, karena saya sangat tertarik dengan sebuah program yang akan dikerjakan oleh namahe.Kalau anda belum tahu tentang namaheanda dapat membaca dari artikel saya sebelumnya.
Namahe merupakan salah satu program bisnis yang akan sangat dibutuhkan mengapa demkian?Akan saya jelaskan, itu karena namahe memberikan sebuah peluang bisnis yang aman, mudah, transparan dan serta mempunyai biaya yang murah dibandingkan program sejenisnya.
Untuk mempermudah pemahaman anda tentang kebutuhan bisnis dengan hubungannya program yang digagas oleh namahe saya akan memberikan video dibawah ini supaya lebih mudah dipahami
Namaheadalah sebuah kerinduan dari penjual kecil atau pembeli kecil yang tidak dapat melakukan transaksi didalam bisnis globalisasi saat ini dan sebuah jawaban dari rangkaian kegiatan operasi manajemen yang tidak efektif serta tidak efisien selain itu namahe juga memiliki konsultan yang handal.
Namahe akan menciptakan sebuah pasar yang dapat mempertemukan penjual besar maupun penjual kecil dan pembeli besar ataupun pembeli kecil dalam satu pasar yang diakomodasi oleh namahe.
Untuk transaksi dipasar tersebut alat pembayaran harus menggunakan token NMH. Saya akan memberikan contohnya seandainya anda ingin melakukan pembelian dipasar namahe anda harus mengkoversi koin atau uang yang anda miliki dengan koin NMH.
Untuk urusan kegiatan manajemen akan menjadi lebih efektif dan efisien dengan bantuan blockchain dan AI. Semua akan menjadi lebih efektif, efiisien, dan transparan dengan biaya yang lebih murah. Ini merupakan sebuah revolusi dalam manajemen yang kuno.
Jika sebuah perusahaan menggunakan layanan namahe otomatis tidak perlu melakukan audit yang memerlukan waktu yang lama serta tidak perlu mengeluarkan biaya audit yang sangat besar.Oleh sebab itu,program ini akan sangat membantu bagi kegiatan operasi manajemen.
Untuk saat ini namahe masih dalam program presale ico dengan minimum pembelian 0,25 ETH. Pembelian token hanya dapat dilakukan dengan MEW, Meta Mask atau dengan dompet sejenisnya yang dapat support dengan ERC20.
Jangan pernahmenggunakan alamat dompet dari pertukaran langsung karena transaksi tersebut tidak akan masuk. Selain itu,token yang akan dijual memiliki soft cap sebesar 4 juta token dan hard cap sebesar 30 juta token sedang jumlah maksimal yang nanti akan beredar 120 juta jumlah tersebut tidak akan diperbaharui.
Untuk pembelian 1 ETH anda akan mendapatkan atau dapat dimiliki sebesar1000tokenNMHselain itu untuk pembelian sekarang ini anda akan mendapatkan bonus pembelian yang sangat besar untuk anda lewatkan, saya akan rinciannya dibawah ini:
Mulai dari tanggal 24 mei sampai 20 juni PRESALE
1.Minggu pertama anda akan mendapatkan bonus pembelian sebesar 20% dari total pembelian saya akan berikan contoh, jika anda melakukan pembelian 1 ETH maka anda akan mendapatkan 1000 X 20% =200 token bonus yang anda dapatkan.
2.Minggu kedua anda akan mendapatkan bonus pembelian sebesar 20% dari total pembelian saya akan berikan contoh, jika anda melakukan pembelian 1 ETH maka anda akan mendapatkan 1000 X 20% =200 token bonus yang anda dapatkan.
3.Minggu ketiga anda akan mendapatkan bonus pembelian sebesar 15% dari total pembelian saya akan memberikan contoh, jika anda melakukan pembelian 1 ETH maka token bonus yang akan anda dapatkan sebesar 1000 X 15% =150 token bonus yang akan anda dapatkan.
4.Minggu ketiga anda akan mendapatkan bonus pembelian sebesar 15% dari total pembelian saya akan memberikan contoh, jika anda melakukan pembelian 1 ETH maka token bonus yang akan anda dapatkan sebesar 1000 X 15% =150 token bonus yang akan anda dapatkan.
Jika anda melakukan pembelian disaat final sale anda akan mendapatkan bonus tetapi lebih kecil dibandingkan saat presale jika menurut saya pada saat seperti ini lah yang sangat tepat untuk melakukan pembelian karena kita akan mendapatkan bonus pembelian yang sangat besar.
Mulai tanggal 28 juni sampai dengan 31 juli FINAL SALE
1.Pada saat minggu pertama anda akan mendapatkan bonus pembelian sebesar 10% cara menghitung bonus sama seperti diatas.
2.Pada saat minggu kedua anda akan mendapatkan bonus pembelian sebesar 10% cara penghitungan bonus sama seperti diatas.
3.Saat minggu ketiga anda akan mendapatkan bonus pembelian sebesar 5% dari total investasi anda cara penghitungan sama seperti diatas.
4.Saat minggu terakhir atau minggu keempat anda tidak memiliki bonus yang akan didapatkan.
Untuk metode penghitungan bonus seperti ini maka dapat saya ambil kesimpulan bahwa pembelian lebih menguntungkan jika anda melakukan pembelian saat presale karena keuntungan bonus yang lebih besar atau dengan kata lain pembelian yang lebih itu merupakan pembelian yang terbaik.
Selain dari paparan diatas Namahe juga telah memiliki banyak mitra yang mempunyaireputasi yang baikdan mempunyaikredibilitas yang tinggisaya akan memberikan dua contoh mitra namahe jika ingin mengetahui lebih banyak mitra namahe anda dapat melihat dengan tautan website yang nanti akan saya sediakan dibawah.
Ini adalah dua contoh mitra namahe:
1. ADQURA > Mitra Teknologi
Konsultasi global terkemuka di bidang Pengambilan Keputusan Pelanggan menggunakan AI. Dengan kantor di Inggris, AS, Turki dan India, layanan Adqura merek bluechip terkemuka secara global. Membawa pengalaman keterlibatan AI dan pelanggan untuk memberikan solusi kelas dunia untuk Namahe.
2. SEWA BHARAT > Mitra Sektor Informal
SEWA Bharat terdiri dari keluarga organisasi SEWA untuk lebih lanjut hak-hak pekerja perempuan informal, mata pencaharian, kemandirian finansial, pendidikan, kesehatan dan jaminan sosial.
Saya akan menjelaskan tim yang menurut saya mempunyai pengaruh besar untuk kesuksesan Namahe yaitu:
1. Kumar Mudaliar (CO-fouder)
Memiliki lebih dari 15 tahun pengalaman di sektor perbankan, spesialiasi Keuangan, Strategi dan Operasi Bisnis.
Dia juga telah terlibat dengan program uji coba ‘One Pay’, sistem pembayaran terdesentralisasi menggunakan teknologi Blockchain yang dimiliki oleh Ripple.
Keahlian khusus terletak pada perencanaan dan analisis keuangan untuk implementasi proyek dengan pengalaman yang luas dalam Pengendalian Biaya untuk Rantai Pasokan dan Logistik.
Keterampilan terpenting yang kumar mudliar miliki semua jenis bisnis yang melibatkan pengiriman Proyek Peraturan yang berkaitan dengan Sistem Keuangan, Perubahan Produk dan Rekayasa Proses.
2. Ajay Lakhanwani (co-founder)
Berpengalaman General Manager dengan sejarah sukses yang ditunjukkan dalam E-niaga, Penjualan Eceran, B2B, dan Tanggung Jawab Sosial Perusahaan dalam peran kepemimpinan global dan regional. 17 tahun pengalaman bekerja di Asia dan Eropa.
Dua profil diatas tersebut sebagian kecil dari timyang akan menyukseskan program namahe masih banyak lagi tim yang ahli didalam bidangnya masing-masing dengan keterampilan yang telah teruji.
Anda dapat melihat seluruh tim melalui tautan nanti yang akan saya sediakan di website namahe, uraian diatas profil diatas merupakan sebagian kecil yang mereka dapat saya tampilkan jika anda ingin mengetahui lebih lanjut tentang keahlian anda dapat membacanya didalam website mereka.
tautan alamat:WEBSITE,TWITTER,TELEGRAM,BITCOINTALK,MEDIUM,WHITEPAPER,EXECUTIVE SUMMARY.
Ingin tahu informasi tentang penulis anda menghubungi saya melalui:
My Bitcointalk:Profesor_kodok,
My Facebook:Agus Ari Wibowo
“ Jadilah Investor Yang Bijak Dalam Melakukan Investasi Dana Anda Karena Banyak Investasi Yang Hanya Memberikan Gambaran Keuntungan Tanpa Memberikan Anda Sebuah Bukti”",https://miro.medium.com/v2/resize:fit:1199/1*fXjEn59-NUznNiQW3yHqMg.jpeg,"Audit, AI, Presales, Token Sale, Blockchain"
https://medium.com/s/story/machine-learning-contoh-artificial-neural-network-di-r-86bca99a4da1,Machine Learning : Contoh Artificial Neural Network Menggunakan Package nnet di R,akademi-ai,2025-06-03T13:09:50.283Z,"We are official team of Akademi AI Indonesia
Pada tulisan kali ini kita akan mencoba menerapkan artficial neural network (ANN) menggunakan package nnet menggunakan software R
Artificial Neural Network (ANN) dalam bahasa Indonesia disebut juga Jaringan Saraf Tiruan (JST) atau pada umumnya disebut juga hanya Neural Network adalah sebuah sistem jaringan yang dimodelkan berdasarkan sistem jaringan saraf manusia. Tujuan dari dibuatnya Neural Network adalah membuat sebuah sistem yang dapat belajar sendiri sesuai dengan datadan kondisi lingkungan yang kita berikan serta dapat memodelkan data statistic (curve fitting)non-linier, permodelan yang kompleks tersebut menghubungkan antara input dan output untuk menemukan pola-pola data, seperti metode curve fitting.
Pembuatan struktur Artificial Neural Network (ANN) atau Jaringan Saraf Tiruan (JST) diilhami oleh struktur jaringan biologi, khususnya jaringan otak manusia. Berdasarkan hal tersebut Artificial Neural Network (ANN) merupakan pembuatan model sistem komputasi informasi yang menirukan cara kerja jaringan syaraf biologis, yaitu dapat menyelesaikan masalah setelah dilakukan pembelajaran sebelumnya.
Arsitektur Artificial Neural Network terdiri dari input , hidden layer dan output
Udah dulu yaa Intro nyaa wkwkw.. karena kita disini akan lebih banyak menerapkan ANN pada aplikasi R hehe.. OK let’s go !!
Pada contoh ANN kali ini kita akan mencoba menerapkannya menggunnakan dara iris. buat temen-temen yang belum tau data iris kaya gimana, tingga ketikkan aja kata iris di R kaya gini..
Data iris diatas berjumlah 150 row/baris namun yang saya tampilkan hanya sampel 20 baris saja. Data iris terdiri dari 4 variabel yaitu Sepal Length, Sepal Width, Petal Length dan Petal Width dengan satu variabel target yaitu species dengan 3 jenis : setosa, virginica dan versicolor.
Langkah selanjutnya install package nnet di Rkemudian jalankan dengan fungsi library(nnet), lalu partisi data menjadi data train dan data test dengan cara melakukan pengambilan sampel secara acak sebanyak 75 data : 25 data untuk kelas setosa, 25 data untuk kelas virginica, dan 25 data untuk kelas versicolor. Disini kita akan menggunakan 75 data untuk data train dan 75 data untuk data test/validasi.
Selanjutnya kita akan membuat model dengan formula seperti di bawah ini dengan nama iris1 dan jumlah hidden layer sebanyak 2. Kita mencoba melakukan iterasi dari data train sampai mencapai nilai konvergen, disini saya mencoba menetapkan maximum iterasi 500 dan sudah mencapai nilai konvergen.
Temen temen bisa panggil model yang sudah kita buat tadi dengan menuliskan iris1.
Artinya kita sudah membuat arsitektur jaringan ANN dengan 4 input, 2 hidden layer, dan 3 output dengan bobot 19.
Kemudian kita akan melihat seberapa akurast hasil prediksi dari model yang telah kita buat terhadap data test dengan menggunakan perintah ini.
Misalkan kita membuat nama data baru yaitu y adalah data iris.test dengan variabel target species dan p adalah prediksi iris.test menggunkan model data iris1 dengan type “class’untuk klasifikasi. berikut adalah hasil klasifikasi menggunakan model yang sudah kita buat.
Hasil prediksi data test sudah cukup baik yaitu 25 data setosa dengan tepat di prediksi sebagai setosa, 25 data versicolor dengan teat diprediksi sebagai versicolor dan 23 data virginica dengan tepat diprediksi sebagai virginica dan 2 sisanya diprediksi sebagai versicolor. Probabilitas keakuratan model iris1 yang sudah kita buat sangat tinggi yaitu sebesar 97% dengan error 2%.
Kemudian apabila temen-temen ingin melihat arsiektur jaringan yang kita gunakan bisa menngunakan perintah berikut.
Naahh model kan udah kita buat tuh.. dengan probabiitas keakurasiannya sangat tinggi yaitu 97%. Sekarang kita akan coba melakukan prediksi menggunakan data baru tetntunya dengan model yang udah kita buat.. Oke kita coba yukkk :))
misalnya kita puna data baru kaya gini !
Apakah dengan masing-masing variabel bunga iris dengan ukuran nilai diatas masuk kategori Setosa, Virginica, atau bahkan Versiclor ? saya sih nggak tau haha.. makanya ayo kita prediksi hehe
sebelumnya kita harus membuat data baru tersebut dalam bentuk data.frame agar bisa diproses nantinya.
Nah data baru kita udah berbentuk data.frame pastinya.. sekarang kita coba lihat hasil prediksinya
Bisa kita liat bahwa setosa memiliki probabiitas paling tinggi yaitu 99%. ini artinya bunga baru yang kita punya tergolong kategori SETOSA.
Oke sekarang kita ke step selanjutnya.. sebenarnya udah selesai sih kalo untuk prediksinya, tapi kalo temen-temen ingin melihat pengaruh jumlah hidden layer terhadap error klasifikasi makan temen temen bisa buat looping atau perulangan kaya gini !!
Setelah itu temen-temen cek, dengan jumlah hidden layer berapa evaluasi model yang akan kita coba lagi nanti berdasarkan data yang kita punya. gini caranya..
Sintaks diatas udah langsung outputnya dengan jumlah hidden layer 2 sampai 5. Ternyata error dengan jumlah hidden layer kecil menggunakan hidden layer sebanyak 2 dan 3.
nah gimana kalo kita buat plot aja untuk melihat jumlah hidden layer misalnya dari 2 sampai 10 hidden layer, mana yang menghasilkan jumlah error yang paling kecil. ini sintaksnya.
Ternyata setelah kita liat pada grafik diatas, dari percobaan hidden layer 2 sampai 10 niai error terkecil dihasilkan dengan meggunakan jumlah hidden layer sebanyak 2.
Untuk bantuan lebih lanjut skripsi, thesis, project, konsultasihubungi kami.
We are official team of Akademi AI Indonesia
Medium Publicationhttps://medium.com/akademi-ai",https://miro.medium.com/v2/resize:fit:296/1*hBWBmMRplnfIulqsJzSuhQ.png,"Deep Learning, Machine Learning, Artificial Neural Network"
https://medium.com/s/story/investasi-taman-tanam-kurma-telp-wa-0822-4069-7469-investasi-kavling-taman-tanam-kurma-86d4d8dd76fe,"Investasi Taman Tanam Kurma, Telp / Wa 0822–4069–7469, Investasi Kavling Taman Tanam Kurma",basirul akbar,2018-08-11T05:08:00.397Z,"Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Sebagai seorang muslim, segala macam kegiatan khususnya dalam kegiatan perekonomian harus dilakukan sesuai dengan ketentuan hukum islam. Hal yang sama juga berlaku saat Anda akan mencari sebuah investasi masa depan.
Carilah investasi berbasis syariah yang bebas riba dan semua tata cara pelaksanaan dilakukan sesuai dengan syariat Islam.Kavling Taman Kurmasebagai salah satu contoh investasi syariah paling baik karena sudah pasti bebas riba, DP dan Angsuran juga tidak akan dibuat ribet, dan juga sudah memilikilegalitas.
Dengan memilih investasi berbasis syariah, maka hasil yang akan Anda dapat pun sudah pasti terjamin halal. Dengan demikian, kita juga akan hidup jauh lebih tenang di masa depan bersama anak cucu Anda.
Memang benar adanya jika investasi menjadi sebuah kepentingan saat ini mengingat persaingan ekonomi yang begitu berat, kemajuan teknologi setiap hari yang juga berdampak pada segala macam kebutuahn hidup yang terus meningkat sementara tidak akan selamanya Anda bekerja, pun tidak akan selamanya kita memimpin sebuah perusahaan atau membuat sebuah usaha.
Pasti akan ada masa dimana semua diganti oleh Orang yang baru dan di masa tua Anda, apa yang bisa Anda lakukan?
Masa tua adalah masa dimana harusnya Anda beristirahat dari segala macam hiruk pikuk dunia. Namun, tidak menutup mata pula jika kebutuhan itu pasti dan akan selalu ada hingga Anda tutup usia.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:1200/1*jyphSkm0h8U2I_v4wSoJTQ.jpeg,"Marketing, Marine, Movies, Machine Learning, Management And Leadership"
https://medium.com/s/story/ml-2-jenis-jenis-learning-dalam-ml-88ba82973067,ML#2: Jenis-jenis Learning dalam ML,Dwi Satria (Ibé),2018-06-23T20:31:16.791Z,"* Tulisan ini diadaptasi dari blog saya yang lama.
Halo! :)Dalampost sebelumnya, saya telah menulis tentang definisi baku dan beberapa contoh penerapanmachine learning. Pada post ini, saya akan membahas lebih jauh tentang jenis-jenislearningdarimachine learning,yaitu Supervised Learning, Unsupervised Learning, Semi-supervised Learning, dan Reinforcement Learning.
Supervised Learningadalah tipelearningdi mana kita mempunyai variable input dan variable output, dan menggunakan satu algoritma atau lebih untuk mempelajari fungsi pemetaan dari input ke output.
Goal-nya adalah untuk memperkirakan fungsi pemetaannya, sehingga ketika kita mempunya input baru, kita dapat memprediksi output untuk input tersebut.
Proses dari sebuah algoritma belajar daritraining datasetdapat diumpamakan sebagai seorang guru yang mengawasi (supervising) proses belajar. Kita tahu jawaban yang benar, dan algoritma secara iteratif membuat prediksi pada data latih (training data) dan dikoreksi oleh ‘guru’nya.
Learningberhenti ketika algoritma mencapai level performansi yang diterima. PermasalahanSupervised Learningdapat dikelompokkan menjadi masalah regresi (regression problem) dan masalah klasifikasi (classification problems).
Berbeda dengansupervised learning,unsupervised learningadalah tipelearningdi mana kita hanya mempunyai data masukan (input data) tetapi tidak adaoutput variableyang berhubungan.
Goaldariunsupervised learningadalah untuk memodelkan struktur dasar atau distribusi dalam data dengan tujuan untuk mempelajari data lebih jauh lagi, dengan kata lain, adalah menyimpulkan fungsi yang mendeskripsikan atau menjelaskan data.
Berbeda dengansupervised learning, di sini tidak ada jawaban yang ‘dibenarkan secara terarah’ dan tidak ada ‘guru’. Algoritma dibiarkan dengan rancangannya sendiri untuk mengetahui dan menyajikan struktur menarik yang ada dalam data.
Lebih jauh lagi, permasalahanunsupervised learningdapat dikelompokkan menjadiclustering problemsdanassociation problems.
Semi-supervised Learningadalah tipelearningdi mana kita mempunyai data masukan (input data) dalam jumlah besar dan hanya beberapa dari data tersebut yang dilabeli. Permasalahan ini berada di antarasupervised learningdanunsupervised learning.
Reinforcement Learningterjadi ketika kita menyajikan algoritma dengan contoh yang kekurangan label, tetapi kita dapat menyertakan contoh denganfeedbackpositif atau negatif bergantung pada solusi yang ditawarkan oleh algoritma tersebut.
Denganreinforcement learning, mesin dansoftwaredapat menentukan tingkah laku ideal terhadap sebuah konteks yang spesifik secara otomatis, dengan tujuan untuk memaksimalkan performanya. Umpan balik (feedback)‘simple reward’dibutuhkan untuk mesin mempelajari tingkah lakunya; hal ini disebutreinforcement signal.
Referensi :[1] Pang-Ning Tan, M. Steinbach, and V. Kumar; Introduction to Data Mining.[2] N. J. Nilson ; Introduction to Machine Learning.[3] P. Flach; Machine Learning — The Art and Science of Algorithms that Make Sense of Data.[4]http://reinforcementlearning.ai-depot.com/
*) Tulisan ini dirangkum dari tugas matakuliah Machine Learning semester ganjil 2017.",https://miro.medium.com/v2/da:true/resize:fit:500/0*8dxaEsD3Vk_CKnmg.gif,"Ngomongin Komputer, Artificial Intelligence, Machine Learning, AI"
https://medium.com/s/story/teknologi-tentu-sangat-erat-kaitannya-dengan-inovasi-894f5556448b,Tanpa Judul,Anissa Putri Dinanti,2018-07-18T11:58:33.474Z,"Teknologi tentu sangat erat kaitannya dengan inovasi. Inovasi-inovasi yang dilakukan bertujuan untuk memudahkan manusia dalam kehidupan sehari-hari. Setiap hari di dunia, jutaan inovasi dikembangkan oleh manusia. Dan pada kesempatan kali ini saya akan membahas inovasi yang sedang giat-giatnya dibentuk oleh para penemu yaitu, Artificial Intelligence.
Artificial Intelligence adalah kecerdasan buatan yang ditambahkan oleh manusia ke suatu sistem agar sistem tersebut dapat mengatur dirinya sendiri sesuai kecerdasan yang ditanamkan. Kecerdasan buatan ini biasanya ditanamkan kepada suatu komputer atau mesin agar mesin tersebut dapat melakukan pekerjaannya seperti dilakukan oleh manusia.
Kebanyakan orang menganggap Artificial Intelligence hanya sekedar dalam bidang robotika. Namun, sebenarnya AI banyak digunakan dalam berbagai aspek lainnya. Contohnya dalam berbagai aplikasi dalam perangkat lunak, dalam fitur pada mesin-mesin canggih yang sering kita gunakan sehari-hari, dan bahkan ada juga dalam bidang militer.
Studi dari Artificial Intelligence saat ini baru dikembangkan sampai bagaimana mesin-mesin di sekitar kita dapat berfungsi secara otomatis. Mesin-mesin ini dapat berpikir sendiri tanpa kita suruh. Contohnya adalah bagaimana perkembangan mobil saat ini yang dapat bergerak atau berkendara sendiri tanpa kita gerakkan. Mobil tersebut memiliki sistem kendali sendiri yang dapat mengendalikan mesin-mesin dari mobil itu sendiri tanpa kita perlu campur tangan lagi.
Pada mesin-mesin lainnya juga digunakan Artificial Intelligence dalam skala kecil seperti ini. Seperti bagaimana dalam suatu rumah lampunya dapat menyala sendiri jika ada orang di dalamnya dan mati sendiri secara otomatis ketika kita sudah tidak berada dalam ruangan. Dan sistem-sistem seperti pemindai wajah yang sekarang digunakan pada sistem keamanan handphone atau kemananan rumah.
Jadi, Artificial Intelligence ini sangat banyak digunakan dalam kehidupan sehari-hari kita dan dapat sangat memudahkan kita dalam beraktivitas. Namun, dalam pengembangannya terdapat banyak pro dan kontra. Mengapa demikian?
Dilihat dari aspek pengembangannya, Artificial Intelligence mempunyai aspek yang sangat besar untuk mengalami perkembangan yang sangat signifikan. Perkembangan ini masih dapat jauh dikembangkan dari perkembangannya sekarang ini yang sudah beredar.
Hal itu menimbulkan banyak pro dan kontra untuk pengembangannya. Semakin dikembangkannya Artificial Intelligence dalam skala yang lebih besar, hal itu dapat membuat seluruh kegiatan di dunia ini bergantung pada Artificial Intelligence itu sendiri. Kita tidak tahu seberapa besar kekuatan Artificial Intelligence itu sehingga para ilmuwan pun sangat berhati-hati untuk mengembangkannya lebih jauh.
Para ilmuwan tentunya khawatir akan Artificial Intelligence itu sendiri yabg kemampuannya dikhawatirkan akan melebihi kemampuan manusia pada umumnya. Dan bagaimana Artificial Intelligence itu dikhawatirkan drngan kecerdasannya sendiri akan mengupgrade kemampuannya sehingga dapat melebohi manusia. Dikhawatirkan juga kemampuannya akan membuat lapangan kerjaan manusia menjadi sedikit, dan bagaimana Artificial Intelligence itu sendiri dapat mengungguli manusia dan dunia dapat diputarbalikkan. Dan juga, penyalahgunaan AI itu sendiri oleh oknum-oknum tertentu yang dikhawatirkan dapat terjadi.
Jika dilihat dari sisi kontra, mungkinnterlihat menyeramkan. Namun, apabila para ahli dapat mengontrol Artificial Intelligence itu sendiri, tentunya kecerdasan buatan ini akan semakin membantu kita untuk sesuatu hal yang lebih besar. Untuk perkembangan besar ini, tentunya dibutuhkan juga perilaku manusia yang mengerti Artificial Intelligence ini. Masyarakat juga perlu diedukasi agar siap untuk pengembangan Artificial Intelligence secara besar-besaran agar tidak dapat dibodohi oleh kecerdasan buatan ini.
Jika masyarakat dunia sudah seluruhnya siap dan teredukasi dengan Artificial Intelligence dan teknologi. Tidak ada salahnya untuk pengembangan Artificial Intelligence secara lebih jauh. Karena perkembangan teknologi tentunya tidak dapat dihentikan. Setiap harinya ribuan inovasi berlomba-lomba untuk diluncurkan kepada dunia. Bukan saatnya kita berhenti untuk berinovasi. Karena inovasi itu sendiri adalah bagian dari diri kita, diri kami, diri saya, sebagai UNIX2017.",,"Inovation, Artificial Intelligence, Technology"
https://medium.com/s/story/pemanfaatan-data-pada-computer-vision-89c46fa65807,Pemanfaatan Data pada Computer Vision,Harry Akbar Ali Munir,2018-02-26T15:26:28.043Z,"Sudah menjadi hal yang biasa pada saat ini di perangkat pintar yang seseorang miliki cukup banyak gambar atau citra yang tersimpan di dalamnya. Gambar tersebut tidak hanya disimpan, namun juga dapat dibaca oleh perangkat pintar kita. Gambar yang ada di perangkat pintar tersebut dapat dilihat informasi apa saja yang dimiliki oleh gambar tersebut. Komputer dapat melihat jika di suatu gambar terdapat kebun, tulisan, bahkan dapat membaca ekspresi pada wajah manusia. Kemampuan komputer dalam menerjemahkan gambar menjadi suatu informasi disebutComputer Vision, yang menggunakan data sebagai alat bantu dalam menerjemahkan suatu gambar.
Suatu gambar atau citra terdiri daripixel-pixelyang mewakili suatu data dari citra tersebut. Data-data yang tergabung dapat membentuk suatu pola tertentu yang kemudian dapat dijadikan informasi baru yang dapat diolah. Sebelumnya,pixel-pixeltersebut diolah terlebih dahulu agar menjadi data yang lebih terstruktur. Sehingga data-data dapat dijadikan sebagai suatuinputpada algoritmamachine learningyang telah memiliki dataset sebagai penentu informasi apa yang terdapat pada citra.
Pada saat ini, penggunaancomputer visioncukup banyak diterapkan pada perangkat pintar. Contohnya adalah pada saat melakukan registrasi akun di suatu aplikasi. Kita diminta untuk mengambil foto dari kartu identitas kita. Dengancomputer vision, aplikasi tersebut dapat mengambil informasi identitas kita yang masih berupa gambar menjadi suatu teks yang secara otomatis dimasukkan ke dalamformpengisian registrasi akun aplikasi tersebut, sehingga kita tidak perlu repot mengisi informasi identitas yang cukup banyak. Ada pun penggunaancomputer visiondigunakan di bidang ilmu kedokteran untuk mendeteksi suatu penyakit, dan juga pada galeri foto yang ada dismartphonedengan melakukan pengkategorian album otomatis berdasarkan kesamaan pola yang ada pada foto-foto di galeri.
Pemanfaatan data tidak hanya bisa kita dapatkan langsung berupa teks, namun kita juga bisa mendapatkan data dari suatu gambar. Sehingga dengan adanyacomputer vision, pekerjaan manusia dapat terbantu di berbagai bidang karena adanya informasi yang dapat diberikan otomatis oleh komputer kita. Dengan begitu, tidak hanya makhluk hidup yang dapat melihat, namun komputer pun juga dapat melihat.
Self development interest | Software Engineer |harryakbaram@gmail.com",https://miro.medium.com/v2/resize:fit:1200/1*tGYW6Kg-QhZsSAJTu6HYsQ.png,"Artificial Intelligence, Image Processing, Computer Vision, Neural Networks, Machine Learning"
https://medium.com/s/story/mengenal-convolutional-neural-network-8bd207ad4a8d,Mengenal Convolutional Neural Network,Richard Dharmadi,2018-04-23T15:55:40.521Z,"Extending Vision Beyond Imagination
Disclaimer:Tulisan ini hanya akan menjelaskan konsep dasar dan intuisi dari Convolutional Neural Network (CNN). Tulisan ini tidak membahas implementasi dari CNN menggunakan framework tertentu.
Part of CNN series:Part I: Mengenal Convolutional Neural Network (you are reading it now)Part II:Mengenal Convolutional Layer dan Pooling Layer
Sudah menjadi kemampuan natural manusia dalam memahami apa yang kita lihat —to make sense of what we actually see.Sebagai contoh, saat kita melihat gambar di bawah ini, kita dapat dengan mudah mengidentifikasi objek-objek yang ada dalam gambar tersebut, beserta dengan hierarki nya.
CNN adalah sebuah teknik yang terinspirasi dari cara mamalia — manusia, menghasilkan persepsi visual seperti contoh diatas. Penelitian oleh Hubel dan Wiesel [1] menunjukkan bahwa neuron dalam korteks visual pada otak mamalia tersusun menjadi sebuahtopographical map, yang setiap level nya fokus pada karakteristik tertentu. Atau dengan kata lain, sebuah gambar direpresentasikan ke dalam level ektraksi fitur (featural hierarchy)yang membentuk gambar tersebut secara keseluruhan.
Sesuai dengan namanya, CNN memanfaatkan proses konvolusi. Dengan menggerakan sebuah kernel konvolusi (filter) berukuran tertentu ke sebuah gambar, komputer mendapatkan informasi representatif baru dari hasil perkalian bagian gambar tersebut dengan filter yang digunakan.
Mengambil contoh gambar seorang anak kecil di atas, hasil dari proses konvolusi pada gambar tersebut dapat diilustrasikan seperti berikut ini:
Setiap gambar kecil dari hasil konvolusi tersebut kemudian dijadikan input untuk menghasilkan sebuah representasi fitur. Hal ini memberikan CNN kemampuan mengenali sebuah objek, dimanapun posisi objek tersebut muncul pada sebuah gambar — atau biasa disebuttranslation invariance.
Proses ini dilakukan untuk semua bagian dari gambar, dengan menggunakan filter yang sama. Dengan kata lain, setiap bagian gambar akan memiliki faktor pengali yang sama, atau dalam konteksneural networkdisebut sebagaiweights sharing.
Hasil yang didapat dari proses diatas adalah sebuahactivation map.Untuk mendapat representasi gambar yang lebih baik, proses konvolusi diulang dengan filter yang berbeda untuk menghasilkanactivation maplain.
Tentunya tidak semua bagian dari gambar adalahobject of interestyang kita cari — beberapa bagian dari gambar merupakanbackground. Untuk mengurangi jumlah parameter yang perlu diperhitungkan, dilakukan prosespooling.
Pada umumnya,poolingyang dilakukan adalahmax pooling, atau mengambil nilaipixelterbesar di setiappooling kernel.Dengan begitu, sekalipun mengurangi jumlah parameter, informasi terpenting dari bagian tersebut tetap diambil. Metodepoolinglain yang umum digunakan diantaranyaaverage pooling, L2-norm pooling[2].
Proses konvolusi danpoolingdapat diulang beberapa kali hingga menghasilkan sebuahConvolutional Neural Network.Diantaraconvolutional layerdanpooling layerumumnya terdapatactivation layer(e.g. ReLU, tanH, etc.) yang banyak ditemukan padaartificial neural network.
Tidak ada aturan khusus perihal berapa banyakconvolution layeryang diperlukan, atau dimana sebaiknya melakukanpooling,untuk menghasilkan representasi fitur yang paling baik. Seberapa besar dan berapa banyak filter yang harus digunakan? Metodepoolingatau jenisactivation functionyang paling baik?It’s all fully experimental. I mean hey, that’s what makes machine learning is fun, innit? :)
*artikel ini banyak mendapatkan inspirasi dari artikel tentang topik yang sama dari Adam Geitgey [2]. Konsep dariconvolutional layerdanpooling layerakan dibahas lebih detail dipostselanjutnya.
Extending Vision Beyond Imagination
Most of my professional work has been related to how technologies can help people - first as an engineer, and later as a product and growth strategist.",https://miro.medium.com/v2/resize:fit:1200/1*koHKIk6yutxUmCM-pgIydg.jpeg,"Bahasa Indonesia, Machine Learning, Indonesian, Computer Vision"
https://medium.com/s/story/chatbot-as-an-internet-of-thing-8c541ef0b31e,Chatbot as an Internet of Thing,Ron Ashrovy,2017-10-29T06:37:32.599Z,"Bagi yang belum mengetahui apa ituInternet of thingatau biasa di singkat IoT, sederhananya adalahsmartphoneanda mampu “berinteraksi” dengansmartphoneyang lainnyatanpa anda perintah. Dengan kata lainsmartphoneanda seperti makhluk hidup yang mampu berkomunikasi mandiri. IoT juga yang membuat kita mengenal iPv6 dan jaringan 5G. Tidak hanyasmartphone,tetapideviceyang lain juga mampu berkomunikasi.
Sounds like movie right? Tapi itulah faktanya
Pada tahun 2020 diestimasiakan ada lebih dari 20 milyardeviceyang akan saling terkoneksi nantinya. Fokus pada artikel ini adalah menjelaskan tentangChatbotdanMachine Learningselamat membaca.
Pada artikel-artikel saya sebelumnya, saya sudah mendefinisikan seperti apa itu chatbot. Intinya adalah Chatbot adalah sebuah sistem dialog yang dirancang untuk berkomunikasi dengan manusia. Pada saat ini beberapa chatbot sudah mampu berkomunikasi secara langsung kepada manusia, baik menggunakan suara atau text, tetapi sebagian lainnya masih sepertisearch enginepada umumnya. Berikut adalah beberapa langkah yang dilakukan oleh chatbot untuk memproses informasi dari manusia:
Langkah-nya ialah mengubah inputanusermenjadi konteks yang dimengerti oleh chatbot. Bagaimana caranya? Menggunakaninput recognizersdandecodersyang menganalisis ucapan, text bahkan gesture. Berikutnya Kita mengaplikasikanNatural Language Processinguntuk meng-analisis text dan mencari semantic. Input diatur dan diproses olehdialog manageruntuk memasitkan bahwa alur informasi sudah tepat dari dan kepadauser.
On the other hand,dialog managerjuga memastikan bahwa inputan yang diberikan pecahkan olehtask manager. Selanjutnya,output managerakan menerjemahkan hasilnya menjadi output yang dapat diterima oleh manusia.Natural language generatorberfungsi meniru cara manusia berinteraksi, dari tulisan hingga mimik visual.
Pertanyaan selanjutnya, pada platform apa chatbot mampu bergerak didalam lingkup IoT, well diantaranya adaFacebook Messengeryang SDK-nya disediakan dan mampu meng-integrasikan layanannya sendiri. Sebagai contoh penulis akan mengambil integrasi antara Uber dan Facebook Messenger yang mampu membuat kita memesan mobil dan menentukan rute sekaligus mengetahui jumlah yang harus dibayarkan.
Saya coba jelaskan dengan bahasa yang lebih familiar, di Indonesia kita memiliki aplikasi yang tidak asing ditelinga kita yaitu gojek. Nah menariknya, selama ini kita memesan gojek dengan aplikasi-nya tersendiri bukan dengan chatting, kita juga perlu mencari lokasi kita dan lokasi tujuan. Apabila itu dihandle dengan chatbot, kita cukup mengatakan “dari posisi saya saat ini ke gedung LKPP” selesai! Apabila alamatnya sangat spesifik anda cukup input kan atau cari lewat map.
Dari hal ini kita mengambil sebuah kesimpulan bahwa sebuah sistem seperti chatbot denganmachine learningmembutuhkan peranannatural language processinguntuk mengkonversi inputan menjadi bahasa mesin dan sebaliknya.
Belakangan ini beberapa perusahaan berusaha mengembangkan chatbot agar terasa mirip dengan manusia dan berkompetisi satu sama lainnya. Jika kita flashback kebelakang bahwa chatbot awalnya dibangun dengan menggunakanpattern recognitionataurule based expression matchingataumachine learningyang simpel, yang intinya chatbot berusaha menangkapkata kunciuntuk memberikan jawaban kembali kepada user. Hal ini yang menyebabkan chatbot terasa cukup kaku didalam membahas suatu topik terkonsentrasi sekalipun.
Chatbot yang baru lebih terasa cerdas dengan menggunakandeep learningyang mampu menganalisis inputan dariuserdan juga menghasilkan sebuah jawaban. NLP (Natural Language Processing) berperan menerjemahkan input dan ouput pada bahasa yang di mengerti oleh manusia dan mesin. Masalah yang sering terjadi saat ini ialah chatbot seringkali mengalami kesalahan gramatical didalam menjawab sebuah pertanyaan yang diberikan. Untuk mengatasi hal ini diperlukanpelatihanyang luar biasa lama lama dan mahal.
Bentuk pertanyaan juga bisa menjadi masalah pada chatbot. Pertanyaan yang pendek bisa lebih cepat di proses sedangkan pertanyaan panjang lebih sulit diproses karena NLP harus menangkap maksud dari pertanyaan tersebut kemudian me-ringkas itu. Selain me-ringkas berikut ini adalah contoh-contoh dari implementasi NLP yang bisa dipelajari:
Peringkasan terkait dengan jumlah inputan yang besar diringkas menjadi pendek dan tepat, seperti pada contoh padabot ini.
Lebih tepatnya ini bot yang mejawab pertanyaan general atau pertanyaan terkonsentrasi
Jujur pada bagian ini saya kurang begitu mengerti, pada artikel aslinya menyebutkan bahwa benda-benda dan kata-kata saling berhubungan dengan contoh “The Office is situated in Rotterdam”. Bot mampu menyebutkan kata lain sesuai dengan maksudofficetersebut. Mungkin ada yang bisa membantu saya menerjemahkan ini?
Seperti yang kita tahu, bot bertanggung jawab untuk menghubungkan makna dengan sebuah kata yang benar dan tata bahasa yang bersifat ambigu
Dikarenakan setiap bot memiliki morfologi yang berbeda, chatbot harus bisa memisahkan menjadi morfem-morfem secara individual
Makna kalimat atau bahasa alami manusia yang terkait. Bisa dibilang ini adalah pemahaman dan cara untuk merespon dari input pertanyan
Terkait dengan struktur teks, tanda baca, penggunaan spasi dan lain-lain yang bersifat teknis.
Chatbot harus bisa mendeteksi polaritas emosional subjek yang dibicarakan oleh manusia. Yang nantinya akan dipaparkankan dari jawaban yang dilontarkan oleh chatbot itu sendiri untuk lebih ramah terhadap emosi manusia.
Seperti halnya NLP,Machine learningmerupakan salah satu AI pada komputer science, hanya saja,machine learningbisa diaplikasikan dimana saja, sedangkan NLP lebih fokus terhadap pengoalahan bahasa alami (seperti chatbot). Didalammachine learningmemiliki dua jenis algoritma pembelajaran, yaitusuperviseddanunsupervised. Untuk jenissupervisedsendiri merupakan algoritma yang input dan hasilnya merupakanvalueyang diinginkan, yang berarti algoritma jeneisinimenetapkan fungsiyang disimpulkan ke data sehingga data yang baru sekalipun tetapmenghasikanoutput yang sama. Terbalik denganunsuperviseddimana mesinnya sendiri yangmemberikan fungsi untuk yang menyimpulkan datamelalui analisis dan ekstrapolasi pola darirawdata. Hal yang perlu kita kenal lainnya didalammachine learningyaitu terdapat layer yang dapat menganalisis data secara hirarki kemudian mengekstrak baik pada algoritmasuperviseddanunsupervised. Dan jugahidden layeryang adalah bagian dari pengolahan data danneural network.
Merupakan salah satu dari algoritma pembelajaran padamachine learningyang memiliki layer-layer untuk melakukan analisis dan mempelajari data.
Hampir disetiaphidden layermencoba mendeteksi pola pada gambar, begitu pola terdeteksi maka layer berikutnya akan melanjutkan perkerjaannya. Gambar diatas adalah contoh yang bisa dijadikan gambaran, dimana pada layer pertama mencoba mendeteksi pada bagian tepi, kemudian layer berikutnya menggabungkan dengan sisi lain yang ditemukan, hingga pada akhirnya mendeteksi pola roda atau dan jendela. Secara garis besarnya bisa dibilang semakin banyak layernya maka akan semakin kompleks sistem dan semakin akurat hasilnya (walau pun tidak selalu seperti itu, anda bisa cek di tulisan blog saya sebelumnya).
Berikut ini adalah beberapa algoritma pembelajaran padamachine learningyang umum ditemukan:
Pada algoritma ialah bagaimana membuat keputusan berdasarkandecision treedari kemungkinan dan konsekuensinya, hingga biaya, peluangh dan utilitas. Metode ini memungkinan untuk melakukan pendekatan secara logis dan bertahap untuk mencapai kesimpulan yang cocok.
Menerapkan teorema Bayesian untuk masalah regresi dan klasifikasi yang terkait probalilitas. Algoritma ini mencoba menunjukkan hubungan porbalilistik antra variabel yang berbeda kemudian menentukannya.
Lebih cocok dipakai untukstastistical machine learning, algoritma regresi akan mencari lalu memodelkan hubungan antara variabel. Dengan meng-observasi relasi maka kita akan bertujuan membentuk sebuah fungsi. Yang berarti akan lebih banyak variabel yang perlu diamati.
Merupakan algortima yang digunakan untuk melakukan pengelompokan titik padademensional plane. Pengelompokan dilakukan dengan menciptakanhyperplaneyangmemisahkan kelompok dengan margin selebar mungkin.
Menggabungkan beragam algoritmasupervised learning. Dengan kombinasi dari model yang berbeda untuk memproduksi hasil yang lebih baik. Dengan mengkombinasikan beberapa mtode kita bisa meng-handle biasdengan model-model tertentu.
Tujuan dari algoritma mengelompokkan data yang ada ke dalam kelompok-kelompok, dimana data-data yang mirip dibandingkan dengan kelompok lain. Metode pengelompokan yang lebih penting adalah hirarkis, sentroid, distribusi dan densitas.
Ini adalah aturan yang dibangun antara itemstes dan setiap transaksi untuk item ini. Antara relasi antara X dan Y, aturan ini ditemukan untuk mengobservasi database dengan itemset dan item therein.
Terinspirasi dari otak manusia, algoritma ini berinteraksi dan berkomunikasi satu sama lainnya. Disetap komunikasi merupakan berat (weighted) dari pembelajaran sebelumnya dan setiap inputan baru akan diperlakukan dari pembelajaran sebelumnya. Algoritma ini yang nantinya akan dikenal sebagai deep learning untuk pembelajaran lebih detail.
Merupakan jumlah variabel didalam data dan dimensi yang dimilikinya. Jenis analisis ini bertujuan mengurangi jumlah dimensi dengna variabel terkait sedangkan pada saat yang sama juga mempertahankan informasi yang sama. Dengan kata lain menghapus data yang kurang tetapi hasilnya juga tetap sama.
Founder at Dialogika",https://miro.medium.com/v2/resize:fit:1200/1*GGoXUKfB4CyF7oHHiyoiZA.jpeg,"Chatbot, IoT, NLP, Artikel Bahasa Indonesia, Machine Learning"
https://medium.com/s/story/jasa-promosi-produk-makanan-telp-wa-0822-4069-7469-promosi-produk-makan-8e83e309aefd,"Jasa Promosi Produk Makanan, Telp / Wa 0822–4069–7469, Promosi Produk Makan",basirul akbar,2018-08-18T02:34:05.471Z,"Jasa promosi produk di balikpapan, Jasa promosi produk di samarinda, Jasa promosi produk di jogja, Jasa promosi produk di yogyakarta, Jasa promosi produk di malang, Jasa promosi produk di bontang, Jasa promosi produk di sangatta, Jasa promosi produk di Melak, Jasa promosi produk di Berau, Jasa promosi produk di Wahau, Jasa promosi produk di Samboja, Jasa promosi produk di Handil, Jasa promosi produk di Penajam, Jasa promosi produk di Grogot, Jasa promosi produk di Banjarmasin
Jasa promosi produk di Tanjung, Jasa promosi produk di Surabaya, Jasa promosi produk di Jakarta, Jasa promosi produk di Serang, Jasa promosi produk di Bali, Jasa promosi produk di Denpasar, Jasa promosi produk di Kediri, Jasa promosi produk di Jombang, Jasa promosi produk di Bandung, Jasa promosi produk di Tasikmalaya, Jasa promosi produk di Jakarta Utara, Jasa promosi produk di Jakarta Selatan, Jasa promosi produk di Jakarta Timur, Jasa promosi produk di Jakarta Pusat
Jasa promosi produk di Banda Aceh, Jasa promosi produk di Langsa, Jasa promosi produk di Lhokseumawe, Jasa promosi produk di Meulaboh, Jasa promosi produk di Sabang, Jasa promosi produk di Subulussalam, Jasa promosi produk di Bali dan Nusa Tenggara, Jasa promosi produk di Bangka Belitung, Jasa promosi produk di Pangkalpinang
Jasa promosi produk di Banten, Jasa promosi produk di Cilegon, Jasa promosi produk di Tangerang Selatan, Jasa promosi produk di Tangerang,
Jasa Promosi Produk Seluruh Indonesia, Jasa Promosi Produk, Jasa Optimasi Facebook Marketing Balikpapan, Jasa Menaikkan Penjualan Dari Facebook Marketing Balikpapan, Jasa Optimasi Internet Marketing Balikpapan.
Jasa Optimasi Internet Marketing Bergaransi Balikpapan Jasa Private Internet Marketing Balikpapan, Jasa Private Facebook Marketing Balikpapan, Jasa Private Fanpage Marketing Balikpapan Kursus Internet Marketing Balikpapan, Kursus Facebook Marketing Balikpapan, Kursus Di Rumah Internet Marketing Balikpapan
Methode penjualan Online
Semakin mudahnya cara penjualan dan pemasaran yang saat ini dapat dilakukan siapa saja, menjadikan banyak orang yang belajar melakukan penjualan di dunia online. Mulai belajar dari media online itu sendiri secara otodidak lewat youtube dan google juga bisa melalui group dan seminar bahkan workshop yang diharuskan membayar hingga mengeluarkan rupiah yang cukup besar.
Informasi Lebih jelasnya, silakan kontak
Bapak Achmad Solihin Prajamas
( CEO PonTren IT & Automotive Madinatul Iman )",https://miro.medium.com/v2/resize:fit:280/1*SvfYOGdbV39vTIa6WXbjFA.jpeg,"Management, Marketing, Manufacturing, Maintenance, Machine Learning"
https://medium.com/s/story/film-fiksi-ilmiah-dan-teknologi-masa-depan-8ed4d82ffa52,Film Fiksi Ilmiah dan Teknologi Masa Depan,Makers Institute,2018-03-01T02:43:28.425Z,"Tahun 2017 yang telah lalu menyaksikan begitu banyak peristiwa penting di bidang inovasi dan teknologi. Mulai dari kebangkitan cryptocurrency juga pesatnya kemajuan-kemajuan di bidangartificial intelligence, machine learning, autonomus robot, AR/ VR, hingga gadget-gadget pintar baru yang muncul di pasaran.
Lalu, bagaimana tren ke depan? Nah, ada satu cara mudah untuk mendapatkan inspirasi untuk menemukan inovasi baru yang mungkin bisa menjadithe next big thingdi masa depan. Yaitu menonton film-film fiksi-ilmiah. Bagaimana mungkin? Tentu saja bisa.
Betapa banyak inovasi baru yang sebelumnya telah ‘diprediksi’ oleh film-film fiksi-ilmiah di masa lalu. Mungkin saat itu penonton agak aneh saat menyaksikan adegan film ‘2001: A Space Odyssey’ ketika seorang astronot berbincang dengan sistemArtificial Intelligence(AI) ‘Hal 9000’ yang mengendalikan sistem pesawat ruang angkasa.
Namun, narasi film besutan sutradara Stanley Kubrick yang dibuat pada pada 1968 itu, nyatanya, kini sudah bukan menjadi hal yang aneh. Gadget asisten pribadi berbasis AI buatan Amazon yang diberi nama Alexa, bahkan dilaporkan menjadi salah satu gadget paling dicari di masa liburan akhir tahun.
Dulu, mungkin penonton tertawa saat Michael Knight berbincang-bincang dengan mobil ‘KITT’ dalam serial Knight Rider (1982), tapi faktanya KIA dan Hyundai berencana menerapkan teknologi semacam itu pada mobil-mobilnya pada 2019 mendatang.
Pada 1987, film Robocop menampilkan gadget Neuralink Human-Computer Vision yang bisa memberikan data-data informasi atas item yang tertangkap oleh kacamata Robocop. Kini, perangkat Augmented reality dan Virtual reality sudah menjadi hal yang jamak di dunia game dan hiburan.
Beberapa teknologi penting lain bahkan nyata-nyata ditemukan berdasarkan inspirasi dari film. Misalnya saja ponsel, temuan Martin Cooper (karyawan Motorola) pada 1970. Ternyata merupakan produk yang terinspirasi dari StartTrek Communicator.
Produsen chipset ponsel Quallcom bahkan sengaja membuat kontes demi mewujudkan gadget fiksi di film StarTrek bernama Tricorder yang berfungsi memindai penyakit. Pada April 2017 lalu, akhirnya kontes ini dimenangkan oleh sekelompok dokter ilmuwan yang tergabung di DxtER.
Selain contoh di atas, masih banyak inovasi-inovasi baru yang awalnya muncul di film fiksi-ilmiah. Jadi, bagi Anda para calon inovator masa depan, mungkin ini bisa menjadi tip menyenangkan untuk mendapatkan ide inovasi besar: perbanyaklah menonton film-film fiksi-ilmiah
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:1200/1*-b-GBFA_W8Y69OOxsSwjjg.jpeg,"AI, Science Fiction, Innovation, Makers Institute, SciFi"
https://medium.com/s/story/anpr-mendeteksi-lokasi-plat-nomor-8f22f08edadd,ANPR: Mendeteksi Lokasi Plat Nomor,MR. Sahputra,2020-03-20T03:01:15.387Z,"Pada bulan Januari 2018, dalam meeting besar awal tahun suatu group usaha di Jakarta, saya mempresentasikan inisiasi projectcomputer visionsebagai salah satu produk awal anak perusahaan mereka. Setelah presentasi selesai, adachallengeyang disampaikan oleh bos divisi Sales group usaha itu mengenai inisiasi tersebut.
Beliau menyampaikan bahwa teknologicomputer visionsudah ada sejak lama. Produk-produk untukface recognition,license plate recognition, danobject detectionsudah menjamur. Pertanyaannya, kenapa kita harus membuat hal serupa? Reinventing the wheels?!
Ada beberapa alasan, salah satunya adalah Indonesia merupakan negara yang besar. Sangat kompleks. Apabila hanya mengandalkan produk-produk dari luar negeri maka hasilnya tidak akan optimal. Produk-produk tersebut dibuat diluar Indonesia dan umumnya dibuat dengan “asumsi”. Contohnya dalam hal mengenali plat nomor kendaraan, data-data yang digunakan adalah data-data dari luar Indonesia. Mulai dari jenis huruf (font), warna plat nomor kendaraan, jarak antar karakter, hingga asumsi bahwa semua plat nomor di Indonesia mengikuti aturan baku yang diberlakukan pemerintah.
Pada kenyataannya asumsi-asumsi diatas banyak tidak tepat. Produk-produk tersebut masuk ke Indonesia sebagai produk jadi, tidak mudah untuk melakukan kustomisasi apabila ada hal-hal yang butuh penyesuaian dengan kondisi lokal di Indonesia. Meminta developer produk tersebut memodifikasi produk-nya agar sesuai dengan keinginan client di Indonesia biasanya bukan hal yang mudah. Akibatnya yang sering terjadi adalah saat PoC / demo akurasi produk-produk computer vision diatas 90% (karena menggunakan data yang diperuntukan demo) namun saat sudah di implementasikan dan berjalan akurasi yang sebenarnya hanya dibawah 50%.
Ada banyak sekali pihak yang membutuhkan teknologi computer vision di Indonesia, mereka membutuhkan dukungan tim developmet yang mumpuni agar dapat menyesuaikanrequirementsetiap pihak yang membutuhkan.
Untuk itulah kita harus memulai langkah untuk mengembangkan produk tersebut secara mandiri.
Ada dua sumber data yang umum digunakan oleh produk computer vision: gambar dan video.
Video sebenarnya adalah kumpulan data gambar yangsequential, sehingga apabila kita bisa mengaplikasikan skenariocomputer visionpada sebuah file gambar maka tidak sulit untuk mengaplikasikan skenario tersebut pada file video.
Pada kasus pengenalan plat nomor kendaraan, secara garis besar ada 2 hal yang harus dilakukan oleh mesin (komputer):
Pada artikel ini kita akan bahas metode yang dipergunakan oleh poin no.1 diatas.
Coba perhatikan contoh gambar dibawah ini
Bagi kita — manusia, mudah saja untuk dengan cepat menunjukan bagian lokasi plat nomor kendaraan pada gambar diatas. Namun bagaimana dengan komputer?
Oh iya, manusia juga tau plat nomor tersebut karena sudah “dilatih” untuk mengenali bagian plat nomor kendaraan. Coba tanya pada anak kecil usia 5 tahun misalnya, dimana lokasi plat nomor pada gambar tersebut, apabila dia belum diajarkan sebelumnya maka dia tidak akan tau bagian mana yang dimaksud.
Proses yang sama harus diberikan kepada mesin / komputer. Struktur otak manusia berbeda dengan struktur “otak” komputer, oleh sebab itu cara mengajarkannya juga berbeda (walaupun pada beberapa metode seperti neural network memiliki kemiripan).
Salah satu tehnik yang kami pergunakan pada adalahCascading Classifiers. Informasi mengenai Cascading classifiers dapat dibaca pada wikipedia.
Cascading Classifiers are trained with several hundred “positive” sample views of a particular object and arbitrary “negative” images of the same size. After the classifier is trained it can be applied to a region of an image and detect the object in question.
Penjelasan mengenai cascading classifieres bagi orang awam, anggap saja cascading classifiers itu merupakan salah satu bagian dari struktur “otak” komputer yang dapat dilatih untuk mengenali bentuk suatu objek. Jadi apabila kita tunjukan gambar plat nomor seperti dibawah ini secara terus menerus,
Maka komputer akan belajar mengenali bentuk plat nomor. Akan terbentuk suatu struktur “otak” yang dapat dipergunakan oleh komputer untuk kemudian mengenali objek plat nomor kendaraan apabila berikutnya dia ditunjukan suatu gambar dengan plat nomor.
Jadi bagaimana caranya melatih komputer untuk mengenali objek plat nomor kendaraan? Kita harus men-supply komputer dengan ragam bentuk plat nomor kendaraan. Pada proses training / pelatihan tersebut, kita harus men-supply 2 jenis sumber data: Positif, dan Negatif. Positif berarti objek plat nomor kendaraan, sedangkan Negatif berarti objek lain diluar plat nomor kendaraan.
Komponen positif dan negatif dibutuhkan agar komputer dapat mengenali lebih tepat mana plat nomor dan mana yang bukan.
Untuk sumber data positif kita juga perlu melakukan proses yang disebut sintesisasi, yaitu merubah posisi objek plat nomor kendaraan dengan cara merotasi 180°, melakukan mirroring, menambahkan efek blur, menambahkan beragam kotoran yang biasa disebut “noisy”, dsb.
Untuk apa melakukan sintesisasi data diatas? Untuk memperkaya referensi objek yang dapat dipelajari oleh komputer, sehingga apabila ada objek plat nomor kendaraan dimana pada gambar tidak terlalu jelas karena blur misalnya, maka komputer tetap dapat mengenali bentuk objek tersebut sebagai objek plat nomor kendaraan.
Pada prakteknya, memproduksi sumber data yang dapat digunakan untuk proses training ini secara programming sangat mudah.
Dari 50 sumber gambar plat nomor kendaraan, misalnya, dengan proses sintesisasi dapat menghasilkan total variasi hingga 1,250. Proses sintesisasi ini sangat bergantung pada kreativitas masing-masing, dan juga disesuaikan dengan kondisi sebenarnya dilapangan.
Pada tahap ini, kita sudah bisa melihat betapa pentingnya mengembangkan produk computer vision sesuai kondisi di Indonesia, bukan? Karena kita bisa memikirkan kemungkinan-kemungkinan apa saja yang mungkin terjadi pada plat nomor kendaraan di Indonesia sehingga bisa kita tambahkan dalam proses training mesin (Machine Learning).
Apa itu algoritma Viola-Jones? Berikut ini informasi dariWikipedia,
The Viola–Jones object detection framework is the first object detection framework to provide competitive object detection rates in real-time proposed in 2001 by Paul Viola and Michael Jones.
Algortima Viola-Jones awalnya dipergunakan sebagai pendeteksi objek wajah pada fiturface recognition, namun algoritma tersebut dapat dipergunakan pada objek selain wajah.
Untuk training plat nomor kendaraan, kita melatih cascade classifier menggunakan algoritma Viola-Jones dengan memasukan gambar-gambar “positif” dan “negatif” diatas sebagai data sample. Pada akhir proses, akan dihasilkan sebuah output yang disebut “cascade_plates.xml”. Sebelumnya saya sempat menyebutkan “bagian struktur otak komputer” yang memahami bentuk objek plat nomor kendaraan, nah, “cascade_plates.xml” inilah yang saya maksud sebagai struktur otak komputer tersebut.
Selanjutnya, kita akan test dengan memberikan gambar plat nomor kendaraan sebagai uji coba dimana plat nomornya tidak termasuk dalam plat nomor yang dilatihkan sebelumnya (karena tujuan kita adalah mesin dapat mengenali “bentuk” plat nomor kendaraan),
Terlihat bahwa komputer dapat mengenali lokasi plat nomor kendaraan. Namun ternyata ada tiga bagian yang terdeteksi sebagai plat nomor kendaraan, dimana dua diantaranya salah. Istilah untuk kondisi ini biasa disebut sebagai “false positives”, yaitu terindikasi sebagai objek plat nomor kendaraan (positives) padahal salah (false).
Mengapa terjadi false positives? Ada beberapa alasan, namun yang perlu diperhatikan adalah kita harus menyaring (filter) false-positives tersebut sehingga komputer benar-benar menunjukan dengan tepat bagian plat nomor kendaraan. Kesalahan pada machine learning adalah hal yang lumrah, penyelesaian umumnya ada dua: pertama, memperbaiki sumber data yang dipergunakan, ataupun memperbaiki proses pelatihan komputer-nya, dan yang kedua, mereduksi kesalahan tersebut melalui metode-metode yang lebih advanced dan “manual”.
Untuk menyaring false-positives pada area plat nomor kendaraan, maka secara garis besar dilakukan proses berikut ini:
Setelah dilakukan algoritma untuk menyaring false region maka hasilnya sekarang menjadi jauh lebih akurat
Setelah lokasi objek plat nomor didapatkan maka proses dilanjutkan dengan pengenalan angka dan huruf, inilah proses inti dariAutomatic Number Plate Recognition.
Namun kita bisa lihat bahwa sebelum proses pembacaan angka dan huruf dimulai maka dibutuhkan beberapa proses lain (pre-processing) agar proses pengenalan menjadi lebih akurat.
Bahkan sebelum proses pengenal angka dan huruf dimulai butuh beberapa pre-processing lainnya seperti membetulkan posisi plat nomor kendaraan agar tidak miring dengan menggunakan metode histogram (horisontal dan vertikal).
Melalui pre-processing tersebut maka kondisi plat nomor yang posisinya ketika ditangkap kamera tidak standar bisa dibuat menjadi lebih standar sehingga mudah untuk dibaca oleh proses pengenalan angka dan huruf nantinya.
Berikut ini salah satu showcase dimana prototype produk tersebut berusaha mendeteksi lokasi plat nomor kendaraan berdasarkan video stream yang diambil dari kamera yang di install di depan kantor.
Bisa terlihat bahwa plat nomor khususnya motor masih belum semua bisa dideteksi dengan baik. Hal ini disebabkan oleh cukup banyak alasan salah satunya karena plat nomor motor ukurannya kecil secara pixel (dan banyak yang bengkok) sehingga lebih sulit untuk dideteksi.
Ini merupakan salah satu tantangan bagi tim developer untuk memoles algoritma produkcomputer visiontersebut sehingga menjadi lebih akurat lagi 😊
Its fun, really really fun 🙂. Banyak sekali tehnik yang dapat kita aplikasikan pada implementasi ANPR. Kondisi di Indonesia yang sangat kompleks pun menambah tantangan bagi kita terutama yang hobi sekali memecahkan masalah-masalah kompleks melalui software engineering.
Kita belum masuk pada bagian pengenalan angka dan huruf namun sudah cukup banyak langkah-langkah yang harus dilakukan, dan pada perjalanan-nya nanti terutama ketika sudah di-deploy pada client tentu akan lebih banyak lagi feedback yang akan kita terima baik berupa bugs ataupun change request, tentu saja…lagi-lagi kembali pada kebutuhan yang disesuaikan dengan kondisi lokal di negara Indonesia tercinta.
Software akan selalu butuh perubahan, perlu di tunning agar akurasinya bisa semakin meningkat untuk ragam use-case di Indonesia.
By end of the day, we would say that, our engineers are here for them. Sitting in Jakarta, or other area remotely, working together to support various needs of our customer.
Die Journale - Vanaf jouself, begin klein dingetjies, en van nou af.",https://miro.medium.com/v2/resize:fit:1200/1*8xWJys1ChjuX7_Co7Z2olg.png,"Research, Matadewa, Machine Learning, Computer Vision"
https://medium.com/s/story/pengenalan-deep-learning-8fbb7d8028ac,Pengenalan Deep Learning Part 1 : Neural Network,Samuel Sena,2018-03-19T15:15:03.132Z,"Deep Learning merupakan topik yang sedangnge-trenddikalangan akademisi ataupun professional. Apa sih Deep Learning itu? Deep Learning adalah salah satu cabang Machine Learning(ML) yang menggunakan Deep Neural Network untuk menyelesaikan permasalahan pada domain ML.
Mungkin nanti akan saya bagi dalam beberapa part. Untuk Part I, kita akan sama-sama belajar tentang apa itu Neural Network yang merupakan bagian yang paling penting dari Deep Learning.
Neural network adalah model yang terinspirasi oleh bagaimana neuron dalam otak manusia bekerja. Tiap neuron pada otak manusia saling berhubungan dan informasi mengalir dari setiap neuron tersebut. Gambar di bawah adalah ilustrasi neuron dengan model matematisnya.
Tiap neuron menerima input dan melakukan operasi dot dengan sebuah weight, menjumlahkannya (weighted sum) dan menambahkan bias. Hasil dari operasi ini akan dijadikan parameter dari activation function yang akan dijadikan output dari neuron tersebut.
Nah, mungkin banyak yang bingung apa dan untuk apa activation function? Sesuai dengan namanya, activation function befungsi untuk menentukan apakah neuron tersebut harus “aktif” atau tidak berdasarkan dari weighted sum dari input. Secara umum terdapat 2 jenis activation function, Linear dan Non-Linear Activation function.
Bisa dikatakan secara “default” activation function dari sebuah neuron adalah Linear. Jika sebuah neuron menggunakan linear function, maka keluaran dari neuron tersebut adalah weighted sum dari input + bias.
Sigmoid function mempunyai rentang antara 0 hingga 1 sedangkan rentang dari Tanh adalah -1 hingga 1. Kedua fungsi ini biasanya digunakan untuk klasifikasi 2 class atau kelompok data. Namun terdapat kelemahan dari kedua fungsi ini, nanti akan coba saya jelaskan di part berikutnya.
Pada dasarnya ReLU melakukan “treshold” dari 0 hingga infinity. ReLU juga dapat menutupi kelemahan yang dimiliki oleh Sigmoid dan Tanh yang nanti akan saya coba jelaskan di part berikutnya.. :D
Sebenarnya masih banyak activation function yang lain, namun beberapa fungsi yang saya sebutkan diatas merupakan fungsi yang sering digunakan. Sebenarnya masih ada satu lagi Softmax Function, tapi nanti akan saya jelaskan pada part Multiclass Classification. Untuk referensi lengkap tentang activation function bisa dibaca di pagewikipedia.
Arsitektur diatas biasa disebut sebagai Multi Layer Perceptron (MLP) atau Fully-Connected Layer. Arsitektur pertama mempunyai 3 buah neuron pada Input Layer dan 2 buah node Output Layer. Diantara Input dan Output, terdapat 1 Hidden Layer dengan 4 buah neuron. Sedangkan spesifikasi Weight dan Activation function adalah sebagai berikut:
Setiap neuron pada MLP saling berhubungan yang ditandai dengan tanda panah pada gambar diatas. Tiap koneksi memiliki weight yang nantinya nilai dari tiap weight akan berbeda-beda.
Hidden layer dan output layer memiliki tambahan “input” yang biasa disebut dengan bias (Tidak disebutkan pada gambar diatas).
Sehingga pada arsitektur pertama terdapat 3x4 weight + 4 bias dan 4x2 weight + 2 bias. Total adalah 26 parameter yang pada proses training akan mengalami perubahan untuk mendapatkan hasil yang terbaik. Sedangkan pada arsitektur kedua terdapat 41 parameter.
Neuron pada input layer tidak memiliki activation function, sedangkan neuron pada hidden layer dan output layer memiliki activation function yang kadang berbeda tergantung daripada data atau problem yang kita miliki.
Pada Supervised Learning menggunakan Neural Network, pada umumnya Learning terdiri dari 2 tahap, yaitu training dan evaluation. Namun kadang terdapat tahap tambahan yaitu testing, namun sifatnya tidak wajib.
Pada tahap training setiap weight dan bias pada tiap neuron akan diupdate terus menerus hingga output yang dihasilkan sesuai dengan harapan. Pada tiap iterasi akan dilakukan proses evaluation yang biasanya digunakan untuk menentukan kapan harus menghentikan proses training (stopping point)
Pada part selanjutnya, akan saya bahas bagaimana proses training pada neural network lebih mendalam. Namun pada part ini akan dijelaskan garis besarnya saja. Proses training terdiri dari 2 tahap :
Forward pass atau biasa juga disebut forward propagation adalah proses dimana kita membawa data pada input melewati tiap neuron pada hidden layer sampai kepada output layer yang nanti akan dihitung errornya
Persamaan diatas adalah contoh forward pass pada arsitektur pertama (lihat gambar arsitektur diatas) yang menggunakan ReLU sebagai activation function. Dimanaiadalah node pada input layer (3 node input),jadalah node pada hidden layer sedangkanhadalah output dari node pada hidden layer.
Error yang kita dapat pada forward pass akan digunakan untuk mengupdate setiap weight dan bias dengan learning rate tertentu.
Kedua proses diatas akan dilakukan berulang-ulang sampai didapatkan nilai weight dan bias yang dapat memberikan nilai error sekecil mungkin pada output layer (pada saat forward pass)
Pada bagian ini kita mau mencoba implementasi forward pass menggunakan Python dan Numpy dulu saja tanpa framework biar lebih jelas. Nanti pada part-part selanjutnya akan kita coba dengan Tensorflow dan Keras.
Untuk contoh kasusnya adalah kita akan melakukan regresi untuk data yang sebenarnya adalah sebuah fungsi linear sebagai berikut:
Sedangkan arsitektur neural networknya terdiri dari :
Neural network diatas sudah saya train dan nanti kita akan melakukan forward pass terhadap weight dan bias yang sudah didapat pada saat training.
MethodforwardPassdibawah ini sangat simple sekali, operasi dot akan dilakukan pada setiap elemen pada input dan tiap weight yang terhubung dengan input dan ditambahkan dengan bias. Hasil dari operasi ini akan dimasukkan ke dalam activation function.
Untuk weight dan bias yang akan kita coba, nilai keduanya sudah didapatkan pada proses training yang telah saya lakukan sebelumnya. Bagaimana cara mendapatkan kedua nilai tersebut akan dijelaskan pada part-part berikutnya.
Kalau dilihat dari weight dan bias diatas, nilai keduanya identik dengan fungsi linear kita tadi :
f(x) = 3x + 2 ≈ f(x) = 2.99999928x + 1.99999976
Pada percobaan kali ini kita akan melakukan perdiksi nilai dari7, 8, 9dan10. Output yang dihasilkan seharusnya adalah23, 26, 29, 32dan hasil prediksi adalah22.99999472, 25.999994, 28.99999328dan31.99999256. Jika dilihat dari hasil prediksi, masih terdapat error tapi dengan nilai yang sangat kecil.
Pada part selanjutnya kita akan sama-sama mencoba forward pass menggunakan activation function yang lain dan mencoba menambahkan hidden layer. Semoga post ini bermanfaat untuk kita semua yang penasaran dengan deep learning :D
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*k-ZPsWqnDq8SPky804n3cg.jpeg,"Deep Learning, Machine Learning, Neural Networks, Artificial Intelligence"
https://medium.com/s/story/pengenalan-terhadap-machine-learning-9011fe71d1e4,Pengenalan terhadap Machine Learning,Makers Institute,2018-03-01T02:41:27.221Z,"Machine Learning adalah metode yang digunakan untuk membuat program yang bisa belajar dari data. Berbeda dengan program komputer biasa yang statis, program machine learning adalah program yang dirancang untuk mampu belajar sendiri.
Cara belajar program machine learning mengikuti cara belajar manusia, yakni belajar dari contoh-contoh. Machine learning akan mempelajari pola dari contoh-contoh yang dianalisa, untuk menentukan jawaban dari pertanyaan-pertanyaan berikutnya.
Memang tidak semua masalah bisa dipecahkan dengan program machine learning. Namun, seringkali algoritma yang sifatnya kompleks, ternyata bisa dipecahkan dengan sangat simpel oleh machine learning. Beberapa contoh program machine learning yang telah digunakan dalam kehidupan sehari-hari:
Kita bisa mengklasifikasikan machine learning berdasarkan tipe-tipe:
Data yang ada diberikan real value, numerical atau floating point, agar dapat mencoba mendeteksi harga saham di kemudian hari. Contoh: time series data dari harga saham berdasarkan waktu.
-Classification(Discrete/ Category)
Data yang ada diberikan label atau kategori, agar dapat diambil keputusan berdasarkan label/ kategori tersebut.
Data tidak diberikan label, tapi secara otomatis dibagi berdasarkan kemiripan dan struktur lain dari data tersebut. Misalnya, ketika kita mengorganisasikan foto. Kita harus melakukan tagging secara manual
Data digunakan untuk melakukan pemberian label If dan Else yang digunakan untuk mengambil keputusan berdasarkan tree pengambil keputusan
Machine learning adalah seperti membuat program yang bisa menebak kotak hitam yang memiliki rumus fungsi yang belum diketahui. Kotak hitam itu diberikan sebuah input dan akan menghasilkan sebuah output tertentu. Dari data-data input dan output yang diperoleh, maka program akan menebak rumus fungsi yang paling mendekati keakuratan.
Adapun alur kerjaMachine Learningmencakup:
Akurasi awal dari program machine learning biasanya sangat buruk. Sebab pada awalnya program ini ‘tidak tahu apa-apa’. Namun, seiring berjalannya waktu, semakin sering kita melatih program, semakin banyak contoh-contoh yang dipelajari oleh program, maka program ini akan semakin ‘cerdas’ dan akurat.
Misalnya saja saat kita bermain gameRole Playing Game(RPG) yang menggunakanArtificial Intelligence. Pertama kali kita bermain dengan RPG tersebut, maka dengan mudah kita akan bisa memenangkan permainan. Namun, setelah beberapa kali permainan,engine/ algoritma game itu akan belajar dari pola-pola sebelumnya, sehingga akan semakin sulit dikalahkan.
*) Disarikan dari materi presentasi Welly Tambunan ( Solution Architect Bank Danamon) pada acara Machine Learning Street Fighter, di Makers Institute, 11 Januari 2018.
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:778/1*douY3vs-5WqjnL2tj3VlCQ.jpeg,"Makers Institute, Artificial Intelligence, Machine Learning"
https://medium.com/s/story/long-path-to-data-journey-for-it-del-student-90560e6d077a,Long path to Data Journey for IT-DEL Student,Welly Tambunan,2018-07-02T06:17:44.953Z,"Kemaren merupakan hari minggu yang cukup menarik bagi saya dan Ferdinand Panjaitan ( Praktisi ) dan juga Mario Simare-mare ( Ketua Prodi SI dari IT DEL) . Diskusi yang sangat alot mengenai apa yang harus dipersiapkan oleh mahasiswa DEL dan juga IT-DEL sebagai universitas.
Ada 2 hal yang harus dicapai untuk setiap lulusan DEL. Kemampuan untuk melanjutkan ke jenjang pendidikan yang lebih tinggi. Menghasilkan research paper, patent dan kembali mengajar di IT-DEL. Dan juga kemampuan bersaing untuk mendapatkan pekerjaan di industry. Kedua tanggung jawab besar ini dapat difasilitasi dengan mengarahkan lulusan mengambil pekerjaan di dunia Data Science ataupun Data Engineering.
Dengan mengambil data scientist dan data engineer sebagai pekerjaan maka lulusan IT-DEL yang sudah bekerja di industry akan dipancing untuk melanjutkan kuliah. Karena hal tersebut merupakan tuntutan secara tidak langsung di dunia Industry untuk jurusan ini.
Pembicaraan berawal dari pembahasan mengenai pembaharuan kurikulum DEL. Menurut saya tidak ada yang salah dengan kurikulum sekarang. Tetapi kita harus memberikan training khusus dan bobot ataupun prioritas mengenai mata pelajaran yang akan diajarkan. Hal ini akan membuat lulusan IT-DEL menjadi lulusan yang sangat siap terjun ke dunia kerja.
Mengapa saya bilang kurikulum tidak perlu di revisi, karena pada akhirnya lulusan akan di uji berdasarkan pengetahuan fundamental yang di dapat dari perkuliahan semester awal.
Saya mengetahui hal ini dari industry karena memiliki banyak rekan dan teman yang cukup sering membahas hal ini.
Kurikulum tidak perlu di ubah tetapi cukup memberikan training tambahan sebelum berangkat ke dunia kerja. Atau mata kuliah yang sangat penting dan fundamental.
Tahap Seleksi masuk Kerja
Dimanapun sekarang untuk masuk kerja memiliki test Algoritma dan Struktur Data sebagai penyaringan awal. Mengapa ? Karena cara ini merupakan cara yang paling mudah dan dapat di automasi dengan menggunakan website sepertiCodilitydanHackerrank.
Algoritma dan Struktur data sudah tentu didapat pada pelajaran semester awal. Ini sebenarnya bukan suatu masalah yang besar. Tetapi kebanyakan siswa sudah melupakan pelajaran ini karena sudah terlalu lama. Jadi ada baiknya di perkuliahan diadakan training atau kompetisi yang diadakan secara berkala untuk tetap mengasah hal ini.
Algoritma dan Data Struktur yang seperti apa ?
Untuk mencari titik aman sebaiknya kuasai Data Struktur sampai dengan Graph Algorithm dan juga problem solving sampai dengan Dynamic Programming. Anda mungkin kaget karena saya seolah olah menyarankan persiapan kompetitif programming. Tetapi yakinlah bahwa soal-soal yang diujikan adalah soal-soal klasik dengan sedikit modifikasi dan dapat diselesaikan dengan mudah bagi yang sering latihan. Tidak perlu expert tetapi fasih saja.
Saya sangat menyarankan untuk melakukan training diTOKI Training Gate. Kuasai kedua materi tersebut.
Tidak usah sampai sempurna mengerjakan semua soal di Kompetitif Dasar. Tetapi wajib mengerjakan soal-soal klasik. Pemrograman dasar adalah materi dasar yang wajib selesai. :). Setelah itu anda boleh cobahackerrankuntuk latihan rutin.
Saya mendapatkan informasi bahwa di Samsung Research Indonesia ( SRIN ) diadakan test Algorithm dan Data Structure yang di grading berdasarkan robot online judge. Sehingga kemampuan anda untuk problem solving sangat dibutuhkan sekali.
Kemampuan Dasar Matematika dan Statistika
Hal ini menjadi sangat penting sekali dan juga diajarkan di semester awal. Anda sudah melihat bahwa semester awal tersebut merupakan fondasi. Nah, di dunia kerja fondasi ini juga yang akan diujikan. Seperti pelajaran Matematika Diskret, Kalkulus, Probability dan Statistika, Linear Algebra dan Analisa Numerik atau Algoritma Numerik.
Hal ini sangat dibutuhkan untuk memahami berbagai macam textbook yang sering digunakan sebagai materi ajar di perkuliahan machine learning. Siswa diharapkan mampu membaca notasi matematika yang ada di text book dan dapat menuliskan nya ke dalam kode komputer.
Khan Academydapat digunakan sebagai tempat untuk memperdalam pemahaman semasa perkuliahan untuk materi tersebut. Ingat matematika bukan hapalan atau hanya menonton orang menyelesaikan masalah, tetapi andalah yang harus banyak berlatih untuk menyelesaikan masalah secara mandiri.
Data Science dan Machine Learning
Di internet banyak sekali bertebaran kursus online mengenai hal ini. Banyak rekomendasi yang anda dapatkan dari mengunjungi blog atau tanggapan orang lain. Saya berharap anda tidak terjebak dengan hal ini. Percayalah bahwa membaca buku teks book merupakan cara belajar yang terbaik sampai sekarang ini. Karena jika anda ingin melanjutkan pendidikan maka anda wajib bisa membaca paper yang ditulis dengan notasi matematika. Dan satu lagi yang paling penting adalah ingat jangan sampai mengabaikan latihan atau exercise yang ada di buku tersebut.
Buku yang sampai saat ini dianggap oleh SRIN sebagai buku yang bagus adalah Learning from Data dari Yaser Abu-Mustofa. Baca buku ini dari awal sampai akhir dan kerjakan latihan-latihan nya. Anda akan kagum melihat penyajian di buku tersebut. Pahami chapter Neural Network nya karena pemahaman ini dibutuhkan jika anda ingin memperdalam Deep Learning.
Akhir nya jika anda masih antusias anda dapat melanjutkan ke arah yang lebih spesifik seperti Deep Learning. Anda dapat membaca buku online dari Michael Nielsen. BerjudulNeural Networks and Deep Learning. Jangan hanya dibaca sambil lalu, ini buku teksbook, jadi usahakan ambil pensil dan kertas dan mulailah aktif menuliskan persamaan, ikuti cara penurunan rumus dan seterusnya. Anda harus membiasakan diri dengan pensil dan kertas untuk menyelesaikan suatu permasalahan. Jangan terlalu bergantung kepada tools seperti komputer ataupun google dan stackoverflow.
Sampai situ saja. Anda tidak perlu bersusah susah mencari banyak bahan atau materi ajar. Semua sudah disediakan di kampus. Pergunakan fasilitas anda dengan sebaik-baiknya. Internet digunakan dengan benar dan untuk kepentingan belajar. Buatlah grup diskusi untuk membaca buku dengan baik dan benar. Karena setiap orang memiliki kemampuan yang berbeda-beda.
Ada orang yang suka matematika dan ada yang suka programming, ada pula yang suka statistika. Anda bentuk kelompok yang merupakan kombinasi ketiga hal tersebut. Mulailah dari sekarang dan dapatkan pekerjaan terbaik setelah selesai kuliah.
Oh ya satu lagi, anda bisa mengikuti kompetisi atau berlatih masalah-masalah yang dapat dipecahkan dengan machine learning diKaggle. Selamat main keroyokan bersama dengan tim anda. :)
Persiapan setelah tamat
Jangan lupa untuk mempelajari dan membahas soal-soal interview dari dunia kerjaan. Lagi-lagi kembali ke algorithm dan data structure juga data science interviewnya. Persiapkan diri anda untuk hal ini dengan dua buku berikut.
Video berikut juga sangat menarik untuk disimak.
https://www.safaribooksonline.com/library/view/cracking-the-data/9781491924259/",https://miro.medium.com/v2/resize:fit:1200/0*9Jxz-ra_u1eBKKhV.png,"Samsung, Mahasiswa, Machine Learning, It Del"
https://medium.com/s/story/perbedaan-supervised-dan-unsupervised-learning-91089634721f,Perbedaan Supervised dan Unsupervised Learning,Antoni F Setiawan,2018-03-24T12:31:26.321Z,"Pada umumnya, problem dalam statistical learning digolongkan pada salah satu dari dua kategori: supervised atau unsupervised learning.
Karakter dari supervised learning, untuk setiap observasi akan diasosiasikan dengan respon, atau berdasarkan prediktor xi=x1,x2,…,xn kita dapat menyimpulkan respon yi. Sedangkan unsupervised learning, kita mencari karakter dari data yang kita miliki dengan mencari hubungan antar variable atau observasi yang mana kita sendiri juga tidak mengetahui karakternya dari awal.
Contoh. Terdapat sejumlah data pelanggan dengan data transaksi yang dimiliki di masa lalu seperti tanggal pembelian, barang yang dibeli, jumlah barang, data pengiriman, kota tempat tinggal, kodepos, total belanja, dan lain-lain.
Kita ingin memprediksi apakah seorang pelanggan akan belanja lagi atau tidak di masa depan berdasarkan data transaksi maka kasus ini dapat kita golongkan sebagai supervised leaning karena kita mengetahui bahwa respon yang kita inginkan yaitu: belanja lagi atau tidak. Problem ini umum dikenal sebagai classification problem.
Notasi untuk kasus tersebut adalah sebagai berikut: pelanggan -> xi, mempunyai observasi: tanggal beli -> x1; barang yang dibeli -> x2; jumlah barang -> x3; …;dan lain-lain -> xn, yang mana berdasarkan observasi ini kita ingin mempredikasi belanja lagi atau tidak belanja lagi -> yi.
Sementara itu, kita juga ingin tahu karakter dari data pelanggan kita, yang mana saat ini kita tidak tahu secara pasti seperti apa karakter sebenarnya dari data-data tersebut. Mungkin jika jumlah observasi, data, tidak bayak atau jumlah variable yang dimiliki tidak dalam dimensi yang besar namun jika dimensi dan volume datanya besar akurasi dari tebakan kita akan bias. Karakter apa mungkin kita bisa ketahui dari data tersebut? mungkin karakter seperti kelompok jumlah nominal belanja, kebiasaan belanja, atau kelompok lokasi asal pelanggan kita yang belanja barang-barang tertentu pada waktu tertentu dan sebagainya. Problem ini biasa umum dikenal dengan cluster analysis atau clustering.",,"Data Science, Machine Learning, Statistical Learning"
https://medium.com/s/story/huawei-p11-akan-memiliki-3-buah-lensa-kamera-belakang-92cdf9e21828,Huawei P11 akan memiliki 3 buah lensa kamera belakang?,HSE HIMA IF Telkom University,2017-12-08T17:51:35.787Z,"Meskipun tren dual kamera masih hangat di dunia smartphone, Huawei mencoba mendobrak kembali tren kamera pada smartphone. Smartphone flagship tahun depan, P11 direncanakan akan memiliki tiga buah kamera belakang untuk meningkatkan kualitas gambar yang dihasilkan.
Ketiga lensa kamera belakang tersebut akan memiliki spesifikasi sensor 40MP. Dengan spesifikasi tersebut, P11 nantinya akan mampu melakukan 5xhybrid zooming. Optic Leica pada smartphone tersebut juga akan memungkinkan kamera menangkap cahaya 100 persen lebih banyak. Selain itu, terdapat juga fitur Pro Night Mode yang memungkinkan pengambilan gambar malam hari lebih mudah meskipun tanpa tripod.
Supaya kamera dapat mengenali objek lebih cepat, Huawei juga akan melengkapi smartphone tersebut dengan teknologi AI (Artificial Intelligence). Dengan teknologi AI, kamera juga akan lebih cepat dalam melakukan auto framing.
Bicara mengenai kamera depan, Huawei juga tidak main-main dalam spesifikasinya. P11 akan dibekali lensa kamera dengan resolusi 24MP dan fitur “low light enhancement”.
Dirangkum dari : Liputan6.com
Hubungan Sosial Ekstra Kampus — Himpunan Mahasiswa S1 Teknik Informatika Universitas Telkom",https://miro.medium.com/v2/resize:fit:500/1*rq7-_snXk_Va1Di70-0HTA.png,"Artificial Intelligence, Triple Rear Camera, P11, Huawei, Smartphone"
https://medium.com/s/story/menguji-kecerdasan-buatan-menggunakan-turing-test-9553ff521f0f,Menguji Kecerdasan Buatan menggunakan Turing Test,AC,2018-01-07T08:01:07.742Z,"turing test adalah metode untuk menguji kepintaran mesin yang menyerupai manusia. Alan Turing mendefinisikan perilaku kepintaran sebagai kemampuan untuk mencapai level kepintaran manusia selama percakapaan berlangsung. Performa ini harus cukup untuk menipu seorang interrogrator untuk berpikir apakah jawaban tersebut dari manusia.
untuk melihat jika sebuah mesin dapat melakukan hal diatas, dia mengusulkan sebuah metode: dia mengatakan bahwa manusia harus menginterogasi mesin lewat tampilan teks. batasan lain adalah manusia tidak bisa mengetahui siapa lawan bicaranya, yang mana itu bisa mesin atau manusia. untuk membuat serangkaian proses ini, manusia akan berinteraksi dengan 2 entitas lewat tampilan teks. dua entitas ini disebut reponden, salah satu dari mereka adalah manusia dan sisanya mesin.
mesin akan lulus test jika interrogrator tidak bisa membedakan apakah jawaban yang datang dari mesin atau manusia.
seperti yang anda bisa banyangkan, rangkaian ini sangat sulit untuk mesin. dalam rangkaian itu bisa banyak hal yang terjadi. biasanya mesin harus fasih dengan syarat syarat sebagai berikut:
1.Natural Language Processing,mesin harus bisa berkomunikasi dengan interrogrator. mesin harus bisa mengurai kalimat, mengekstrak konteksnya, dan memberikan jawaban yang sesuai.
2.Knowledge Representation,mesin harus bisa menyimpan informasi yang disediakan sebelum tes. mesin juga harus tetap pada informasi yang disediakan selama percakapan berlangsung sehingga mesin dapat merespon dengan tepat jika pertanyaannya datang lagi.
3.Reasoning,menjadi penting bagi mesin mengerti bagaimana menginterpretasi informasi yang sudah disimpan. manusia cenderung melakukan ini secara otomatis untuk menggambarkan kesimpulan secara real time.
4.Machine Learning,hal ini sangat diperlukan sehingga mesin bisa beradaptasi dengan kondisi baru secara real time. mesin harus bisa menganalisas dan mendeteksi pola-pola sehingga mesin dapat menyimpulkan.
mungkin bertanya tanya mengapa manusia berkomunikasi lewat text interface. menurut turing, simulasi fisik dari manusia tidak diperlukan untuk kecerdasan. itulah mengapa turing test menghindari kontak fisik secara langsung antara manusia dan mesin. ada hal lain namanya Total Turing Test yang mana mesin menghadapi task yang lebih sulit dengan pengelihatan dan pergerakan. untuk lulus dari tes ini, mesin harus melihat objek menggunakan computer vision dan bergerak menggunakan robotics. dikutip dari bukunya Prateek Joshi, Artificial Intelligence with Python
Sudah ada mesin yang berhasil melewati turing test ini pada tahun 2014, selengkapnya bisa lihat dihttp://www.bbc.com/news/technology-27762088",https://miro.medium.com/v2/resize:fit:639/1*d3OiMO7V1jbmkiTZHzKZrg.png,"Artificial Intelligence, Machine Learning, AI, Python"
https://medium.com/s/story/correlation-computing-solving-classification-problem-with-simplest-way-95b43ee7175c,Correlation Computing : Solving classification problem with the simplest way,Rangga Rizky A,2017-11-29T11:42:20.090Z,"masalah klasifikasi merupakan masalah yang banyak muncul di dunia nyata. mulai dari deteksi suatu penyakit, memperkirakan cuaca bahkan mengelompokan dokumen berdasarkan topik maupun mendeteksi kematangan sebuah buah. dalam dunia machine learning dikenal beberapa algorima klasifikasi seperti nearest-neighbor,decision tree maupun support vector machine.
dengan correlation computing kita juga dapat menyelesaikan masalah klasifikasi dengan menghitung kemiripan antar data. kali ini saya akan melakukan ilustrasi sederhana dengan contoh mengklasifikasi cuaca
untuk menghitung kemiripan data dengan nilai kategorikal, cara yang paling mudah adalah dot product .
Kita akan mencoba menghitung dengan data sebagai berikut :suhu sedang, kelembapan tinggi, penguapan tinggi, angin tidak kencang dan tekanan udara sedang.
setelah itu dicari nilai yang paling besar. pada kasus ini adalah hujan adalah yang paling besar. maka data tersebut terklasifikasi sebagai hujan. cari ini sangat mudah dan komputasi yang ringan hanya saja kekurangan dari cara ini adalah kita harus tahu fitur-fitur yang jelas membedakan antar data. hal itu yang terkadang susah dilakukan pada kasus dunia nyata. karena itu pendekatan ini tidak bisa memecahkan masalah-masalah pattern recognition dan masalah yang tidak bisa dimodelkan secara hirarki.
What cannot be proven is wrong. Cannot be proven correctly",,Machine Learning
https://medium.com/s/story/2018-eranya-artificial-intelligence-97f084217a06,2018: Eranya Artificial Intelligence,Amanda Bahraini,2018-05-02T10:57:27.020Z,"Kabar baik bagi penggemar film Her (2013), karena tahun 2018 merupakan tahun dimana teknologi AI (artificial intelligence) semakin aktif digunakan dan diadaptasi untuk berbagai sektor kehidupan. Memang, mungkin belum akan secanggih Samantha (disuarakan oleh Scarlett Johannson), sioperating systemserba bisa yang tidak hanya mampu melakukan tugas seorangpersonal assistantyang andal dan menghasilkan kalkulasi akurat, tapi juga bisa menggubah simfoni dan berempati layaknya manusia.
Meskipun belum setingkat Samantha di film Her (2013), perkembangan AI di tahun ini semakin menunjukkan kemajuan yang menjanjikan, dan tidak, kita tidak hanya akan membicarakanchatbots(AI yang diprogram untuk melakukan simulasi percakapan).
Berikut adalah beberapa peluang penggunaan AI dalam berbagai sektor kehidupan yang sangat mungkin kita nikmati di tahun 2018 ini:
Konfirmasi identitas menggunakan teknologibiometrics verification(identifikasi berdasar karakteristik unik dari seorang individu baik secara fisik maupun perilaku) akan terus diintegrasikan dalam kehidupan sehari-hari. Pengenalan berdasar sidik jari, suara, muka, bahkan perilaku seperti logat bicara akan semakin detail dan terus dimanfaatkan untuk mencegah adanya pemalsuan identitas.
Baru-baru ini,Mastercard sudah mengumumkan adanya pilihanface recognitionuntuk transaks pembayaran menggunakan kartu kredityang bisa digunakan awal tahun depan. Bank-bank lain seperti HSBC dan First Direct juga telah mengungkapkan rencana untuk memanfaatkan teknologi yang masa, beberapa bahkan membicarakan penggunaaniris recognitionke depannya.
Setelah teknologifacial recognitionuntuklog inaplikasi keuangan dan perintah berbasis suara untuk fungsi sederhana seperti cek saldo, dunia keuangan kembali diramaikan dengan kabar tentang adanya teknologi berbasis AI yang dapat berperan sebagai penasehat keuangan pribadi.
Clinc, sebuah perusahaan yang berfokus pada riset conversational AI, telah mengembangkan sistem intelijensia buatan untuk institusi keuangan yang mampu membaca pola belanja dan pemasukan pengguna secara personal, melaporkan analisa pola tersebut, juga memberikan jawaban atas pertanyaan pengguna.
Jadi kemungkinan besar, pertanyaan-pertanyaan semacam “Berdasarkan pemasukan saya selama setahun kemarin, berapa banyak uang yang bisa saya sisihkan untuk persiapan dana pensiun?” atau “Pengeluaran apa yang harus saya kurangi untuk bulan ini?” tidak harus menunggu untuk diajukan padafinancial planneratau banker pribadi Anda. Tahun 2018 bisa jadi merupakan awal dimana aplikasi keuangan dalamsmartphoneAnda bisa menjawab semuanya. –How Artificial Intelligence Is Going To Affect The Financial Industry In 2018.
Bulan April 2018 ditandai dengan munculnyateknologi komputer yang dapat menghasilkan video palsuyang menggunakan figur tampilan mantan presiden AS, Barrack Obama. Ancaman akan penggunaan penyalahgunaan teknologi tersebut pun ramai dibicarakan, namun Dr Supasorn Suwajanakorn selaku pencipta dari algoritma AI yang mampu menghasilkan konten tersebut mengatakan bahwa meskipun memang AI bisa digunakan untuk usaha pencemaran nama baik, konten-konten video ciptaan AI yang menggunakan individu manusia sebagai model untuk saat ini masih sangat mudah untuk dideteksi kepalsuannya.
Konten video ciptaan AI mungkin masih membutuhkan pengembangan lebih lanjut, tapi saat ini ada banyak penerapan teknologi AI yang telah berhasil menciptakan konten berkualitas untuk penggunanya. Salah satu contohnya adalahapp musik Spotify yang telah mengintegrasikan teknologibig datadanmachine learninguntuk merekomendasikan musik-musik sesuai dengan pola perilaku dan selera pengguna.
Penggunaan AI untuk menciptakan konten yang sesuai dengan trend dan gaya pembacanyajuga telah banyak dimanfaatkan oleh portal-portal berita besar seperti Yahoo dan Fox.
Seperti yang sudah disebutkan di awal tulisan ini, meskipun belum secanggih Samantha di film Her (2013), tahun 2018 menandai perkembangan AI, khususnya teknologineural networks(AI yang deprogram untuk bertindak selayaknya otak manusia)yang memungkinkan aplikasi-aplikasismartphonetersebut untuk melayani dan berinteraksi layaknyapersonal assistantbagi penggunanya.
IBM baru-baru ini mengumumkan salah satu produknya, Neural Network Modeler,sebuah software untuk membantu mengembangkandeep neural networks modelsecara lebih cepat dan terstandarisasi. Produk dari IBM ini bukanlahsoftware deep learningpertama, namun menandai semakin besarnya peluang penggunaanneural networksdi tahun 2018.
Pemanfaatan teknologimachine learningdi dunia kesehatan telah lama dibicarakan dan dikembangkan. Mengingat betapa pentingnya keakuratan dalam segala hal yang berhubungan dengan diagnosa dan pengobatan, baik itu lewat alat medis maupun sistem manajemen rumah sakit, tidak heran jika banyak institusi dan perusahaan yang menginvestasikan dananya untuk pengintegrasian AI.
Beberapa teknologi AI yang sudah berhasil dikembangkan adalahsistem AI yang bisa menangkap indikasi kanker tertentu serta Alzheimerdari observasi sklera mata,mental health assistantyang mampu memprediksi kondisi psikologis pasien berdasar jawaban pasien, hinggapenggunaan AI untuk mengoptimalisasi proses riset dan penelitian obat-obatan medis.
Bagaimana? Tertarik untuk menggunakan lebih banyak teknologi berbasis artificial intelligence di tahun 2018 ini?
INFJ | Storyteller | A human nightfury",https://miro.medium.com/v2/resize:fit:1000/1*WiUb9qP5oVYmWfE6Fh9Bzw.jpeg,"AI, Artificial Intelligence, Technology, Intelijensia Buatan"
https://medium.com/s/story/pemanfaatan-machine-learning-dalam-ecommerce-988a3a28952c,Pemanfaatan Machine Learning dalam E-Commerce,AC,2017-11-12T10:46:46.816Z,"ditulisan saya sebelumnya, saya membahas aplikasi machine learning dalam media digital. kali ini saya ingin membahas di industri yang berbeda. ya, e-commerce. sedikit telat karena trend e-commerce sudah berlalu tapi kali ini ingin membahas lebih dalam tentang pengunaan machine learningnya.
Machine learning fokus kepada personalisasi user dan memberikan rekomendasi kepada user terhadap produk atau layanan. sehingga rekomendasi tersebut memberikan user experience yang lebih baik dan memberikan revenue yang lebih besar terhadap perusahaan.
Machine learning bukanlah hal yang sulit (red: lumayan) untuk diterapkan di saat ini, kebanyakan e-commerce sudahawaredengananalytics toolsseperti Google Analytics, Piwik, Kissmetrics, dll. diluar dari keterbatasan analytics tools yang disebutkan tadi, diasumsikan semua data terkait user behavior dan transaksi sudah dalam 1 data warehouse, sehingga mendapatkan gambaran yang lebih luas terhadap apa yang terjadi pada bisnis tersebut.
sebelum menerapkan machine learning ada baiknya untuk memahami target pasar yang dituju sehingga ada batasan batasan yang jelas pembuatan machine learning itu dipakai sehingga permasalahannya pun tidak terlalu besar, hal ini berguna untuk menghindari bias pada recommendation engine.
Pada dasarnya semua lini bisnis ingin melakukan customer centric, singkatnya bertumpu pada apa yang customer butuhkan a.k.a palugada, dan memberikan pelayanan terbaik after and before sales.
Customer centric is a way of doing business with your customer in a way that provides a positivecustomer experiencebefore and after the sale in order to drive repeat business, customer loyalty and profits. — superoffice.com
untuk dapat memahami customer dapat dilakukan berbagai cara, dengan data-data yang tersedia seperti user demographic sudah dapat membantu kita apa yang harus kita lakukan. contohnya jika kita mengetahui yang datang ke website kita pria maka kita dapatcustomizeddihomepagedengan barang barang yang lebih maskulin, begitu juga sebaliknya dengan wanita. maka dengan begitu rekomendasi yang kita berikan lebih relevan terhadap visitor website, tidak hanya dari pendekatan gender saja, tapi juga dari usia, geographic, devices, interest, event, etc. sehingga kita tahu betul apa yang benar benar yang dibutuhkan oleh customer.
start from the basic, hal yang sering kita temui ketika mengunjungi website E-Commerce, hal ini bisa dilakukan dengan beberapa pendekatan. Bisa berdasarkan produk yang customer lihat, atau dari demographic customer, atau juga bisa dari user behavior, sehingga dengan metode supervised learning bisa memberikan rekomendasi yang relevan. section rekomendasi ini bisa ditempatkan dibagian:
sehingga ketika customer tidak tahu mencari barang apalagi, bisa mengklik barang barang yang ada di section rekomendasi, sehingga dapat meningkatkan peluang berbelanja lebih besar, dengan begitu revenue pun meningkat.
sering dianggap sepele, tapi hal yang krusial. complement yang dimaksudkan disini di detail product, maka ada kolom produk pelengkap lainnya, jika customer melihat page detail product smartphone, maka di kolom complement of productnya bisa ada microsd, hard case, kartu perdana dan lain lain, compelemnt of prouduct ini mirip seperti system bundling, dengan learning algorithm (say association rule) nilai transaksi pun bertambah dari semula hanya membeli handphone maka ditambah barang barang lainnya.
Bermula dari kebiasaan pembeli atau toko yang memberikan pilihan lain, section similar products ini biasa ditempatkan di bagian bawah setelah detail product, pendekatannya bisa berdasarkan user behavior atau dari spesifikasi produk. dengan begitu, customer diberikan pilihan yang bervariasi, sehingga exit time tidak terlalu cepat dan pengguna dengan mudah mendapatkan produk yang diinginkannya.
mirip dengan complement of product namun pendekatannya berbeda, jika complement dilihat dari produknya, yang ini berdasarkan user behavior dan section ini pun tidak hanya melihat dari customer melihat barang A dan melihat barang B, namun ada data transaksi yang membuktikan bahwa kebanyakan orang membeli barang A juga membeli barang B.
bermula dari customer yang tidak tahu beli produk yang mana atau brand mana yang cocok untuk dirinya, maka analisis ini muncul. untuk mencapai customer-centric maka kita harus memberikan pemahaman yang baik kepada customer kita. sehingga hal ini dapat membantu customer dalam memilih, misalnyagiven data customer A based on some metrics,kita dapat menyimpulkan bahwa produk yang tepat ada produk A karena berdasarkan kebutuhan. real example customer dengan gender male, 19 y.o, yogyakarta, student mengunjungi website, dengan data data tersebut kita dapat mensuggest produk dan brand apa yang cocok dengan kriteria tersebut, sehingga nantinya hasil analisis ini berupa description untuk produk ini sangat cocok bagi karakteristik customer tersebut.
dari sekian aplikasi machine learning diatas hanya terbatas pada tampilan di website, selain itu machine learning juga bisa diaplikasikan untuk analisis seperti “kapan perusahaan harus menambah stok barang?”, “kapan perusahaan melakukan promosi produk A” dan lainnya sehingga tidak hanya dari sisi user experience saja tapi juga membantu perusahaan dalam menjalankan bisnisnya.",https://miro.medium.com/v2/resize:fit:800/1*8aYrzLrEOCs2dHv3cG6Llw.jpeg,"Customer Centric, Customer, Ecommerce, Applied Machine Learning, Machine Learning"
https://medium.com/s/story/ketika-python-bertemu-pandas-part-1-993bc9122f13,Ketika Python bertemu Pandas — #Part 1,Ramdhan Rizki,2017-12-04T01:58:36.010Z,"Codelabs is a division of digital experts. We created an application to help the people. And we’re from Indonesia Computer University.
Hello world, pada artikel kali ini saya akan sedikit mengulas mengenai Pandas. Berbicara mengenai pandas, pasti kalian sudah membayangkan hewan lucu yang suka makan bambu dari China, kalau itu sih panda bukan pandas hihihi. Yang akan kita bahas kali ini merupakan sebuah library Python yang akan sangat berguna nantinya untuk mengolah data, seperti apa sih rupa si pandas ini, yuk mari kita simak.
Pandas merupakan toolkit yang powerfull sebagai alat analisis data dan struktur untuk bahasa pemrograman Python. Dengan menggunakan pandas kita dapat mengolah data dengan mudah, salah satu fiturnya adalahDataframe.Dengan adanya fiturdataframekita dapat membaca sebuah file dan menjadikannya tabble, kita juga dapat mengolah suatu data dengan menggunakan operasi sepertijoin,distinct,group by, agregasi, dan teknik lainnya yang terdapat pada SQL.
Banyak format file yang dapat dibaca menggunakan Pandas, seperti file.txt, .csv, .tsvdan lainnya. Agar lebih jelas mari kita mencobanya secara langsung. (Catatan: Pada artikel ini saya menggunakan python versi 3.6, jika teman-teman menggunakan versi 2.7 mungkin akan sedikit berbeda)
Untuk dapat menginstall pandas, kita bisa menjalankan perintah berikut :
Atau jika teman-teman menggunakan library Anaconda, kita bisa menginstallnya dengan perintah beriktu :
Series merupakan sebuah array satu dimensi yang memiliki label dan digunakan untuk menyimpan beragam tipe data seperti integer, string, float, bahkan objek, dan lain sebagainya. Label pada series disebut dengan index. Bagaimana cara membuat series ? Perintah dasar untuk membuat sebuah Series dengan pandas adalah
Penjelasan Kode :data :parameter ini diisi dengan data yang akan dibuat seriesindex :parameter ini diisi dengan index dari series. Jumlah index harus sama dengan jumlah data. Jika kita tidak mengisi parameter index, maka series akan memiliki index integer seperti halnya array biasa.dtype :parameter ini diisi dengan tipe data dari series, sebenarnya kita tidak perlu untuk mengisi parameter ini, karena secara otomatis python akan menyimpulkan tipe data yang kita masukkan.copy :parameter untuk copy data, secara default akan bernilai false.
Bingung ? yuk mari langsung saja kita coba dengan beberapa contoh sederhana:
Pada contoh diatas kita membuat sebuahSeriesyang berisi karakter a,b,c, dan d. Jika kode diatas dijalankan maka hasilnya adalah:
Series yang kita buat memiliki index default, lalu bagaimana jika kita ingin membuat custom index ? Yup benar, kita harus menambahkan parameter kedua yaitu index. Berikut contoh sederhananya:
Jika kita jalankan, maka hasilnya adalah sebagai berikut:
Yey, sekarang kita berhasil membuat custom index pada Series kita. Sekarang bagaimana caranya untuk mengakses series berdasarkan indexnya. Caranya sama seperti kita mengakses sebuah array.
Jika dijalankan hasilnya adalah:
Lalu apa bedanyaSeriesdengan array biasa pada python?Seperti yang sudah kita bahas sebelumnya, pandas menyediakan beragam fungsi operasi untuk mengolah data. Contoh jika kita menggunakan series kita bisa mencari nilai max, min, dan mean secara langsung, bahkan kita juga bisa melakukan operasi perpangkatan pada nilaiSeriessecara langsung. Biar gak bingung yuk mari kita coba.
Sekarang kita coba membuat sebuahSeriesbaru yang akan berisi nilai float. Berikut adalah kode nya:
Sekarang mari kita coba mencari tahu berapa nilai minimum, maximum dan nilai rata-ratanya. Caranya cukup mudah, kita tinggal menggunakan saja method yang sudah disediakan oleh pandas. Berikut contoh sederhanya :
Hasilnya adalah sebagai berikut:
Cukup mudah bukan? bayangkan jika semua operasi tersebut harus kita lakukan secara manual. Contoh yang lain kita akan mencoba memangkatkan setiap nilai pada Series angka dengan pangkat 2. Bagaimana caranya? yuk langsung kita coba.
Jika perintah diatas dijalankan, maka seluruh nilai yang ada pada Series angka akan dipangkatkan 2. Hasilnya adalah sebagai berikut:
Banyak sekali method yang disediakan oleh pandas yang tentunya akan sangat memudahkan kita dalam mengolah sebuah data. Jika teman-teman ingin melihat lebih lanjut seluruh method yang disediakan pandas untuk Series, dapat menuju link berikut:
Gimana? enak kan pakai pandas?Pembahasan kali ini saya cukupkan sampai disini saja, pembahasan berikutnya kita akan sama-sama mencoba membuatDataframedan membaca file CSV dengan menggunakan pandas.
Terimakasih telah membaca artikel ini, jika dirasa bermanfaat jangan lupa tekan tombol clapnya ya..
See you next time,Salam Programmer
Image Credit :https://www.nbcnews.com/news/world/giant-pandas-are-no-longer-endangered-n643336
Codelabs is a division of digital experts. We created an application to help the people. And we’re from Indonesia Computer University.
Hi my name is Ramdhan, i’am Web Programmer | I love Coffee so much and doing code.",https://miro.medium.com/v2/resize:fit:1200/1*i1vVm3EqqDIkyucD0079wg.jpeg,"Data Science, Python"
https://medium.com/s/story/import-dataset-ke-python-dan-r-99c636c1aa69,Import Dataset ke Python dan R,R. Kukuh,2018-02-13T03:52:16.266Z,"Berikut adalah cara untuk meng-import dataset ke dalam Spyder / Python atau RStudio / R. Adapun dataset yang dibutuhkan dalam tutorial kali ini dapat di-download disini.
Buka Anaconda Navigator
Importlibraries yang dibutuhkan
Jalankan kode tersebut, maka di window console akan muncul pesan ini:
Klik tab variable explorer, akan tampak dataset yang baru saja ter-import
Dobel klik pada dataset cell, akan muncul popup window seperti ini:
Seperti yang kita lihat, dataset ini memiliki:
Kita akan membuat 2 matrix of features yang berisi values dari independent variable dan dependent variable di atas.
Ketik kode berikut untuk membuat independent variable matrix:
Eksekusi kodenya. Jika berhasil maka hasilnya seperti ini:
Ketik kode berikut untuk membuat dependent variable matrix:
Eksekusi kodenya. Jika berhasil maka hasilnya seperti ini:
Sampai di sini, jika semua dilakukan dengan benak, maka variabel explorer akan berisi seperti ini:
Cari folder tempat kerja Anda, lalu Set As Working Directory
Tulis dan eksekusi kode berikut:
Perintah di atas akan membuat sebuah global environment baru
Klik pada dataset, dan akan muncul tabel berisi dataset
Demikian import dataset dalam R. Kita tidak perlu membuat matrix of features-nya.
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*Q2pKJUS9hjcwA6QdR8krCw.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/platform-pertukaran-iklan-dengan-teknologi-ai-9a7f871c71ef,PLATFORM PERTUKARAN IKLAN DENGAN TEKNOLOGI AI,Crypto Events,2023-03-06T02:13:41.418Z,"Segera bergabung dalam Tahap Program Penjualan Token (Tokensale)UBEXPlatform
Kami menawarkan bagi anda yang ingin berinvestasi dalam Ubex Platform. Ya, sekarang sedang diluncurkan programTokensalehingga 13 Agustus mendatang dengan rincian program sebagai berikut:
⇛ UBEX TOKEN EKONOMI
Ubex Platformadalah sebuah platform yang dibuat khusus dalam bidang periklanan dengan menggunakan teknologi AI (Artificial Intelligence) yang dapat memberikan pelayanan secara maksimal bagi klien, pengiklan, dan penerbit.Ubexmemecahkan masalah industri melalui penggunaan jaringan saraf dan teknologi blockchain. Ubex Platform akan menjawab semua masalah dari 3 pihak yang berperan dalam dunia periklanan
Bagi Pengguna / Klien :
UBEX juga memiliki beberapa misi, diantaranya :
Untuk menciptakan ekosistem periklanan global dengan tingkat kepercayaan timbal balik yang tinggi dan efisiensi maksimum. Selain itu Ubex memiliki tujuan dalam pemecahan masalah periklanan, yaitu :
Memaksimalkan efisiensi pembelian iklan melalui:
Meminimalkan risiko Peserta melalui penerapan blockchain dan kontrak cerdas. Blockchain memberi peluang untuk:
PRODUK UBEX PLATFORM
Pertukaran iklan Ubex akan berlokasi diwww.ubex.comdan akan memiliki antarmuka yang mudah digunakan yang akan bekerja dengan layanan pertukaran dari PC, tablet, dan perangkat seluler. Dua bagian utama untuk pengiklan dan penerbit akan disajikan di situs web yang terdiri dari :
⇛ Layanan untuk Pengiklan
Pengiklan dapat mengatur dan menjalankan kampanye iklan dalam beberapa klik. Tugas yang mengharuskan seluruh tim pemasar digital sekarang dapat diambil alih oleh algoritme Ubex.
⇛ Layanan untuk Penerbit
Penerbit dapat dengan cepat dan mudah menghubungkan slot iklan mereka ke platform iklan Ubex dan mulai mendapatkan penghasilan segera setelah pelacakan add-on mengumpulkan cukup data tentang lalu lintas situs untuk analisis lebih lanjut oleh jaringan saraf.
Platform Ubex akan memberikan banyak peluang tidak hanya untuk penerbit dan pengiklan, tetapi juga untuk perusahaan yang memerlukan analisis regresi data pengguna. Inti sistem syaraf, yang bertanggung jawab untuk membuat keputusan pada semua tahap siklus hidup materi iklan, akan memiliki API, akses yang dapat diberikan kepada perusahaan mitra.
Token UBEX akan bertindak sebagai unit pembayaran di platform Ubex. Kebutuhan akan Token didasarkan pada kebutuhan untuk membiayai dana dari akun pengiklan untuk menampilkan materi iklan yang telah mereka beli. Cryptocurrency dasar tidak mengijinkan pengkreditan dana dari akun pihak ketiga bahkan jika seseorang memiliki izin dari pihak ketiga. Token memungkinkan untuk menetapkan kuota (kelonggaran) ke alamat tertentu di mana pemilik alamat dapat menggunakan dana yang tersedia di akun. Pemberian kredit hanya dimungkinkan dengan syarat bahwa pemegang akun telah menetapkan ukuran kuota dan hanya dari alamat tempat kuota dikeluarkan, jika tidak, dana tidak dapat dikreditkan.
Untuk itu, direkomendasikan untuk anda yang ingin mendapat keuntungan dimasa depan dengan bergabung dengan Proyek Ubex Platform. Untuk informasi selengkapnya, silahkan kunjungi situs resmi UBEX Platform dan beberapa link terkait lainnya dibawah ini :
WEBSITE||WHITEPAPER||ANN||YOUTUBE||MEDIUM||REDDIT
LINKEDIN||GITHUB||FACEBOOK||TWITTER||TELEGRAM||INSTAGRAM
Get free BTC ETH att.me/cryptoevents_ch",https://miro.medium.com/v2/resize:fit:598/0*n-DLvU9lJjcURJcv.jpg,"Advertisement, AI, Platform, Ads, Blockchain"
https://medium.com/s/story/topic-modeling-dengan-sagemaker-9d4ff364ccb5,Topic Modeling dengan SageMaker,AC,2018-05-14T10:50:22.639Z,"Founded in 2017, Warung Pintar has officially concluded its operations, marking the close of a meaningful chapter. Thank you for reading our stories along the way.
Sagemaker adalah sebuah layanan dari Amazon Web Service untuk keperluan machine learning.
Amazon SageMaker is a fully-managed service that enables data scientists and developers to quickly and easily build, train, and deploy machine learning models.
untuk memulainya login terlebih dahulu ke AWS. Jika sukses akan muncul halaman seperti di bawah ini.
Selanjutnya buka Amazon Sagemaker di section Machine Learning.
Pilih region yang diinginkan disini menggunakan US West (Oregon) dan penting untuk dicatat, data yang training yang di simpan di S3 harus sama regionnya dengan Sagemaker.
Untuk membuat notebook instance, klik linkcreate notebook instance
lalu masukan nama notebook instancenya dan IAM role terpilih secara otomatis, pilih notebook instance type disini menggunakanml.t2.medium, dibagian bawah klikcreate notebook instance.
untuk melihat spesifikasi instance type yanga ada bisa lihat di link berikut https://aws.amazon.com/sagemaker/pricing/instance-types/dan harganya ada dilink berikuthttps://aws.amazon.com/sagemaker/pricing/
setelah sukses dibuat, dibagian page notebook instance terdapat instance yang kita buat tadi, lalu klik start > open. jika notebooknya sudah selesai dipakai tinggal klik stop saja.
tampilan notebook sama seperti pada biasanya di komputer local menggunakan jupyter notebook. Untuk membuat notebooknya, kliknewlalu pilih kernel mana yang mau dipakai, disini menggunaka conda_python3.
Disini juga menggunakan S3 untuk menyimpan hasil dump modelnya, untuk melakukannya buat bucket terlebih dahulu.
buka notebook yang tadi dibuat lalu masukan script dibawah ini untuk import libraries yang dibutuhkan.
load data yang ingin diolah, disini menggunakan dataset dari scikit-learn. dataset ini digunakan untuk topic modeling menggunakan K-Means. built in algorithms yang ada di sagemaker bisa lihat dilink inihttps://docs.aws.amazon.com/sagemaker/latest/dg/algos.html
Feature Extraction menggunakan TF-IDF Vectorizer.
disini menggunakan K-Means biasa, jika ingin menggukana MiniBatchKmeans, ubah variabel minibatch menjadiTrue.training yang dilakukan disini dilakukan lokal instance, bisa juga create instance job, lebih lanjut bisa buka linkini
kemudian dump model yang dibuat lalu store di s3
lalu buat endpoint config berikut
lalu buat endpoint seperti di bawah ini.
hasilnya bisa kita lihat di page endpoint.
untuk menguji hasil train model tersebut kita bisa menggunakan endpoint yang telah dibuat.
Founded in 2017, Warung Pintar has officially concluded its operations, marking the close of a meaningful chapter. Thank you for reading our stories along the way.",https://miro.medium.com/v2/resize:fit:1200/1*3XtpdXGseA8PBqNU3WAQmg.png,"Topic Modeling, Machine Learning, Sagemaker"
https://medium.com/s/story/preview-of-how-to-build-chatbot-more-human-9e3da6e1e69,Preview of How to Build Chatbot — More Human?,Ron Ashrovy,2017-10-26T05:13:52.222Z,"Kita hidup di era perkembangan chatbot yang sangat pesat, dimana kemunculan aplikasi-aplikasi bot terus bermunculan, beberapa mediachattingpun mulai merambah ke memunculkan bot, dimulai dari telegram hingga kini ke messenger. Pertanyaan yang mendasar ialah dimana letak AI pada chatbot? Lalu dimana letakMachine LearningdanNatural Language Processing? Lalu bagaimana cara membuat chatbot menjadi cerdas
Well, pada literatur yang sering saya baca, chatbot atau chatterbot menjadi populer karena adanyaTuring Test. SederhananyaTuring Testadalah sebuah AI yang dibangun mirip seperti manusia. Dan ini menjadi sangat menarik karena ada kompetisi bergengsi yang bernamaLoebner Prizeyang melombakan ini. Nah saat ini kita banyak mengenal aplikasi chatbot yang sangat populer seperti Siri, Google Assistant, Alexa, Bixby dan lain-lain yang memungkinkan untuk memberikan sebuah informasi, mencatat sebuah keperluan hingga menjalankan perintah dengan sebuah “perintah yang tidak terstruktur” yang anda berikan kepada mereka.
Kecerdasan ini yang bisa kita sebut sebagai AI-nya Chatbot
Mengapa demikian? Karena bot mampu memberikan secara kontiniu informasi atau menjalankan perintah secara relevan sesuai dengan yang minta.
Menurut Russell dan Norvig pada “Artificial Intelligence: a Modern Approach”Intelligent agentatau chatbot harus mampu mencapai siklus“sense-think-act”(merasa-berpikir-bertindak) agar bot terasa cerdas. Lalu apa maksudnya?
Agencyadalah salah satu fitur yang penting didalam chatbot. Kemampuan chatbot untuk mencapai goal dan memproses secara otomatis (tanpa instruksi secara spesifik) hal tersebut membuktikan bahwa chatbot sudah sudah sangat cerdas. Pastinya hal ini tidak mudah dicapai karena membutuhkan model yang rasional, social interaction, emotion dan lain-lainnya. Sebagai bacaan anda bisa mempelajaricrowd simulation.
Sensingadalah kemampuan bot untuk beradaptasi terhadap lingkunganya. Jika pada chatbot maka kemampuan untuk menangkap pembicaraanusermenjadi bagian dari siklussenseitu sendiri. Didalam implementasinya menciptakansensepada bot merupakan tantangan besar.
Thinking, artiannya sama dengan literal terjemahannya yaitu berpikir. Pada siklus ini dibagi menjadi beberapa bagian:
1.Mengubahinformasi yang diterima menjadi bentuk yang dapat dimengerti atau diduga oleh mesin.
2.Menyimpaninformasi padaknowledge basedlalu diproses (atau dipikirkan oleh bot tersebut).
3.Memperbaharuistateagent atau chatbot berdasarkan kemampuan yang sudah ada sebelumnya dan kemampuan yang baru diperoleh.
4.Membuat keputusanberdasarkan semuaknowledgeyang telah diperoleh.
5.Mengubah keputusanmenjadi sebuahtindakan.
Saat ini dengandeep learningdanneural network,learningmenjadi sangat amat mungkin.Learningmemungkinkan chatbot untuk melihat pola didalam sebuah informasi yang diterima kemudian di respon. Tetapi inilearningsaja tidak cukup membuat chatbot dinyatakan memiliki AI karena ada beberapa chatbot yang dibangun tanpa menggunakanlearning component. Contoh, anda bisa meng-akses eksperimen padalink ini.
Hal lainnya yang biasa digunakan untuk membuat chatbot mengerti pada suatu input ialahNatural Language processing and understanding.
Selain ini semua ada bagian yang cukup krusial untuk membuat chatbot menjadi sangat cerdas yaituknowledge baseatauknowledge representationyang intinya adalah bagaimana sebuah informasi diperoleh dan di simpan. Hal ini menjadi sangat penting karena sedari awal (sebelum mesin dilatih) informasi yang diberikan untuk dipelajari sudah ditentukan sehingga mesin menjadi cerdas. Kita mungkin merasa bahwa chatbot sepertiSiridanAlexaterasa sangat cerdas, hal tersebut ternyatabeberapa pengetahuan diwakili secara internal saja. Oleh sebab itu (1) mesin dapat belajar lebih cepat (2) dapat menentukan informasi yang relevan (3) dan dapat memberikan alasan mengapa informasi itu relevan. Jika informasi yang disimpan denganstrukutur data dan aturan yang baik maka efektivitas disetiaplearningyang dilakukan menjadi sangat efektif.
Siklus terakhir ialahdecide, yaitu sistem memutuskan sesuatu dari informasi yang diterima dan yang telah dipelajari. Kalau kita mengambil kasus chatbot maka. Chatbot mampu untuk memprediksi kata, atau apa saja yang ingin dia katakan nantinya. Akan sangat cerdas apabila bot mampu untuk memodifikasidecidedari informasi yang baru dikumpulkan.
Setelahdecidedibuat maka berikutnya ialahaction.
Action Chatbot dalam bentuk teks lebih mudah dibandingkan dengan menggunakan suara, karena intonasi yang menjadi output akan cukup menentukanmeaningdari konteks yang ada.
Saya sengaja mem-posting tulisan ini dengan ringkas dibanding versi aslinya karena saya mencari poin-poin terkait dengan algoritma didalam membangun sebuah chatbot.
Founder at Dialogika",https://miro.medium.com/v2/resize:fit:1200/1*XP7bt7SmwicJhZdrHFGpjg.jpeg,"Chatbot, AI, Preview, More Life, Artikel Bahasa Indonesia"
https://medium.com/s/story/service-dan-perbaikan-alat-berat-hub-0812-5391-5508-9e84db39de8b,heavySERVICE DAN PERBAIKAN ALAT BERAT HUB :0812–5391–5508,jeje TA,2018-09-25T03:27:47.554Z,"Kami menerima jasa perbaikan alat berat seperti, Excavator,Dozer dll
Dalam pekerjaanService and Maintanance, kami mampu memperbaiki banyak hal antara lain:
Cylinder and Jack Hydraulic
Hydraulic Pump and Hydraulic Motor
Testing Pump and Motor Hydraulic
B. LAYANAN TAMBAHAN YANG KAMI BERIKAN
Ada beberapa jenis Sistem Perawatan/Pemeliharaan Yang Ditawarkan, yaitu:
Layanan on-caIl selama jam kerja(Senin — Jumat pukul 08.30–17.00 WIB ) kecuali hari libur.
Memberikan Response secepatnya sesuai jam kerja setelah Permintaan diterima
Memberikan Layanan Emergency Call Free 24 jam bagi Customer yang ingin mengadukan keluhannya kapan saja padanotelp 0812–5391–5508, dan kami akan mengirimkan Mekanik dengan segera ke lokasi Customer jika diperlukan.
Pekerjaan diluar jam kerja (hari libur) dikenakan biaya uang lembur, uang makan dan transport sesuai dengan standart perusahaan pelanggan.",https://miro.medium.com/v2/resize:fit:741/1*zlU_mBEYCD_RQ_s3rldMuA.jpeg,"Manufacturers, Machismo, Manufacturing, Machine Learning, Management And Leadership"
https://medium.com/s/story/sekilas-tentang-r-dan-mengapa-saya-menyukainya-9eb8886339e1,Sekilas Tentang R dan Mengapa Saya Menyukainya,Jati Satrio,2018-04-27T12:38:09.435Z,"Saya berkenalan dengan R baru sekitar setengah tahun ke belakang. Itu pun kalau tidak dipaksa berkenalan dengan seorang dosen, mungkin saya juga ogah-ogah segan untuk berkenalan dengan R. Saat ini saya sedang jatuh cinta dengan R, dan sering membuat analisis-analisis iseng dengannya.
R itu apa sih? Jadi R itu singkatnya adalah sebuah bahasa pemograman dansoftware environmentyang biasa digunakan untuk melakukan analisis data. Banyak program analisis data seperti SPSS atau Stata yang juga mumpuni namun datang dengan harga yang tentu saja tidak cukup untuk kantong kebanyakan dari kita. Oleh karena sifatnya yangfree to useini, R banyak disukai oleh para analis data. Bahkan hasil analisis R ini seringmejengdi koran-koran berita ternama.
Cool!Keren banget ya R ini berarti? Ya bisa dibilang begitu, tapi bisa dibilang kurva belajar R ini cukup curam terutama apabila kita tidak paham sama sekali tentang pemograman. Namun apabila logika kita jalan, sesungguhnya melakukanscriptingdi R ini sangatlah mudah.
Kalau kalian berminat untuk menjajal seperti apa sih melakukan analisis data dengan R, pertama-tama kalian butuh R dan R Studio. Kita butuh dua hal ini banget? Ya. Anggap saja R ini sebagai OS dan R Studio ini sebagaiHandphonekalian. Nanti di R Studio, kalian dapat menginstal berbagai aplikasi keren untuk melakukan analisis data.
Untuk menginstall R, kalian dapat mengakseshttps://cran.r-project.org/
Untuk menginstall R Studio, kalian dapat mengakseshttps://www.rstudio.com/
Pantai Sanur di Pulau Bali,
indah sekali di kala pagi.
Jikalau adalah lain kali,
mungkin nanti disambung lagi.
Coret-coret bagi ilmu",https://miro.medium.com/v2/resize:fit:1200/1*BgoeftOFRdO7g8F3O1eXSQ.png,"R, Data Science, Data Analysis"
https://medium.com/s/story/ai-yang-membuat-kode-sendiri-bayou-9f65de70cd25,AI yang Membuat Kode Sendiri: BAYOU,Andi muhammad,2018-05-11T00:14:26.113Z,"Forming tech based society
Artificial Intelligence (AI) sekarang mencapai sebuah kemajuan yang besar. Bayangkan, sebuah AI bisa menulis kodenya sendiri. Apakah AI itu akan menjadi salah satu pencapaian besar untuk umat manusia?
Atau ini berarti akan datang sebuah saat yang benar — benar mengguncang, sebuah umat robot telah muncul dari layar kode ciptaan manusia? Mari kita telaah lebih dalam lagi.
Nama AI tersebut adalah BAYOU. BAYOU merupakan AI yang dibuat oleh universitas Rice di Houston,USA dari sokongan dana Militer dan Google. Dengan 2 sokongan raksasa, mereka melatih BAYOU menggunakan lebih dari 1500 aplikasi Android (yang berarti sekitar 100 juta baris kode Java).
Semua Kode itu dimasukkan ke dalam neural net dari BAYOU dan hasilnya adalah sebuah AI yang bisa menghasilkan baris program untuk software yang kalian inginkan.
Cara kerja BAYOU adalah kita memberikan input berupa kode — kode yang akan kita buat. BAYOU akan membaca kode — kode tersebut untuk menangkap apakah “maksud/tujuan” dari kode yang dibuat. BAYOU kemudian membuat saran — saran berupa kode alternatif yang bisa menggantikan kode java yang dibuat dengan kode alternatif yang dinamakansketcehs.
Sketches ini masih harus disusun kembali oleh programmer untuk menjadi program yang benar. Walaupun begitu, pekerjaan programmer tambah mudah dengan adanya BAYOU.
Pertanyaan self replicating
Sekarang, pertanyaan terbesarnya adalah apakah ini merupakan salah satu tahap dimanga AI akan bisa berpikir sendiri dengan membuat program? MenurutFuturism, BAYOU hanya akan membuat saran berupa beberapa alternatif baris kode yang masih harus disusun kembali oleh para programmer.
Bayou tidak bisa membarikan “konteks” sendiri untuk menentukan program mana yang bisa berguna. Tapi ini tidak menutup kemungkinan bahwa masa — masa dimana AI bisa berfikir sendiri sudah ada di depan mata.
Disadur:https://futurism.com/military-created-ai-learned-to-program/
Forming tech based society",https://miro.medium.com/v2/resize:fit:400/1*ovifVIaMo6jag7cj_00_yg.jpeg,Artificial Intelligence
https://medium.com/s/story/lawan-di-depan-diabaikan-9fbe5665caaf,Lawan di Depan Diabaikan,Reynaldo Santa,2018-02-27T14:48:53.381Z,"Dunia yang kita tinggali saat ini sudah tidak sama lagi seperti belasan atau puluhan tahun lalu. Mungkin 30 tahun lagi, berwisata ke luar angkasa seperti ke Mars bukan lagi hal yang mustahil. Saat ini saja Elon Musk sudah menciptakan SpaceX dan mengujicobanya, setelah sebelumnya cukup berhasil menghadirkan mobil elektrik premium yang dapat melaju dengan kecepatan tinggi.
Kita selalu berfikir tentang hal yang kita hanya ketahui dan terlihat masuk akal, namun kita tidak pernah berfikir tentang sesuatu yang mustahil,beyond our imagination. Namun beberapa orang mampu berfikir dan bertindak serta merealisasikan imajinasinya yang liar, siapa yang menyangka smartphone saat ini dipenuhi oleh layar besar dan dapat kita sentuh. Sebelum tahun 2007, sebelum Apple meluncurkan iPhone generasi pertama, kita semua dapat melihat bahwa model ponsel ataupun ponsel pintar (bersistem operasi Windows Mobile) memiliki layar yang relatif kecil dan terdapat tombol-tombol atau menggunakan layar sentuh namun masih menggunakanstylus. Namun, setelah Steve Jobs (Apple) meluncurkan iPhone generasi pertama, perubahan perlahan tapi pasti terjadi pada seluruh produsen ponsel mereka mulai menerapkan konsep ponsel dengan layar sentuh besar dan dapat digunakan tanpastylus.
Itulah mengapa imajinasi liar yang mungkin tidak masuk akal menjadi sesuatu yang sangat mungkin bahkan merubah segalanya dikemudian hari. Begitupula dengan persaingan saat ini, kita dapat meyakini bahwa sepuluh atau 15 tahun lagi persaingan kita bukan lagi antar sesama manusia namun dengan kecanggihan teknologi terutama AI (Artificial Intelligence). Jika dahulu robot diciptakan dengan cara diprogram dan hanya terbatas pada hal yang sudah diprogramkan, namun berbeda dengan AI. Ia dapat belajar sendiri serta berfikir sendiri. Berikut adalah contoh AI dalam bentuk robot manusia.
Kita harus berfikirbeyond our imagination, jika selama ini kita berfikir bahwa kita menciptakan robot dan kita lebih pintar dari yang kita ciptakan. Hal tersebut sudah terjawab dengan Alpha Go yang merupakan sebuah proyek kecerdasan buatan dari Google yang didesain untuk bisa bermain Go, sebuah board game strategi klasik asal Cina untuk dua orang pemain. Alpha Go berhasil mengalahkan Lee Se-Dol, pemain Go terbaik di dunia asal Korea Selatan. Tidak tanggung-tanggung, Alpha Go bahkan mengalahkan pemegang 18 kali piala dunia bermain Go itu dalam dua kali pertandingan. Kemenangan tersebut membuat banyak ahli sulit untuk memercayainya. Karena untuk bisa mengembangkan algoritma untuk bermain Go sangatlah rumit, tidak semudah game lain seperti Catur maupun Monopoli yang sudah sempat dikembangkan.
Hal yang lebih mustahil lainnya adalah AI buatan OpenAI yang membuat kecerdasan buatan berupa pemain Dota 2 dan terbukti berhasil mengalahkan Danylo “Dendi” Ishutin di kancah pertandingan e-Sport internasional. AI besutan OpenAI mampu mempelajari dan menyempurnakan teknik bermainDota 2dalam masa latihan selama dua minggu. Dalam kurun waktu yang terbilang singkat itu pula kemampuannya berkembang sehingga mampu menandingi manusia pemain profesional dengan pengalaman yang jauh lebih banyak.
Jadi, sangat mungkin dan sudah mulai terlihat bahwa lawan kita sebenarnya di masa yang akan datang adalah robot, Artificial Intelligence, dan semacamnya. Kita lihat saja pada saat ini, peran manusia dalam pabrik-pabrik besar sudah mulai digantikan oleh robot-robot yang hanya diprogram dan bukan merupakan AI (yang jauh lebih hebat). Mobil tanpa supir yang mulai dikembangkan dan semakin bagus dapat menggantikan jasa supir. Sehingga kita tidak bisa lagi mengabaikan lawan di depan kita yang berupa teknologi yang kita anggap tidak mampu namun kita harus mulai memikirkan strategi-strategi jika hal tersebut terjadi. Hal yang sangat mungkin dilakukan saat ini agar tidak terjadi penyalahgunaan AI kedepannya adalah mengatur regulasi tentang pengembangan kecerdasan buatan atauArtificial Intelligence. Karena akan sangat berbahaya jika hal ini disalahgunakan.
“I think we should be very careful about artificial intelligence. If I had to guess at what our biggest existential threat is, it’s probably that. So we need to be very careful.”―Elon Musk",,"Artificial Intelligence, Disruptive Technologies, Technology"
https://medium.com/s/story/belajar-machine-learning-sederhana-dengan-weka-a07032db8e12,Belajar Machine Learning Sederhana dengan WEKA,Rosdyana Kusuma,2018-01-03T17:06:44.341Z,"Sebelum kita memulai dengan sebuahsimple problem machine learningdenganWEKA, ada baiknya kita mengetahui terlebih dahulu secara singkat apa ituMachine Learningdan apa ituWEKA.
Machine Learningmerupakan salah satu cabang dari disiplin ilmu Kecerdasan Buatan (Artificial Intellegence) yang membahas mengenai pembangunan sistem yang berdasarkan pada data. Saya tidak akan menjelaskan secara detail karena topic utama disana adalah cara menggunakanWEKAuntuk memecahkan simple problem dimachine learning. Artikel menarik daridailysocial.idini cukup bagus untuk mengerti lebih lanjut tentangmachine learning(https://dailysocial.id/post/machine-learning/)
Sedangkan WEKA adalah sebuahmachine learning toolyang ditulis dengan bahasa pemrogramanJava.WEKAini memuat banyakmachine learningalgoritma di dalam nya. DenganWEKAkita bisa melakukanpre-processing data,classification,regression,clustering,association rules, dan jugavisualization. WEKA merupakan singkatan dariWaikato Environment for Knowledge Analysis, yang dikembangkan oleh Waikato University di New Zealand. WEKA juga merupakan sebuah nama dari burung yang cuma berhabitat di New Zealand.
Mari kita mulai untuk mencoba sebuahsimple machine learning problemdengan menggunakan WEKA. Sebelumnya, anda dapat mengunduh WEKA di tautan berikut,http://www.cs.waikato.ac.nz/ml/weka/downloading.html.
Kita akan memakai dataset yang sudah saya persiapkan, atau bisa dengan dataset yang lain, karena inti dari artikel ini adalah penggunaan WEKA. Dataset dapat di unduh di tautan berikut,unduh dataset.
Part-time software engineer and full-time daddy.",https://miro.medium.com/v2/resize:fit:359/1*tP6kRT52bV73hNvAbn-DJQ.jpeg,"Machine Learning, Weka"
https://medium.com/s/story/cara-mudah-membangun-android-realtime-chatbot-dengan-dialogflow-rxjava2-dan-kotlin-a415caa67efb,"Cara mudah membangun Android Realtime chatbot dengan Dialogflow, RxJava2 dan kotlin",Ibnu Muzzakkir,2017-10-24T03:59:53.406Z,"Kebutuhan sebuah sistem yang sifatnya otomatis sangatlah diperlukan untuk saat ini. Banyak banget sistem yang awalnya konvensional sekarang sudah dirubah menjadi terkomputerisasi dan otomatis. Hal ini menyebabkan kebutuhan akan orang — orang IT yang berkualitas sangatlah banyak.
Contoh dasar yang sangat berperan adalah chatbot. kenapa sih milih chatbot? alasannya adalah kemampuan Customer service menjawab pertanyaan user di sesi chat itu memerlukan waktu yang cukup ada jeda, kemudian ditambah lagi dengan frekuensi jumlah pertanyaan user yang sama itu membuat CS pastinya merasa bosen untuk menjawab hal yang sama dengan pertanyaan yg sifatnya mirip.
Solusi dari masalah diatas adalah kita membuat sebuah chatbot, dimana user mengajukan pertanyaan kepada bot yang telah dibuat dan bot akan memberikan jawabannya secepat mungkin. trus itu mungkin ada pertanyaan dari kalian gimana cara bikin botnya ? kan harus ada pengetahuan machine learning ? jawabannya adalah “Google”. yap google, google punya service untuk NLP (Neuro Linguistic Programming) namanya Dialogflow yang sebelumnya bernama API.AI. jadi ini membuat gua ( Mobile Developer ) menjadi lebih instant membuat sebuah chatbot dibanding harus membuatnya from scratch untuk machine learningnya.
Sebelum masuk ke dalam getting startednya, gua mau jelasin dulu ke kalian gimana sih alurnya ?, Diagram flow diatas itu alur chatbot yang akan kita buat secara simple. jadi pertama kali user akan mengirimkan sebuah pesan dari devicenya ke firebase yang kemudian diteruskan ke dialogflow. kenapa harus di simpan ke firebase db ? jawabannya supaya kita ada historynya user nanya apa dan botnya nnti jawab apa.
Setelah itu device user akan mendapatkan response dari dialogflow terkait dari pertanyaan yang dibuat oleh user. saat itu juga device melakukan state penyimpanan response dari dialogflow ke dalam firebase database. jadi sebenarnya device ini hanya berpacu kepada data yang disimpan di dalam firebase database (hasil sebelum dan sesudah olahan dari dialog flow).
oke kalau teman-teman sudah kebayang dari gambaran sistem chatbot yang kita mau coba buat. ada beberapa requirement yang harus teman- teman penuhi biar lancar urusan membuat chatbotnya nih. diantaranya :
Awal memulai project android studionya sangatlah mudah. tambahkan library dialogflow ke dalam project android kalian. jangan lupa menambahkannya di dalam app module ya gradlenya.
Setelah kalian menambahkan libnya dialogflow ke dalam project kalian maka saat kalian coba sync akan ada error.
etelah ditelusuri kita harus menambahkan annotationProcessor pada library “implementation ‘ai.api:libai:1.4.8'” agar tidak terdeteksi error saat melakukan proses build. tambahkan seperti line code di bawah ini
Setelah kita menambahkan annotationProcessor masalah tersolved, namun masih ada 1 masalah lagi ini gua ngetesnya pake emulator android O sih belum coba versi dibawahnya jadi saat gua juga coba build and run appnya dia kena error kaya gini
untuk solve masalah ini kita harus menambahkan exclude module dari log4j di library “ai.api:libai:1.4.8”. jadi hasilnya akan seperti line code dibawah ini
Setelah library dialog flownya selesai kita setup. nah full setupnya dan tech yang dipake untuk project ini ada di gits ini ya
Sebelum masuk ke functionnya kita harus buat layoutnya dulu. ada beberapa layout yang kita butuhkan diantaranya :
untuk membuat tampilan UI item chatnya seperti line code dibawah ini
untuk membuat tampilan main activitynya. kalian bisa contoh seperti line code dibawah ini
Secara logic untuk menampilkan sebuah list di android biasanya kita menggunakan RecyclerView dan RecyclerViewAdapter untuk handling setiap item dari chatnya. di app ini sedikit berbeda karena kita menggunakan Firebase Database untuk realtime chatnya jadi kita menggunakan FirebaseRecyclerAdapter yang akan dijadikan sebagai adapter.
setelah itu kita membuat adapternya terlebih dahulu dengan nama ChatAdapter.java. Hasilnya akan terlihat akan seperti dibawah ini
Setelah itu di dalam sebuah adapter terdapat viewholder untuk handling setiap yang tercreate didalam recyclerview. jadi kita buat lagi 1 file untuk chat view holdernya. hasilnya akan terlihat seperti line code dibawah ini
setelah ChatVH dan adapternya kita buat sekarang kita setup adapternya di activity yang sudah ada. kita configurasi dialog flownya terlebih dahulu.
maksud dari line code diatas adalah kita membuat sebuah AIConfiguration dimana kita masukkan ACCESS_CLIENT_ID, bahasa yang dibuat , dan AIConfigurationnya. setelah itu kita set di class AIDataService config yang kita buat tadi.
Potongan kode diatas adalah kita menset adapter dan new instance dari class chatadapter yang telah kita buat. jadi kalau di app ini gua memisahkan setiap chat user dengan bot menggunakan id dari user itu sendiri, sehingga kita membutuhkan sebuah query ke firebase db untuk menselect semua chat kita dengan bot yang idnya sama dengan id user.
Jika kalian bertanya dimana sih proses sending message dari device ke dialogflownya, contohnya seperti dibawah ini. saya buat menggunakan rxjava2.
jadi kita membuat ai request baru kemudian message yang kita buat akan masuk ke dalam query dari request tersebut. dari request tersebut akan dilanjutkan oleh aiservice yang telah kita konfigurasi sebelumnya. setelah itu seperti halnya kita hit sebuah REST API kita akan mendapatkan response dari dialogflow yang kita parsing ke dalam class yang kita butuhkan.
untuk menampilkan resultnya di chat. maka kita perlu mengirimkan repsonse dari chatbot tersebut ke firebase. hal ini dibuat seperti ini karena device kita untuk data chatnya melisten ke firebase. sehingga apabila terjadi perubahan di database maka firebasekan akan trigger chat kita dan device akan melakukan query kembali ke firebase db hingga hasilnya ditampilkan ke chat list.
Strong Research, Hot tea and bread ! that’s enough. Follow my Github :https://github.com/astroidnu",https://miro.medium.com/v2/resize:fit:780/1*gF2Bwsma_aCnEcVtykj4NQ.png,"Kotlin, Chatbots, AndroidDev, Machine Learning, Android Apps"
https://medium.com/s/story/pekerjaan-yang-terancam-oleh-automasi-mesin-a75c980a697f,Pekerjaan yang Terancam oleh Automasi Mesin,Makers Institute,2018-03-01T12:14:18.486Z,"Banyak pekerjaan yang ada saat ini belum ada di masa lalu. Bila Anda bertanya kepada anak kecil zaman ini: apa cita-citanya bila ia besar? Boleh jadi jawaban mereka mungkin belum terbayangkan pada lima tahun hingga satu dekade ke belakang. Jawaban macam ‘buzzer/endorser’, ‘Selebgram’ atau ‘Youtuber’ sebagai cita-cita anak-anak yang terlahir sebagaidigital native, misalnya, tak akan pernah tergambar oleh generasiBaby Boomers(lahir antara tahun 1940–1960) atauGeneration X(lahir pada 1961–1980).
Nah, begitu pula bidang pekerjaan di masa depan. Akan banyak pekerjaan baru yang mungkin sekarang belum terbayangkan oleh kita saat ini. Tentu saja, akan banyak pula pekerjaan-pekerjaan yang bakal punah akibat kemajuan teknologi.
Bahkan, menurut riset yang dilakukan oleh Gallup, generasiMillenial(lahir pada 1981–2000)akan mengalami ancaman kehilangan pekerjaan terbesar yakni sekitar 37 persen akibat kemajuanArtificial Intelligence(AI). Adapun generasi yang lebih tua mengalami ancaman yang lebih kecil, karena posisi yang mereka miliki tentu saja sudah lebih senior.
Hasilriset dari McKinseybahkan mengatakan bahwa setengah dari bidang pekerjaan kita akan dapat diautomasi (dikerjakan oleh mesin) pada 2055, atau bahkan skenario paling ekstrem, yakni pada 2035. Lalu bidang pekerjaan apa saja yang terancam di masa depan? Menurut prediksi McKinsey fungsi pekerjaan di bidang-bidang tersebut adalah:
Di bidang ini, fungsi pekerjaan operasional mesin dan pekerjaan-pekerjaan fisik dalam lingkungan kerja yang mudah diprediksi, seperti memuat material, produksi pengepakan, hingga pemeliharaan peralatan, akan sangat mudah untuk diotomasi. Namun, fungsi pekerjaan yang sifatnya lebih sulit diprediksi, seperti pelayanan pelanggan (customer care), akan lebih sulit untuk diotomasi oleh mesin
oAkomodasi dan jasa makanan (73%)
Sektor ini, di mana separuh waktu kerja yang dijalankan adalah melibatkan kegiatan fisik dan operasional, akan sangat riskan terhadap potensi otomasi mesin. Fungsi-fungsi tersebut mencakup kegiatan memasak, memperisapkan makanan, membersihkan bahan makanan, membersihkan tempat makan, mengumpulkan cucian, dan lain-lain
oPerdagangan ritel (53%)
Seperti halnya manufaktur, ritel akan diuntungkan oleh efisiensi menggunakan kecanggihan teknologi, dalam hal manajemen dan logisitk, misalnya. Pengepakan barang seblum pengiriman, stocking item, pencatatan penjualan, pengumpulan informasi produk pelanggan, dan kegiatan-kegiatan pencatatan koleksi data, semuanya dengan mudah diotomasi. Sementara fungsi penjualan oleh salesperson memiliki potensi otomasi yang lebih kecil (47%) karena juga membutuhkan kemampuan kognitif dan keahlian sosial selain aktivitas fisik
oKeuangan dan Asuransi (43%)
Jasa keuangan dan asuransi lebih banyak melibatkan kegiatan pengumpulan dan pemrosesan data. Kegiatan di sektor ini lebih banyak berkutat dengan komputer dan teknologi, mulai dari pencatatan pengajuan aplikasi, invoice dengan file PDF, ilutrasi dan informasi produk, hingga proses verifikasi calon nasabah. Oleh karenanya potensi otomasi terhadap sektor ini cukup besar.
Sektor konstruksi lebih banyak mencurahkan waktu kerja mereka pada fungsi-fungsi pekerjaan fisik namun dikerjakan di lingkungan kerja yang tidak mudah diprediksi. Lingkungan kerja yang tidak mudah diprediksi maksudnya adalah lingkungan kerja yang kondisinya berubah-ubah/ dinamis setiap saat, misalnya mengoperasikan mesin crane.
Sama seperti halnya konstruksi, pertanian juga banyak melibatkan pekerjaan-pekerjaan fisik pada lingkungan yang kurang dapat diprediksi. Misalnya memanen hasil pertanian, menangkap ikan, dan lain-lain
Pada prakteknya, proses automasi dipengaruhi oleh lima faktor penentu: kemungkinan teknis (technical feasibilty), biaya automasi (cost to otomate), kelangkaan relatif (relative scarcity), kemampuan (skill), serta biaya pekerja (cost of worker).
Jadi, mulai saat ini Anda harus jeli untuk menentukan bidang pekerjaan mana yang akan Anda geluti agar tak kalah bersaing -termasuk dengan mesin/robot- di masa mendatang.
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:603/1*qVFL3HmV3JQzZ6vILOkB8A.png,"Artificial Intelligence, Job Market, Automation, Makers Institute, Future Of Work"
https://medium.com/s/story/ai-dan-jurnalisme-robot-a84b3759464c,AI dan Jurnalisme Robot,Makers Institute,2018-03-02T15:35:10.311Z,"PengembanganArtificial Intelligence(AI) danMachine Learningdi segala bidang pada akhirnya juga memasuki wilayahnewsroom. Media dan kantor berita mulai menerapkan jurnalisme robot atau jurnalisme yang menggunakan bantuan mesin/ komputer -tanpa campur tangan reporter manusia- untuk menghasilkan laporan/ artikel.
Kantor beritaAssociated Press(AP) merupakan salah satu pionir, pada 2014 menggandeng startup bernamaAutomated Insightmengadopsi AI untuk memproduksi artikel tentang laporan keuangan perusahaan. Di tahun yang sama,LA Timesjuga mulai membuat berita peringatan otomatis tentang gempa mengunakan algoritma komputer yang bersumber dari data USGS (badan geologi nasional AS).
Pada 2016,Washington Postjuga mulai menggunakan sistem AI yang dinamakanHeliografuntuk memproduksi artikel, termasuk laporan-laporan singkat mengenai Olimpiade Rio de Janeiro. Awal tahun ini, kantor berita CinaXinhuajuga secara resmi mengumumkan integrasi AI ke dalam platform produksi berita mereka.
Tak hanya AI, platform yang mereka namakan ‘Media Brain’ itu juga akan menerapkancloud computing,Internet of Thing, danBig Data. Tugasnya adalah mengoptimalkan setiap tahapan di dalam proses produksi berita; mulai dari pengumpulan bahan berita, pencarianleadtulisan, pembuatan berita, penyuntingan, distribusi berita, hingga analisa umpan balik.
Tak mau ketinggalan, salah satu media baru di Tanah Air,Beritagar.id, Sabtu 24 Februari 2018 mengumumkan penerapan jurnalisme robot dalam artikel-artikel beritanya. “Mulai pekan ini, salah satu hasil eksperimen tim produk akan mulai dipublikasikan. Kami menamainya,Robotorial,” tulis Beritagar dalam blog mereka.
Sebagai awalan, Situs milik grup Djarum itu, memproduksi artikel hasil pertandingan sepakbola Liga Inggris. karena pola datanya dianggap konsisten di setiap pertandingan; seperti informasi tentang gol yang tercipta, pencipta gol, dan pemenang pertandingan.
Bila disimak sekilas, beberapa artikel Robotorial memiliki redaksi tulisan yang sulit dibedakan dengan tulisan seorang jurnalis. Namun bila diperhatikan pola di beberapa artikel, kita akan menemukan pengulangan-pengulangan yang terasa sekali bernuansa ‘template’. Misalnya pada struktur paragraf pembuka di dua artikel di bawah ini; hasil pertandingan Brighton vs Swansea, dengan hasil pertandingan MU vs Chelsea:
“Brighton menjaga langkah untuk mendekatkan diri dengan rival Bournemouth di klasemen Liga Primer Inggris (EPL). Bertanding di The American Express Community Stadium (Falmer, East Sussex), Sabtu (24/02/2018), tuan rumah Brighton menundukkan tim lawan Swansea dengan skor 4–1.”
“Manchester United menjaga langkah untuk mendekatkan diri dengan rival Manchester City di klasemen Liga Primer Inggris (EPL). Bertanding di Old Trafford (Manchester), Minggu (25/02/2018), tuan rumah Manchester United menundukkan tim lawan Chelsea dengan skor 2–1.”
Selain itu, penggunaan AI dalam artikel berita juga dapat menghasilkan disinformasi bila data yang dipergunakan oleh sistem ternyata salah. Misalnya pada kasus yang dialami oleh LA Times Juni tahun lalu. Saat itu LA Times melaporkan terjadinya gempa bumi dengan magnitude 6,8 di California. Padahal penduduk setempat tidak merasakan getaran apapun saat itu.
Ternyata, kesalahan laporan disebabkan karena staf USGS baru saja mempublikasikan data gempa yang terjadi pada tahun 1925. Karena algoritma program itu memang tak mampu melakukan fungsi reporter yang bisa melakukan klarifikasi ke berbagai sumber berbeda, maka kesalahan informasi tak terhindarkan. Terlepas dari itu semua, harus diakui bahwa teknologi memudahkan tugas-tugas jurnalistik. Bahkan artikel yang dibuat oleh para jurnalis pun terkadang diralat karena mengalami kesalahan.
Dengan bantuan AI, laporan pandangan mata atas pidato seorang tokoh bisa dipublikasikan secara cepat, bahkan sebelum sang tokoh turun panggung. Dengan AI, Associated Press sukses meningkatkan produksi artikel terkait laporan keuangan perusahaan, dari 300 artikel menjadi 4000 artikel. Bahkan survei yang diselenggarakan Reuters Institute of Journalism pada 2017 mencatat bahwa kini sudah ada ribuan artikel tulisan buatan robot yang diproduksi di Eropa, setiap bulan.
Beberapa media bahkan menggunakan AI lebih dari sekadar mengautomasi produksi artikel. Mereka mulai menggunakan machine learning untuk mencari tema yang menarik untuk tulisan.ProPublicamenggunakan machine learning untuk mendeteksi isu penting dari setiap anggota kongres.
SitusAtlanticmemanfaatkan machine learning untuk mengetahui apakah Presiden Donald Trump menulis tweet-nya sendiri atau tidak. Bahkan timBuzzFeedmemanfaatkan machine learning untuk mengidentifikasi pesawat pengintai yang dioperasikan oleh US Marshalls dan perusahaan kontraktor militer AS.
Saat ini, AI dan machine learning memang tidak pada posisi ‘mematikan’ mata pencarian reporter, karena banyaknya keterbatasan dan tantangan yang dihadapi untuk menggantikan fungsi seorang jurnalis sepenuhnya.
Namun, Kris Hammond, Cofounder Narrative Science (startup di bidang otomatisasi penulisan artikel), optimis dalam waktu 15 tahun ke depan, 90 persen tulisan akan dihasilkan oleh mesin/ robot, bukan manusia. Boleh percaya atau tidak, tapi sepertinya memang kini saatnya media mulai mempertimbangkan untuk mengadopsi AI dan machine learning.
Learn to code, learn to create, and be a maker!",https://miro.medium.com/v2/resize:fit:623/1*so39vur6sBsT4kxRjWG_hw.png,"Robot Journalism, Artificial Intelligence, Journalism, Makers Institute, Machine Learning"
https://medium.com/s/story/alan-turing-pahlawan-it-di-perang-dunia-ii-a99dd79a8fa8,ALAN TURING: PAHLAWAN IT DI PERANG DUNIA II,Chikita Dinda,2018-01-18T10:16:02.692Z,"Alesan aku pengen nulis artikel ini sebenarnya muncul saat berlangsung kelas mata kuliah Sistem Informasi Manajemen semester lalu. Ketika Bapak dosen nanya ke temen-temen, “Ada yang tahu Alan Turing?” dan dari empat-puluh-mahasiswa yang ada di dalam kelas (seingetku) cuma dua orang yang jawab tahu. Yaitu aku sama Gilar, yang kutahu memang doi banyak banget pengetahuan umumnya.
Bukannya apa-apa, tapi kurasa tingkat kepedulian generasi muda terhadap sejarah emang kurang sih, yang aku tahu lho ya. Anak muda cenderung males untuk baca biografi seseorang karena dianggep ngebosenin.
Padahal, lebih dari itu, aku menyadari bahwa dari biografi dan cerita pengalaman yang pernah dialami seseorang, bagaimana dia menghadapi situasi yang dialami, apa keputusan yang dia ambil, dapat jadi pelajaran supaya kita nggak terjebak dalam keputusan yang sama (atau setidaknya bisa buat pembelajaran).
Kalian tahu nggak kenapa Soekarno bisa melakukan Revolusi?Ya karena dia berani. Dan dia banyak baca buku –mangkanya dia cerdas. Satu lagi: dia suka sekali belajar sejarah.
Nangkep yang aku maksud nggak?
Gini,Misal si A pernah ngalamin ditinggalin gebetan sebelum pacaran. Usut-punya-usut, ternyata gebetannya ternyata nggak suka cowok yang bertele-tele, nggakto the point. Nah cowok lain bisa belajar nih, kalo suatu saat mau ngedeketin cewek itu, doi jadi mengurangi probabilitas untuk salah karna udah tau dari pengalaman si A.Itulah sebenarnya ilmu dasar kita menjadi seorang leader yang memang butuh untuk mengambil keputusan serta ‘meramal’ masa depan. Aku inget dosen ganteng idolaku, Pak Ari Darmawan pernah ngomong gitu di kelas. HAHAHA
Balik lagi ke Alan Turing.Mungkin bagi kalian yang tertarik sama kriptografi dan tema detektif, agen rahasia, hacking atau yang suka main kode-kodean sama gebetan, akan tertarik sama kisah hidup dia. Dan bagi yang lebih suka visual, akan sangat senang karena ceritanya di abadikan dalam sebuah film berjudul “The Imitation Game”. Apalagi yang berperan jadi Alan Turing siBenedict Cumberbatch,idola aku:”).
Film adaptasi sejarah Perang Dunia II yang dapet 7 nominasi dan 1 piala Oscar ini mengisahkan sepenggal kehidupan Alan Turing, seorang pahlawan Perang Dunia yang namanya sempat disamarkan dalam sejarah, sekaligus bapak daricomputer sciencedanartificial intelligencealias AI.
Di tulisan ini aku sebisa mungkin mau nyeritain siapa si Alan Turing ini dan bisa menjadi salah satu pengetahuan baru buat kalian semua. Terlepas dari itu, aku sadar bahwa nggak mungkin untuk merangkum seluruh kehidupan seseorang, apalagi seorang Alan Turing, hanya dalam sebuah artikel. Untuk itu, aku harap kalian yang baca maklum kalo ada banyak kisah yang nggak sempat diceritain/kelewatan.Di tulisan ini, tujuan aku sebenarnya adalah untuk mengenalkan kembali sosok Alan Turing di kalangan para muda-mudi, temen-temen sekalian, agar dia bisa kembali jadi tokoh yang menginspirasi kita dalam kehidupan kita sebagai pribadi, sebagai intelektual muda, dan sebagai calon penggerak roda negeri ini di masa depan. Terlepas dari kontroversi yang ada dalam hidupnya.
Alan Mathison Turing, atau biasa dikenal Alan Turing, dilahirkan di London pada tanggal 23 Juni 1912. Ia merupakan anak kedua dari pasangan Julius Mathison Turing dan Ethel Sara Stoney, dan memiliki seorang kakak laki-laki bernama John. F Turing.Ayahnya bekerja sebagai pegawai pelayanan sipil di India ketika India masih dalam penjajahan Inggris. Ibunya sendiri merupakan anak dari pegawai kereta api. Ketika mengandung Alan Turing, orang tuanya menghendaki agar Alan Turing lahir di Inggris oleh karena itu mereka kemudian kembali ke Inggris.Saat Turing berusia enam tahun, dia masuk ke sekolah dasar St. Michael dan di sekolah itu dia mulai menunjukan kejeniusannya dan membuat kepala sekolahnya kagum. Bahkan, saat usia remaja, ia sudah banyak membaca karya-karya dari Albert Einstein dan akhirnya memutuskan minatnya dibidang Matematika dan Sains.Tahun 1926, Turing kemudian masuk di Sherbone School. Dia memasuki form 6 di Sherborne dan berteman baik sama Christopher Morcom, anak berbakat lainnya yang juga menyukai Matematika dan Sains. Di filmnya, dikisahin si Alan memang ada-sedikit-hubungan-lain sama si Christopher ini. Mungkin semacambromancetapi kelewat batas. Karna si Alan kelihatan sangatintrovertdan emang nggak punya temen selain si Christopher ini.Tapi sedihnya, waktu mereka berdua mendaftar ke Universitas, Christopher tiba-tiba meninggal dunia. Si Alan galau banget karena emang Christopher satu-satunya temen yang dia punya. Sangking deketnya mereka berdua, mereka sampe punya sandi rahasia sendiri kalo ngobrol. Mereka sering surat-suratan via kertas pas lagi kelas dengan sandi yang mereka punya itu. Jadi kadang pas ketauan guru dia lagi ngobrolin apa, nggak bakal kena marah dan dihukum.
Lulus dariSherbone School, Alan Turing kemudian melanjutkan pendidikannya dengan kuliah diCambridge Universitydi jurusan Matematika dimana ia terpengaruh tulisan Von Neumann, Russell dan Whitehead mengenai Matematika dan Sains. Alan menjadi sangat tertarik dengan karya Albert Einstein dan mampu mengembangkan pekerjaan yang dilakukan oleh Einstein berdasarkan sebuah pertanyaan tentang Hukum Gerak Newton.Dan di tahun 1930, Alan Turing berhasil membuat sebuah mesin bernama ‘Turing Machine’ atau Mesin Turing yaitu sebuah mesin yang dapat menjalankan sekumpulan perintah sederhana yang kemudian menjadi cikal bakal dari komputer modern saat ini.Dikutip dari bukuThe History of Psychologykarya George Boerre, mesin buatan Turing ini merupakan sebuah alat yang mampu mengubah dan memanipulasibasicsimbol abstrak. Mesinnya ini dalemnya ada pita yang bisa buat baca dan nulis simbol di pita mesinnya, dan juga pitanya bisa dijadiin tempat penyimpanan. Tapi, lebih dari itu, Mesin Turing ini nggak terbatas pada operasipushdanpopketika mengakses media penyimpanannya, dia juga nyiptain konsep-konsep ilmu komputer walaupun ketika itu masih belum dikenal.Di tahun 1934, Alan Turing lulus dariCambridge Universitydan berkat kecerdasan dan juga prestasinya, ia kemudian dapet beasiswa dari Universitas Princeton di Amerika Serikat untuk menempuh gelar doktornnya dan berhasil menyelesaikannya di tahun 1936.Setelah selesai menempuh pendidikannya, Alan Turing kemudian kembali ke Inggris dan bekerja di Departemen Komunikasi Britania Raya. Ketika Perang Dunia II berkecamuk tahun 1939 hingga 1945, ia ditugaskan untuk memecahkan informasi yang terdapat di mesinEnigma(Mesin Enkripsi) buatan Nazi Jerman.Nggak butuh waktu lama, Turing berhasil memecahkan kode-kode yang ada dalam mesin Enigma dengan membuat sebuah mesin pemecah kode (code breaker) Enigma yang diberi nama ‘The Bombe’ yang di ciptakan pada tahun 1939. Di film-nya mesinnya dinamain ‘Cristopher’. Mungkin biar ada sedikit sentuhan dramanya kali ya. Mesin inilah yang kemudian jadi pioner awal menuju era komputer digital.Adanya mesin pemecah kode yang Turing buat ini sangat berdampak bagi berlangsungnya Perang Dunia II.The Bombememainkan peran penting dalam menguraikan pesan yang dienkripsi oleh mesin Enigma Jerman. Mesin ini berhasil menterjemahkan pesan berisi data intelijen penting bagi Sekutu sehingga dapat mengetahui pesan rahasia yang diberikan setiap hari. Berkat temuannya, Turing berhasil memperpendek usia Perang Dunia II selama dua tahun dan menyelamatkan lebih banyak nyawa dari siapa pun di Perang Dunia II ini.Jasanya yang sangat besar bagi negaranya membuat ia kemudian dianugerahi OBE sebagai pahlawan perang.Usai perang dunia, Turing kemudian menerbitkan sebuah paper ilmiah yang berjudul ‘Computing Machinery and Intelligence’. Dalam tulisannya itu dia mengajukan sebuah metode apakah sebuah mesin juga memiliki ‘Artificial Intelligence’ layaknya otak manusia yang kemudian usulannya tersebut dikenal dengan nama ‘Tes Turing’.Selama hidupnya, Turing berusaha keras agar mesin ciptaannya yaitu Mesin Turing dapat menjadi mesin komputer otomatis diNational Physical Laboratorynamun sayangnya hal tersebut nggak pernah tercapai. Ia kemudian pindah keUniversity of Manchesterdan kemudian disana ia membuat panduan untuk mesin komputer otomatis MADAM (Manchester Automatic Digital Machine).
Kemudian di 1954, Alan Turing mendapatkan masalah. Ia ditangkap polisi setempat dengan tuduhan melakukan hubungan seksual dengan seorang pemuda dimana hal tersebut dilarang di Inggris. Turing kemudian dihukum dengan perawatan kimia -yang artinya mengebirinya secara kimiawi, dengan disuntik estrogen dengan tujuan menetralisasi hormonnya sebagai alternatif pemenjaraan. Kala itu diyakini hormon wanita sanggup menekan nafsu homoseksual. Tapi hormon yg sama juga menumbuhkan ciri-ciri fisik wanita sehingga ia jadi stres berat.Akibat peristiwa tersebut, reputasinya menjadi hancur. Ia kemudian kehilangan pekerjaannya setelah banyaknya tekanan dari publik sebab diketahui sebagai seorang homoseksual. Tekanan yang terus menerus dari publik membuat Turing kemudian memutuskan untuk bunuh diri dengan memakan apel yang mengandung racun sianida di rumah yang berada di wilayah Wislow, Inggris pada tanggal 7 juni 1954.Kelakuannya sebagai homoseksual membuat kerajaan Inggris tidak memberikan pengampunan meskipun reputasinya sebagai pahlawan perang dan seorang ilmuwan yang terkenal di Inggris. Baru pada tahun 2013,Ratu Elizabeth mengampuni dirinya setelah kematiannya 60 tahun yang lalu. Terlepas dari prilaku Alan Turing, sampai saat ini ia dikenal sebagai tokoh penting dalam perkembangan komputer digital dan ilmu komputer sehingga ia dijuluki sebagai ‘Bapak Ilmu Komputer’.
“We can see a short distance ahead, but we can see plenty there that needs to be done.” -Alan Turing.
— — — — — — — — — — — — — — — — -Itulah sepenggal kisah tentang Alan Turing. Semoga kehidupan dan perjuangannya bisa menjadi sumber inspirasi bagi kalian semua. Bagi aku pribadi, terlepas dari dia yang memiliki kelainan seksual, Alan Turing adalah sosok teladan yang paling unik. Denganpassionyang dia punya, ketertarikan yang mendalam tentang matematika dan sains, bisa menjadikan dia pahlawan perang dengan latar belakang yang dia miliki. Dan aku juga belajar untuk lebih objektif dalam menilai seseorang, bukan hanya menilai dari satu sisi saja, tapi harus juga menjamah sisi lainnya yang bisa jadi kita mendapat banyak pembelajaran lebih banyak.Last but not least, semoga semakin banyak anak muda Indonesia yang tahu, dan bisa mengambil pelajaran dari kisah hidupnya yang menarik!
Bachelor of Business Administration (UB — 2015); Islamic Economy and Halal Industry (UGM — 2021) | 🌏Travel learner, 📌 Fake graph designer, 📝Dream writer",https://miro.medium.com/v2/resize:fit:1200/1*8ySF1fvAcaS66YO6i1pQGw.jpeg,"World War II, Artificial Intelligence, Biografia, Alan Turing, Biography"
https://medium.com/s/story/belajar-r-programming-dari-mana-ac75e4550bf5,Belajar R Programming Dari Mana?,AC,2017-12-05T01:11:59.627Z,"Yup, itulah salah 1 pertanyaan yang sering gue temui ketika orang yang mau belajar data science pakai bahasa R. bagi sebagian orang yang tidak dapat mata kuliah (assume she/he is not engineering major) pemrograman akan bingung… apa yang harus dilakukan dengan console-nya atau script R, dan sejujurnya gue juga tidak dapat bahasa R sewaktu kuliah tapi dapat mata kuliah bahasa pemrograman. menurut gue semua bahasa itu sama aja, cuma sytaxnya doang yang beda, jadi cepet buat adaptasi dengan bahasa baru, kalo lu udah sering gonta-ganti bahasa pasti akan menemukan sense programming language, ujung ujungnya cuma bilang “pemrograman kan gitu gitu doang, nothing special” ya… itu juga sama yang terjadi sama gue…
tanpa banyak basa basi busuk… let’s dive in.
make sure gengs, R nya di install dulu, gimana mau belajar R kalo R-nya belom diinstall :D setelah itu baru install R Studio. inihttps://cran.r-project.org/danhttps://www.rstudio.com/
kalo sudah diinstall, buka deh R Studionya.
di R Studio ini “ngoding” nya lebih enak ketimbang pake R Console. jadi ketimbang langsung ngetik di Console mending di Script aja, bisa dibawa pulang :D eh masuknya bisa di save jadi bisa dipakai lagi sewaktu waktu butuh.
As you know R ditujukan untuk statistik. jadi hal hal operasi aritmatika bisa lah ya… untuk hal basic ada hal yang perlu kalian ingat jadi kalau hal hal fundamental sudah terbiasa bikin model dan visualisasi segala macem jadi gampang, hal yang mendasar yaitu:
As it’s name. operasi dasar seperti tambah, kurang, kali, bagi, sqrt, mod,
Variabel adalah tempat untuk menyimpan sebuah value. X <- 10 itu artinya X mengandung nilai 10.
Tipe data di R ada numeric, character, Dates, Logical (Boolean)
Vector adalah sekumpulan elemen untuk tipe data yang sama misalkan:
itu artinya variabel V mengandung nilai 1,2,3,4,5,6
Kita dapat membuat fungsi sendiri di R. Fungsi ini memudahkan kita jika memiliki serangkaian proses dan mengembalikan nilai.
In short, data frame ini kayak table di excel dia ada nama kolom diikuti dengan isi dibawahnya.
ada yang bilang sih list seperti container yang bisa memuat tipe data yang berbeda. misalkan ada index A,B,C di list opearasi. nah index A di list operasi ini bisa berisikan sekumpulan angka, lalu di index B bisa berupa data frame, index yang terakhir bisa di isi matrix.
Matrix, seperti yang dipelajari waktu SMA. matrix berisi index colomn dan row, mirip mirip data frame tapi representasinya dan rules nya yang beda.
ini nih yang jarang dibahas orang, karena agak mikir keras kalo mau pake. kalo matrix itu 2 dimensi, array ini 3 dimensi.
ini nih yang lumayan penting, sering dipakai karena nilai input yang bervariasi. IF x > 8 then print(“good”) ELSE print(“So so”)
ya sekian pejelasan singkat hal hal yang harus dikuasai terlebih dahulu, source codenya banyak bertebaran dimana mana, dan setiap kali ada orang yang bertanya tentang hal ini, gue selalu menjawab hal yang sama, point point di atas harus dikuasai terlebih dahulu, nantinya akan mudah untuk programming yang automate buat bikin model dan visualisasi segala macamnya.",https://miro.medium.com/v2/resize:fit:746/1*uNGn-08LycUMTk_zhYNVNQ.png,"R, Beginner, Programming, Data Science, R Programming"
https://medium.com/s/story/transformasi-industri-asuransi-dengan-artificial-intelligence-ai-acf5e8bed34f,Transformasi Industri Asuransi dengan “Artificial Intelligence” (AI),Kupukoo - World's first P2P Smartphone Insurance,2018-06-18T06:20:05.838Z,"Jasa keuangan dan telekomunikasi ini cukup unik oleh karena produk yang mereka jual ada di dalam sistem computer namun bukan merupakan objek yang terlihat secara nyata (benda fisik). Sejak tahun 1970 perusahaan ini telah memanfaatkan teknologi secara intensif, tidak hanya untuk menangani kegiatan administrasi kantor seperti gaji karyawan namun juga kegiatan bisnis inti (operasional).
Industri asuransi, seperti contohnya, yang menggunakan perangkat lunak untuk membuat kebijakan/polis baru, mengelola klaim, melakukan proses perhitungan ‘underwriting’ dan aktuaria atau dengan kata lain mendukung semua proses pekerjaan dalam perusahaan asuransi. Namun dengan munculnya AI (Artificial Intelligence/Kecerdasan buatan) dan Machine Learning (Mesin Pembelajaran) semua hal tersebut kini telah berubah, hal menjadi kuncinya ialah kalimat sebelumya yakni “memproses” menjadi — memproses segala proses” menjadi “mengirimkan” segala atau semua proses menjadi — memberikan semua proses.
Dan perubahan ini tidak akan terjadi dalam waktu cukup dekat. Perusahaan asuransi tidak menjadi pilihan pelanggan yang terbaik apabila berhubungan dengan “kepuasan pelanggan”, sehingga hasil dari survei menunjukkan jika perusahaan asuransi berada diurutan terbawah:
Sektor dan persentase pelanggan yang memiliki layanan pelanggan terbaik:
1. Eceran — 38%2. Hotel / perhotelan — 37%3. Layanan makanan / restoran — 35%4. Teknologi — 24%5. Kenyamanan — 24%6. Perbankan — 23%7. Perjalanan udara — 20%8. Pelayanan publik — 18%9. Broadband / media — 17%10. Ponsel — 15%11. Angkutan umum — 14%12. Utilitas — 12%13. Otomotif -12%
Membaca melalui komentar di situs ulasan pelanggan, umpan balik negatif sangat luas dan cukup bervariasi namun ada dua tema umum yang muncul:
Waktu layanan pelanggan dan respons yang buruk”Dalam 12 bulan terakhir saya belum bisa berbicara dengan seseorang di departemen klaim tanpa ditahan selama 20 menit plus”.
Penanganan Klaim yang buruk”Penanganan klaim benar-benar suram sejak awal. Saya akan dengan senang hati membayar lebih banyak tahun depan untuk menghindari orang-orang ini “.
Tanyakanlah kepada perusahaan asuransi seberapa penting pelayanan pelanggan dan mereka akan meletakkan keberadaan pelayanan pelanggan (customer service) berada pada prioritas pertama atau kedua di perusahaan; hal ini telah menjadi kasus yang cukup menarik sejak tahun 1980an, jika mereka meletakkan pelayanan pelanggan menjadi prioritas utama mereka namun mengapa mereka mendapatkan hasil nilai kepuasan pelanggan yang terburuk diatara semuanya?
Jawadannnya sangatlah mudah, hal ini karena secara garis besar semua proses dalam perusahaan asuransi dilakukan oleh orang-orang yang dibantu oleh informasi yang disediakan oleh sistem computer.
Oleh sebab itu dengan perangkat AI dan Machine Learning hal ini akan memungkinkan jawaban dari setiap pertanyaan dan proses klaim, respon yang diberikan akan lebih cepat, pelanggan akan medapatkan jawaban dari setiap pertanyaan dalam hitungan detik. Proses dalam melakukan klaim akan konsisten dan saat pelanggan tidak menyukai jawaban atas pertanyaan mereka maka system akan mengeluarkan aturan yang sebelumnya telah dijelaskan sehingga pelanggan akan dengan cepat mendapatkan solusi atas apa pilihan yang telah mereka buat sebelum sampai pada pengambilan kesimpulan.
Teknologi asuransi terbaru, seperiKupukoo,World’s First Peer-to-Peer smartphone insurance, dengan menggunakan teknologi AI dan Machine Learning telah menawarkan suatu revolusi di bidang industry asuransi dan pada akhirnya akan memberikan pemecahan masalah terhadap permasalahan mengenai system pelayanan pelanggan yang menjadi momok dunia industry selama 40 tahun terakhir — Semoga semuanya akan berjalan seperti itu!
https://www.theactuary.com/news/2015/09/insurance-sector-bottom-of-best-customer-service-league-table/
Originally published atwww.kompasiana.com.",,"Artificial Intelligence, Kupukoo, AI, Insurance, Insurtech"
https://medium.com/s/story/klasifikasi-kanker-payudara-dengan-metode-random-forest-dan-support-vector-machine-menggunakan-r-ad15adc23e52,"Menggunakan metode Random Forest dan Support Vector Machine di R, Studi Kasus Klasifikasi Kanker Payudara",Hafizhan Aliady Afif,2018-04-06T09:55:21.647Z,"Well Asalamualaikum People,
Sebenernya ini itu makalah penelitian saya untuk tugas Statistika Consulting. jadi ya enjoy the tulisan :)
Sebelum Masuk ke Bagian utama, telebih dahulu melakukan instalasipackage
Pada Bagian ini package e1071 merupakan package SVM dan RandomForest+reprtree untuk machine learning random forest dan menampilkan pohonnya.
Untuk Package Reptree nya bisa di unduh disinikemudian folder di dalammnya di copy ke documents/R/win-library . nanti paste saja folder reptreenya disitu
Lalu bagian ini input data terlebih dahulu, silahkan cocokan dengan tempat menyimpan file dari datacanceryang telah di download.
varaibel yang di gunakan adalah hanya variabel rata-ratanya saja (variabel 1–9)
Di bagian ini berhubung saya anak statistika maka dalam menentukan jumlah sample di gunakan metode slovin yang rumusnya itu
Nah setelah itu di dapatkan n=235
Kemudian membagi data menjadi 2 bagian yaitu data Latih(Train) dan data Uji(Test).
Fungsi set.seed digunakan untuk menentukanseed /random number generator agar data /ke acak ansesuatu bisa di atur sehingga antara percobaan pertama dan seterusnya menggunakan data acak yang sama.
sample disini digunakan untuk mengacak data keberapa yang di gunakan sebagai data latih dimana dari 1 sampai N di ambil sebanyak (235) data secara acak. Kemudian test disini adalah data yang tidak terambil pada pengacakan data train.
kemudian lanjut ke membuat modelnya disini yang ingin di klasifikasikan adalah diagnosisnya dalam diagnosisnya ada 2 kelas yaitu M(Maligant) Kanker ganas dan B(Beningn) Kanker Jinak.
Lalu nama data yang di gunakan adalah data, subset adalah set atau urutan data yang di gunakan dalam train(latih) model. Type klasifikasinya adalah “c-classification”
nah itu adalah model yang di hasilkan daritraining model.kemudian melakukan validasi untuk melihat tingkat akurasi dari data yang di gunakan untuk melatih model tersebut
Pada baris pertama melakukan inisisasi X yang merupakan data latih. kemudian melakukan prediksi menggunakan model “cancer.svm” yang telah di buat sebelummnya lalu membuat tabel akurasi untuk melihat apakah hasil prediksi dan data hasil observasi sesuai. kemudian melakukan perhitungan presentase datanya benar.
Bisa dilihat pada output di atas, pada matriks akurasi di dapatkan bahwa terdapat 143 data benar di prediksi dan 7 data salah prediksi untuk kategori Benign. Dan pada kategori Malignant terdapat 80 data benar prediksi dan 5 data lainnya tidak tepat sehingga di dapatkan tingkat akurasi sebesar 94.8%. walaupun menggunakan data yang di gunakan untuk model ternyata SVM belum mampu 100% memprediksi dengan baik.
Selanjutnya Masuk ke bagian Random Forest. untuk modelnya seperti ini
pada bagian ini model rendom forest memiliki 1000 pohon dan random forest memang di gunakan untuk klasifikasi. sehingga tidak perlu mendefinisikan metode regresi atau klasifikasi yang akan di gunakan.
Untuk hasil otuputnya didapatkan bahwa Tipe random forestnya adalah klasifikasi dan jumlah pohon yang di tumbuhkan ada 1000, kemudian jumlah bariabel yang di coba setiap percabangan ada 3. Lalu ada OOB(out of box) estimate of error rate adalah estimasi error yang di dapatkan jika data di luar dari data pengujian.
Pada Plot di atas dapat dilihat pada tree ke 500 pelatihan model sudah mulai stabil. sehingga model dapat di gunakan untuk klasifikasi secara general pada data yang lain.
Pad plot ini merupakan decision tree dari pohon yang sudah stabil tersebut sehingga menampolkan pohon sepeerti gambar di atas
Selanjutnya bagian validasi dimana pengujian di lakukan pada data yang di gunakan untuk melatih model
Seperti pada bagian sebelumnya model yang di gunakan adalah model cancer.rf
Dari output di atas di dapatkan bahwa model random forest dapat mengklasifikiasikan dengan baik (dengan mendapatakn 100% akurasi) menggunakan data yang di latih.
Selanjutnya adalah pengujian dari hasil prediksi, shingga dapat mengetahui sejauhmana model dapat memprediksi kategori diagnosis dengan baik.
Data yang di gunakan adalah data yang di luar data latih seingga di gunakan syntax data[-train,] yang berarti menggunakan data yang di kuraing kolom train. lalu sama seperti sebelumnnya mendefinisikan prediksi menggunakan model cancer.svm
Di dapatkan output sebagai berikut pada hasil prediksi menggunakan support vector machine. pada klasifikasi Benign ada 195 data pada klasifikasi yang benar dan 12 nya salah prediksi. lalu pada kelas Maligant terdapat 117 data dengan klasifikasi benar dan 10 data salah klasifikasi. dari hal tersebut di dapatkan akurasi sebesar 93.4%.
Dan di atas merupakan untuk memprediksi menggunakan model random forest.
Pada model random forest di dapatkan hasil prediksi benar pada kategori Benign ada 199 data dan 8 data lainnya salah prediksi. kemdian untuk kelas Maligant ada 118 data pada kategori yang benat dan 9 data lainnya salah prediksi. berdasarkan hal tersebut di dapatkan nilai akurasi keselurhan sebanyak 94.9%
Berdasarkan perbandingan kedua model, metode machine learning random forest memiliki lebih tinggi tingkat akurasi sehingga bisa di katakan lebih baik. namun perbedaan ke dua model pada tingkat akurasinya sangat kecil sehingga bisa dikatakan kedua model sama sama baik.
Untuk selanjutnya sebaiknya gunakan data yang lebih banyak untuk melatih model karena dalam machine learning semakin banyak data yang di gunakan dalam melatih model maka model akan semaikn baik.
Sekian Semoga Bermanfaat.
Jika ada pertanyaan silahkan kontak ke hafizhan.afif@gmail.com
Problem Solver | ML / AI Engineer",https://miro.medium.com/v2/resize:fit:1059/1*zT4m_kdpghtbOlCSRBj9cQ.png,"Random Forest, Machine Learning, Svm, Bahasa Indonesia, Klasifikasi"
https://medium.com/s/story/ubex-implementasi-kecerdasan-buatan-pada-industri-periklanan-review-bahasa-indonesia-aecaab75e14a,UBEX — Implementasi Kecerdasan Buatan pada Industri Periklanan (Review Bahasa Indonesia),Praina,2018-07-16T17:39:02.089Z,"“Industri Periklanan”, yaa, Indusri yang menjadi perbincangan hangat dalam beberapa tahun belakangan ini menjadi salah satu indusri yang sangat flesibel, mudah dan tentunya sangat menguntungkan. bagaimana tidak, dalam perkembangan dunia yang mengarah pada dunia dengan teknologi digital, sangatlah mudah bagi kita untuk melakukan periklanan maupun promosi dan meraup rupiah. Contohnya seperti di Sosial Media dengan Endorse, di Google dengan AddWords, VideoTron, dan masih banyak lagi.
Dikutip dari Wikipedia, Periklanan merupakan bentuk komunikasi yang digunakan untuk membujuk audiens (pemirsa, pembaca atau pendengar) untuk mengambil beberapa tindakan sehubungan dengan produk, ide, atau layanan. Tujuan dari periklanan sendiri adalah mengarahkan perilaku konsument terhadap suatu penawaran komersial ataupun mempersuasi seseorang dalam melakukan sesuatu (seperti iklan politik/layanan masyarakat yang nonkomersial). Sistem pada industi periklanan saat ini, terdiri dari 3 pelaku utama yaitu Users (orang yang melihat iklan), Advertisers (orang/perusahaan yang meminta untuk diiklankan) dan Publishers (orang/perusahaan yang menerbitkan Iklan). Dibalik kemajuan industri periklanan juga terdapat beberapa masalah yang menyertai pengembangannya, beberapa masalah ini akan saya bagi menjadi 3 poin yaitu :
Sebagai pengguna internet, pastinya kita pernah menjumpai iklan yang sangat menganggu sehingga membuat kita tidak bisa nyaman untuk mencari informasi, tentunya hal ini sangat menjengkelkan dan membuang waktu kita.
Tidak jarang bagi kita, ketika membeli barang mendapat harga yang berkali kali lipat dengan harga aslinya, hal ini biasanya dikarenakan barang yang kita beli bukan berasal langsung dari pihak perusahaan, tapi berasal dari beberapa orang yang sebelumnya membeli barang tersebut dengan jumlah banyak, kemudian barang tersebut dijual lagi dengan harga yang lebih tinggi.
Banyaknya Penipuan, Iklan yang provokatif dan koersif
Biasanya advertisers dipaksa membayar iklan dengan jumlah klik, jumlah iklan yang ditampilkan tanpa jaminan penjualan yang aktual.
Tidak jarang advertisers kesulitan mencari publisehers yang benar benar bagus, dan dapat diandalkan.
Hal ini biasa terjadi pada beberapa agensi periklanan, yang menerbitkan iklan setelah angaran habis.
Sama dengan masalah yang di hadapi advertisers, terkadang sangat sulit bagi publisher untuk mencari pengiklan dan mengisi semua slot iklan.
Biasanya publisher dibayar murah karna tidak mampu untuk memperkirakan pembayaran.
Tidak jarang pihak publisher mendapat penundaan pembayaran bahkan tidak dibayar dari jasa yang telah mereka lakukan.
Berdasarkan masalah diatas, UBEX hadir dengan ide terbaru yaitu menciptakan sebuah platform untuk pasar periklanan, Ubex diyakini dapat memecahkan masalah pada industri ini melalui penggunaan neural network dan teknologi blockchain.
Pengembangan Platform Ubex bertujuan untuk membuat sistem baru untuk masa depan pada dunia periklanan, Ubex menciptakan pertukaran iklan dengan sistem yang terdesentralisasi secara global di mana perusahaan dapat beriklan secara efektif, dan penerbit mendapat keuntungan dari iklan yang mereka tampilkan.Misi Utama dari UBEX adalah menciptakan ekosistem periklanan global dengan tingkat kepercayaan timbal balik yang tinggi serta efisiensi yang maksimum. Misi tersebut akan saya jabarkan menjadi 4 poin yaitu :1. Membuat sebuah website yang dapat digunakan oleh Advertiser dan publisher berinteraksi.Ubex akan menggunakan Neural Network dan Sistem akumulasi data, sehingga kita dapat memilih konten iklan yang paling relevan pada waktu tertentu dari pengguna tertentu, sehingga memaksimalkan kemungkinan mendapat hasil yang sesuai.2. Secara efektif dapat mengurangi biaya dan menghapus perantara.Menghilangkan interaksi tidak langsung antara pengiklan dan penerbit dengan cara menerapkan sistem smart contract untuk semua jenis pelacakan, sehingga dapat meningkatkan efisiensi ekonomi dalam berkerja sama.3. Membuat peluang bagi publisher untuk mengontentasikan dan memaksimalkan slot iklan.Menggunakan semua statistik yang tersedia dari situs web dan peserta iklan dalam platform exchange ini, serta memberikan perkiraan potensi penghasilan dan memberi peluang untuk mengumpulkan terlebih dahulu jika reputasi cukup tinggi.4. Mengembangkan Sistem multi-layered berdasarkan rating reputasi pada setiap peserta platform.Menerapkan sistem pemberian peringkan pada advertisers dan publisher, sistem ini akan disimpan dalam blockchain dan juga akan menampilkan history kerja yang pernah dilakukan, hal ini dilakukan untuk mencegah penipuan dan untuk memungkinkan mekanisme menangani mitra yang paling dapat dipercaya.
Pengunaan teknologi blockchain tidak lantas tanpa sebab, teknologi ini memungkinkan ubex untuk menerapkan model sistem (Pay-Per-Result.) pembayaran berdasarkan hasil. Jadi semua peserta(Publisher dan Advertises) dapat secara maksimal bekerja tanpa takut pada masalah yang sebelumnya menyertai indusri periklanan.Anda diperkenankan untuk mencoba prototipe dari UBEX pada Link ini :https://app.ubex.com/home
Dapat digambarkan sebagai berikut, “jika platform UBEX adalah mobilnya, token ubex adalah bahan bakarnya”, Advertisers pada Platform Ubex akan menggunakan token Ubex untuk dari saldo akun mereka untuk menerapkan sistem smart contract yang sudah transaksi pada platform ini. Saat ini telah berlangsung penjualan token ubex dengan bonus sebanyak 6% yang akan berkahir dalam 5 hari kedepan. Token ini dapat anda beli menggunakan Coin Ethereum, Bitcoin dan Litecoin. Berikut saya sertakan gambar rincian dari penjualan token UBEX.
Token ubex saat ini telah dibeli sebanyak 17,159 ETH, atau 71% dari total alokasi dari token yang dijual. Token ubex tidak dapat dibeli oleh warga USE, IRAN dan KOREA UTARA. anda dapat mendownload dokumen token ekonomi pada mengeklik link ini :Token Ekonomi Ubex
Ubex sendiri telah direview oleh banyak website ICO rating dan mendapat hasil yang sangat memuaskan, dan dapat dipastikan investasi pada ubex mempunyai risiko yang rendah.
Tim dari Ubex disusun berdasarkan kemampuan individu dan pengalaman yang sangat luas dalam berbagai bidang, berikut adalah susunan dari team ubex.
Penasihat Ubex memiliki segudang pengalaman, dan akan selalu dapat diandalkan dan terpercaya, berikut adalah daftar dari penasihat Ubex :
Sekian Review dari saya Puguh Bahtiar, untuk mengetahui lebih lanjut tentang Ubex silahkan kunjungi link dibawah ini :
Contact :support@ubex.com
Partners:partners@ubex.com
Press:press@ubex.com
Profil Link Bitcointalk Penulis
Curently Studying Informatics technology in brawijaya university Terlalu Pagi",https://miro.medium.com/v2/resize:fit:1200/0*CiwgyNBRFfiXxTpa.jpg,"Cryptocurrency, Advertising, Artificial Intelligence, Tokensale, Blockchain"
https://medium.com/s/story/mencicipi-a-i-dari-ibm-watson-af09cfef9b6b,Mencicipi A.I. dari IBM Watson,Equan P.,2018-03-17T06:32:48.236Z,"Insight & Tutorials in Bahasa Indonesia
Sepertinya bagi saya pribadi sebagai pengembang akan selalu berada di jaman emas dimana saat ini bertebaran berbagai potensi teknologi sepertiartificial intelligence, visualisasi data 3D, konektifitas internet di berbagai sudut,digitalisasi historis(Big Data), penggunaan teknologi terdistribusi seperti Blockchain ataupun sangat mudahnya membikin dan memilih perangkat keras secara kustom dll.
Seperti halnya Blockchain yang sudah mulai merambah di berbagai sudut penggunaan, A.I pun sudah banyak tersedia terutama sebagai layanan kelas enterprise oleh perusahaan-perusahaan besar sepertiAmazon,Microsoftdengan Azure Cognitive dan IBM misalnya dengan layanan sepertiIBM Watson
Biasanya dengan kemudahan harus ada kompensasi yang harus dibayar dan dalam konteks ini memang layanan-layanan perusahaan besar tersebut tidaklah bersifat gratis dan sudah pasti anda harus merogoh kocek yang dalam kalau mau bener-bener terjun dalam bisnis yang memakai layanan-layanan berdasarkan A.I tersebut akan tetapi sebagai pengembang anda masih diberi kesempatan untukmencicipinya.
Kenapa harus memilih layanan A.I yang satu ini?
ya itulah jawabannya,personallykeputusan tidak dari segi bisnis endorsement maupun affiliate tetapi baru-baru ini secara resmi IBM merilis Unity SDK untuk layanan A.I mereka yang bernama IBM Watson.
Ada banyak layanan yang bisa dipakai dari IBM Watson ini dan yang menarik untuk ditelusuri yaituSpeech to TextdanVisual Recognition(meskipun sebenarnya sangat bergantung kebutuhan kalau anda ingin memakai layanan yang lain).
Sayangnya untuk versi gratis anda tidak bisa memakai layanan lebih dari satu instance secara bersamaan untuk satu layanan tetapi jika anda ingin mencobanya model payment adalah pay-as-you-go seperti Amazon.
Untuk pengembangan saya rasa cukuplah untuk mengetahui bagaimana cara kerja layanan A.I secara umum.
Dalam setiap layanan tersebut anda akan di berikancredentialsyang kemudian akan anda gunakan untuk mengakses layanan-layanan tersebut melalui protokol REST (kalau anda developer web pasti sudah sangat akrab, untuk yang lain silahkan pakai mesin pencari kesayangan anda)
UntukSpeech to Textanda akan diberikanusername, password dan URLsedangkan untukVisual Recognitionanda akan memperolehAPI KeydanURL
Kalau anda meng-importaset SDK IBM Watson dari Unity Store dan mencoba contohSpeech to Textyang berada di sceneExampleStreaming,
Jangan lupa untuk mengedit credentials di berkas ExampleStreaming.cs sesuai dengan credentials yang diberikan layanan yang anda buat di IBM Watson
Kemudian anda menjalankan scene tersebut maka secara otomatis akan mendeteksi Microphone yang anda pakai dan akan mulai mendengarkan suara yang akan dirubah ke teks oleh layanan IBM Watson.
atau kalau anda ingin mengetest langsung tanpa Unity 3D bisa dilakukan melalui tool seperticurl,hanya saja anda harus menyediakan berkas rekaman audio karena tool ini setau saya tidak bisa mengambil audio langsung dari Microphone.Lihat tutorial resmi Speech to Text.
Demikian juga dengan layananVisual Recognitionanda bisa mencobanya malalui sceneServiceExamples
Jangan lupa untuk meng-enable komponenExampleVisualRecognitiondan mengedit berkasExampleVisualRecognition.cssesuai dengan API Key dan URL dari IBM Watson.
Satu nilai lebihVirtual RecognitiondariSpeech to Textini adalah adanyaTraining Tool, artinya anda bisa mentraining A.I dengan data-data gambar sehingga nantinya A.I tersebut akan bisa mengenali gambar sesuai dengan klasifikasi yang anda harapkan, kalau anda penasaran apa yang saya maksudkan contoh yang saya buat sebagai berikut ini mungkin bisa menjadi bayangan.
Karena layanan ini menyediakan akses dengan protokol REST artinya banyak sekali teknologi klien yang bisa didukung seperti Node.js, Python dll.
Saya yakin kalau dunia pengembangan game danA.I yang sebenarnyaakan semakin dekat karena adanya dukungan layanan cloud dari perusahaan besar dan keluarnya berbagai macamdevice chipyang ter-embed oleh algoritma A.I secarahardwaresehingga pemakaian A.I akan menjadi seperti sebuah norma baru dalam pengembangan sebuah game maupun perangkat lunak.
Menurut saya poin penting jika anda ingin membuat layanan A.I entah untuk pribadi ataupun komersil yaitu perlunya kemudahan akses (protokol).
Tentang SayaProgramer yang tidak terlalu bertendensi ke bahasa pemrograman tertentu, meskipun saya termasuk fans berat dari Node.js tetapi selama problem terselesaikan ok saja kalau harus pakai teknologi lain —Github,Twitter
Insight & Tutorials in Bahasa Indonesia",https://miro.medium.com/v2/resize:fit:1200/0*8yYKjd6BDTua_F9O.,"Ibm Watson, Artificial Intelligence, Speech Recognition, Unity3d, Programming"
https://medium.com/s/story/pengenalan-deep-learning-part-7-convolutional-neural-network-cnn-b003b477dc94,Pengenalan Deep Learning Part 7 : Convolutional Neural Network (CNN),Samuel Sena,2018-03-19T15:13:49.295Z,"Padapart-5kita sudah membahas tentang penggunaan MLP untuk melakukan klasifikasi dengan hasil yang cukup baik. Sebelum kita membahas lebih lanjut tentang CNN, kita akan melihat kelemahan dari MLP jika digunakan untuk data image dan kenapa kita membutuhkan CNN.
Data MNIST diatas terdiri dari nilai 0–255 untuk setiap pixel yang ada. Kita bisa saja menggunakan MLP untuk melakukan klasifikasi untuk semua digit dengan hasil yang cukup baik karena sebagian besar data pada MNIST, object yang akan dikenali berada ditengah-tengah gambar.
Lalu bagaima jika object yang akan dikenali tidak berada ditengah-tengah gambar? Disinilah kelemahan dari MLP. Angka 6 yang berada ditengah-tengah gambar akan berhasil dikenali, tetapi angka 6 yang berada dipojok kiri mungkin tidak akan dikenali.
Kita bisa menggunakan data yang sangat banyak dengan tiap digit berada pada lokasi yang berbeda, namun ini bukan cara yang efisien untuk mengatasi permasalahan tersebut.
Convolutional Neural Network (CNN) adalah salah satu jenis neural network yang biasa digunakan pada data image. CNN bisa digunakan untuk mendeteksi dan mengenali object pada sebuah image.
Secara garis besar CNN tidak jauh beda dengan neural network biasanya. CNN terdiri dari neuron yang memiliki weight, bias dan activation function seperti yang sudah kita pelajari pada part sebelumnya.
Lalu apa yang membedakan? Arsitektur dari CNN dibagi menjadi 2 bagian besar,Feature Extraction Layer (istilah saya sendiri :D) dan Fully-Connected Layer (MLP).
Saya gunakan istilah ini karena proses yang terjadi pada bagian ini adalah melakukan “encoding” dari sebuah image menjadi features yang berupa angka-angka yang merepresentasikan image tersebut (Feature Extraction).
Feature extraction layer terdiri dari dua bagian. Convolutional Layer dan Pooling Layer. Namun kadang ada beberapa riset/paper yang tidak menggunakan pooling.
Gambar diatas adalah RGB (Red, Green, Blue) image berukuran 32x32 pixels yang sebenarnya adalah multidimensional array dengan ukuran 32x32x3 (3 adalah jumlah channel).
Convolutional layer terdiri dari neuron yang tersusun sedemikian rupa sehingga membentuk sebuah filter dengan panjang dan tinggi (pixels). Sebagai contoh, layer pertama pada feature extraction layer biasanya adalah conv. layer dengan ukuran 5x5x3. Panjang 5 pixels, tinggi 5 pixels dan tebal/jumlah 3 buah sesuai dengan channel dari image tersebut.
Ketiga filter ini akan digeser keseluruh bagian dari gambar. Setiap pergeseran akan dilakukan operasi “dot” antara input dan nilai dari filter tersebut sehingga menghasilkan sebuah output atau biasa disebut sebagai activation map atau feature map.
Stride adalah parameter yang menentukan berapa jumlah pergeseran filter. Jika nilai stride adalah 1, maka conv. filter akan bergeser sebanyak 1 pixels secara horizontal lalu vertical. Pada ilustrasi diatas, stride yang digunakan adalah 2.
Semakin kecil stride maka akan semakin detail informasi yang kita dapatkan dari sebuah input, namun membutuhkan komputasi yang lebih jika dibandingkan dengan stride yang besar.
Namun perlu diperhatikan bahwa dengan menggunakan stride yang kecil kita tidak selalu akan mendapatkan performa yang bagus.
Sedangkan Padding atau Zero Padding adalah parameter yang menentukan jumlah pixels (berisi nilai 0) yang akan ditambahkan di setiap sisi dari input. Hal ini digunakan dengan tujuan untuk memanipulasi dimensi output dari conv. layer (Feature Map).
Tujuan dari penggunaan padding adalah :
Pada ilustrasi diatas, dimensi dari input sebenarnya adalah 5x5, jika dilakukan convolution dengan filter 3x3 dan stride sebesar 2, maka akan didapatkan feature map dengan ukuran 2x2. Namun jika kita tambahkan zero padding sebanyak 1, maka feature map yang dihasilkan berukuran 3x3 (lebih banyak informasi yang dihasilkan)
Untuk menghitung dimensi dari feature map kita bisa gunakan rumus seperti dibawah ini:
Pooling layer biasanya berada setelah conv. layer. Pada prinsipnya pooling layer terdiri dari sebuah filter dengan ukuran dan stride tertentu yang akan bergeser pada seluruh area feature map.
Pooling yang biasa digunakan adalah Max Pooling dan Average Pooling. Sebagai contoh jika kita menggunakan Max Pooling 2x2 dengan stride 2, maka pada setiap pergeseran filter, nilai maximum pada area 2x2 pixel tersebut yang akan dipilih, sedangkan Average Pooling akan memilih nilai rata-ratanya.
Dimensi output dari Pooling layer juga menggunakan rumus yang sama seperti pada conv. layer
Tujuan dari penggunaan pooling layer adalah mengurangi dimensi dari feature map (downsampling), sehingga mempercepat komputasi karena parameter yang harus diupdate semakin sedikit dan mengatasi overfitting.
Feature map yang dihasilkan dari feature extraction layer masih berbentuk multidimensional array, sehingga kita harus melakukan “flatten” atau reshape feature map menjadi sebuah vector agar bisa kita gunakan sebagai input dari fully-connected layer.
FC Layer yang dimaksud disini adalah MLP yang sudah pernah kita pelajari sama-sama padapart-4danpart-5. FC Layer memiliki beberapa hidden layer, activation function, output layer dan loss function.
Yang penasaran dengan bagaimana backpropagation untuk CNN. Secara garis besar sama saja, yang sedikit berbeda adalah saat backprop pada conv. layer. Yang penasaran mungkin bisa baca lebih lanjut pada artikel dibawah ini
Kali ini kita akan melakukan klasifikasi terhadap data Fashion MNIST. Fashion MNIST ini adalah dataset yang terdiri dari 10 kategori fashion sebagai berikut :
Tiap kategori terdiri dari 6.000 images untuk training dan 1.000 images untuk testing. Jadi total untuk training data ada 60.000 images dan 10.000 untuk testing data.
Tidak jauh berbeda dengan part-part sebelumnya. Tapi kali ini kita akan gunakan layer baru yaitu Conv2D, MaxPooling2D, ZeroPadding2D dan Flatten. Kita juga akan gunakan TensorBoard untuk melakukan visualisasi pada saat training.
Input Fashion MNIST akan di-rescale dari 0–255 menjadi 0–1 seperti yang kita lakukan padapart-6dan melakukan reshape data menjadi 4-D karena requirement dari framework yang kita gunakan adalah seperti itu (batch_size, width, height, channel) => (256, 28, 28, 1).
Target juga akan dirubah menjadi one-hot dengan menggunakan method to_categorical seperti yang sudah kita lakukan padapart-5.
Arsitektur model yang akan kita buat adalah seperti gambar diatas. Feature map yang berhasil di-extract dari input berukuran 3x3 sebanyak 64. Selanjutnya terdapat Flatten layer yang merubah feature map tersebut menjadi 1-D vector yang akan digunakan pada FC Layer.
Kita akan menggunakan TensorBoard untuk melakukan visualisasi pada saat training. Seluruh training loss/accuracy dan validation loss/accuracy akan disimpan dan kita bisa melihat grafiknya.
Dengan ini kita bisa melihat apakah terjadi underfit, overfit dan performa dari model kita. Mungkin nanti akan saya bahas di part terpisah tentang optimasi ini.
Setelah 100 epoch saya mendapatkan loss sebesar 0.2236 untuk training dan 0.3071 untuk validation, sedangkan accuracy yang didapat adalah 0.9186 (91.86%) untuk training dan 0.8891 (88.9%) untuk validation.
Untuk menggunakan TensorBoard kita bisa gunakan command sebagai berikut:
tensorboard — logdir=Graph/
Validation Accuracy 88.9% nanggung banget ya.. Gimana kalau kita ingin mendapatkan val accuracy setidaknya 90%?
Kita bisa gunakan arsitektur yang lebih “deep” dengan menambahkan conv. layer. Tapi yang patut diperhatikan adalah semakin deep arsitektur yang kita gunakan semakin lama proses training karena semakin banyak parameter yang harus diupdate. Arsitektur yang seperti ini juga rawan terjadi overfitting.
Model kedua yang akan kita coba menggunakan ukuran filter yang sama yaitu 5x5 dan 3x3, namun kita gunakan stride yang lebih kecil dan kita melakukan dua kali downsampling.
Perlu diingat bahwa semakin kecil stride maka semakin banyak informasi yang dapat di-extract.
Dengan menggunakan TensorBoard, kita juga bisa membandingkan performa kedua model yang telah kita train.
Grafik warna biru diatas adalah grafik dari model kedua yang menggunakan conv. layer yang lebih banyak. Bisa dilihat disitu kalau performa dari model ini jelas lebih bagus daripada model pertama.
Training dan Validation Loss yang didapatkan adalah 0.1521 dan 0.2571, sedangkan Training dan Validation Accuracy sebesar 94.46% dan 91.28%.
Sebenarnya kedua model ini masih bisa dioptimasi lagi, bisa dicoba pake learning rate yang lebih tinggi misalnya 0.001, tapi epoch lebih kecil lagi atau setting jumlah neuron pada FC Layer dan masih banyak lagi untuk improve performa dari model kita.
Pada part ini kita sudah sama-sama melihat implementasi CNN untuk melakukan klasifikasi pada image grayscale. Lalu bagaimana dengan RGB Image dengan object yang lebih kompleks?
Kita akan bahas itu semua pada part selanjutnya tentang Pre-Trained Model atau Transfer Learning..So Stay Tune Guys…:D
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*CFv-qOlaUZ_PIq8b8kw8WA.jpeg,"Deep Learning, Artificial Intelligence, Neural Networks, Object Recognition, Machine Learning"
https://medium.com/s/story/jaminan-kaamanan-wa-62-852-1630-6730-jual-kawat-duri-dan-kawat-s-b2584b2a8b6f,Jaminan Kaamanan!!! WA+62 852 1630 6730 Jual Kawat Duri Dan Kawat S…,kawat Duri,2018-02-17T05:50:56.056Z,"Anda mencariJual Kawat Duri Dan Kawat Silet Di Kenari, membutuhkan Jaminan Kamanan!!! Hubungi : Tel./WA +62 852 1630 6730.Kawat duri Paling banyak dipakai pada rumah tinggal (untuk mencegah orang jahat memanjat pagar), peternakan (mencegah predator masuk), padang rumput, jalan, atau proyek konstruksi untuk mengamankan property.Kami siap melayani pemesanan skala besar maupun kecir, untuk seluruh wilayah indonsiaUntuk contact follow up lebih bisa melalui :Bapak Adrian Tel./WA 085216306730 (TELKOMSEL)
EMAIL adrianrenovadi@gmail.com
Originally published atkawatsiletdanduri.skyrock.comon February 12, 2018.",,"Management, Marketing, Machine Learning, Marijuana, Marriage"
https://medium.com/s/story/deepfake-algoritma-kontroversial-yang-memasang-wajah-kamu-di-film-b290604841f,Deepfake: Algoritma Kontroversial yang Memasang Wajah Kamu di Film!,Andi muhammad,2018-04-29T13:58:20.112Z,"Forming tech based society
Bagaimana kalau suatu saat kamu bisa memasukkan wajah kamu ke dalam film manapun secara mudah? Bayangkan, jika kamu bisa menonton dirimu sendiri berperan di FilmTitanicbersama Leonardo Dicaprio?
Nah sekarang, kita bisa membuat mimpimu menjadi kenyataan cukup dengan 300–1000 foto kamu, Film Titanic, dan satu algoritmaDeepfake!
Deepfake adalah algoritma asalReddityang dibuat memakaideep learning, yaitu sebuah teknik dalam pengembangan AI.
Misalkan ada 2 orang individu A dan individu B dan kamu ingin memasang wajah dari A kepada sebuah video mengenai si B yang sedang berpidato.
Caranya adalah kamu menumpulkan ratusan hingga ribuan foto si A (idealnya dari berbagai sudut) dan memasukkannya ke sebuahencoderdeepfake.
Deep learningdiencodertersebut akan mengambil data dari ribuan foto wajah si A dan memancarkannya ke video yang mempunyai muka si B di dalamnya, tanpa harus mempunyaiskillmengedit video.
Dengan Deepfake, kegunaannya bisa bermacam — macam. Seperti:
Deepfake membuat mimpimu menjadi aktor bisa terwujud. Seperti yang disebutkan, kamu bisa melatih program Deepfake untuk membuat mukamu masuk menjadi salah satu aktor film. Ingin jadi Thor di film Marvel? Bisa.
Pengen main sama Daisy Ridley di Star Wars? Bisa. Cukup dengan 300–1000 fotoselfiekamu masukkan kedalam algoritma Deepfake, hasilnya adalah kamu bisa membuat film-mu sendiri.
Sekarang kita bisa membuat film amatir dengan menggunakan aktor khas Hollywood (bayangin buat video youtube bareng Leonardo Dicaprio).
Buat kalian yang diberikan rezeki wajah yang ganteng/cantik, maka kalian bisa bayangin menjadi model tanpa harus berkerja seperti model — model konvensional!
Deepfake bisa menyimpan data muka kamu dan bisa menjiplak nya di setiap model boneka yang ada.
Bayangkan kamu bisa membuat lisensi untuk penggunaan wajahmu dan bisa dapetin uang dengan menjual lisensi untuk menggunakan wajahmu!
Buat perusahaan/organisasi juga bisa dapat untung juga dengan penggunaan Deepfake.
Bayangkan perusahaan bisa membuat iklan yang menggunakan data penontonnya untuk memasang wajah mereka di iklan.
Walaupun sedikit menyeramkan, para perusahaan bisa membuat iklan yang lebih “personal” dan bisa lebih mendekatkan produk mereka dengan persepsi konsumer.
Sayangnya,Deepfakesendiri langsung mendapat kecaman pada saat pertama kali muncul. Banyak orang yang menggunakan deepfake untuk memasang wajah artisnya kedalam kontenasusila.
Impresi orang pertama kali untukDeepfakemerupakan impresi negatif, sehingga reddit pun melarang semua konten seperti deepfake untuk di post di dalam Reddit.Deepfakesudah membuat jalan untuk orang — orang membuat video yang bisa merusak nama baik dari orang lain.
Tapi bahaya terbesar dari penggunaanDeepfakeadalah jika hal tersebut digunakan untuk membuat video politik yang berpengaruh. Liat saja video dibawah ini. Insinyur di German bisa membuat video palsu Donald Trump.
DenganDeepfake, orang manapun (dengan kemampuan koding) bisa membuat video palsu tokoh manapun dan bisa membuat statement palsu.
Bayangin aja tiba — tiba seorang presiden membuat video bahwa dia akan mengirim nuklir ke negara lain? Pasti video tersebut akan membuat keributan. Karena itu banyak situs yang akan melarang penggunaan Deepfake di situs mereka (Twitter dan Reddit sudah melarang penggunanya).
Tapi dengan jumlah komunitas Deepfake yang semakin banyak, maka mereka akan mencari jalan untuk membuat komunitas deepfake tetap hidup.
Teknologi selalu menjadi pisau bermata dua, kegunaannya yang bermanfaat selalu diikuti dengan adanya potensi untuk menggunakan teknologi tersebut untuk berbuat kejahatan. Deepfake sendiri merupakan salah satu hasil dari teknologi yang mempunyai manfaat dan potensi untuk merugikan orang lain.
Nasib dari Deepfake sendiri ditentukan oleh para pengembang dan penggunanya. Apakah di masa depan deepfake bisa membawa manfaat yang mengalahkan impresi negatif kita sekarang? Hanya waktu yang menentukan.
Sadur:https://hackernoon.com/exploring-deepfakes-20c9947c22d9
Forming tech based society",https://miro.medium.com/v2/resize:fit:695/1*xm4saqRE_rkZSVsZ8PpWtg.png,"Artificial Intelligence, AI, Deepfake"
https://medium.com/s/story/human-artificial-intelegence-b8af13e63dae,Cerdas Saja tidak Cukup,Muhammad Ihsan,2017-10-25T15:25:29.724Z,"“Stephen Hawking memperingatkan bahwa bangkitnya robot mungkin menjadi bencana bagi umat manusia.”
Membaca kutipan pendek dari Stephen Hawking diatas membuat saya teringat film Transformers, filmscience fictionyang menceritakan bagaimana sekumpulan robot canggih saling bertempur demi menyelamatkan bumi. Ingatan itu muncul, seketika saat saya memikirkan bagaimana robot pada masa depan itu bekerja.
Adalah Artificial Intelegence (AI) atau kecerdasan buatan yang menjadikan bayang-bayang mengenai robot itu suatu saat bisa diwujudkan. Tak butuh waktu lama, silahkan cari dengan kata kunci “Dmitry Itskov” di mesin pencari pada telepon pintar anda. Milyuner asal Rusia itu saat ini, sedang merencanakan proyek besar yang cita-citanya adalah menciptakan hologram pada tahun 2045. Robot yang disempurnakan menjadi seperti layaknya manusia.
Hologram tersebut adalah AI super cerdas yang nantinya punya potensi melebihi kecerdasan manusia sendiri. Bayangkan jika suatu hari nanti kita akan menjumpai robot yang seperti manusia, melakukan sesuatu dan berpikir seperti kita.
Sebagai gambaran awal AI bisa dikatakan sebagai algoritma-algoritma yang diterapkan pada suatu data, menjadi suatu proses yang kemudian membentuk sesuatu. Bisa berupa pola, menjadikan hal rumit menjadi lebih sederhana. Contoh sederhana adalah google maps, algoritma google ini mampu mencari rute terpendek dengan waktu tempuh tercepat dari tempat asal ke tempat tujuan. Cara kerjanya adalah dengan membandingkan jarak dan waktu yang akan dibutuhkan dalam satu kali perjalanan, di tambah dengan kemungkinan kemacetan, kendaraan yang digunakan, bahkan cuaca yang saat itu berlangsung. Mencari alamat menjadi lebih gampang, cepat dan efisien.
AI sekarang sudah masuk hampir semua lini. Transportasi, hunian, alat rumah tangga, mesin pertanian serta masih banyak lagi alat yang sudah terpasang teknologi ini. Paling nyata adalah telepon pintar kita genggam setiap hari.
Nanti tinggal tunggu waktunya saja, kucing peliharaan kita bisa kita pasangi alat yang bisa memperingatkan kapan kucing itu lapar atau minta di ajak jalan-jalan.
Kecanggihannya memang banyak membantu kita dalam banyak hal. Namun, bersamaan dengan kemudahannya ada pula resiko yang menyertainya. Ada hal-hal yang kemudian dilucuti sedikit demi sedikit dari sisi kemanusiaan kita. Dulu ketika kita kebingungan arah, momen bertanya adalah sesuatu yang menjadi sarana kita buatsrawungdengan orang lain.Tepa slirakalau dalam bahasa jawa, saling sapa dan menyapa. Namun sekarang, jangankan dengan orang lain, dengan tetangga sendiri pun jarang kita lakukan. Itu yang lingkupnya kecil, belum dan masih banyak hal yang besar lainnya. Kapan-kapan kita bahas untuk masalah ini.
Fenomena tersebut ternyata ditangkap pula oleh Mbah Nun. Pada suatu waktu Mbah Nun pernah berpesan bahwa IT nanti akan menjadi masa depan. IT adalah kebudayaan kita yang baru. Maka kita dianjurkan untuk menguasainya.
Hal itu selaras dengan semakin cepatnya perkembangan teknologi yang berkembang. AI menjadikan mesin semakin cerdas, sehingga kalau manusianya tak mampu mengimbanginya. Bisa jadi teknologi itu akan menjadi bumerang bagi kita sendiri.
Bagaimana bisa? sadar atau tidak setiap hari aktifitas kita ketika terhubung ke internet adalah satu bentuk dari pembuatan senjata, yang nantinya bisa membantu atau membunuh kita. Histori dari pencarian, status yang kita buat, serta segala aktivitas di sosial media kita adalah proses pengumpulan data. Data tersebut kemudian dikumpulkan menjadibig data.Data besar yang setelah diolah bisa menunjukkan makanan favorit kita, hal-hal yang kita sukai, atau dalam skala besar bisa sampai menunjukkan tingkat pemasaran suatu produk misalnya.
Pun begitu bukan berarti dengan adanya bahaya yang mengancam tersebut tidak lantas membuat kita harus berpaling. Sebab selalu ada manfaat yang bisa diambil dari sesuatu. Tidak ada yang sia-sia. Kalaupun pada akhirnya mesin-mesin itu akan membunuh kita secara perlahan. Yang perlu di salahkan bukan mesinnya, melainkan siapa yang mengendalikannya.
Bukan apanya tapi siapanya. Manusia yang menggunakannya adalah faktor penentu apakah kecerdasan buatan itu akan membawa dampak baik atau buruk. Dampak jangka pendek AI tergantung pada siapa yang mengendalikannya, sedangkan dampak jangka panjangnya bergantung pada apakah bisa kita mengendalikannya.
Maka pekerjaan rumah kita yang paling utama adalah bagaimana bersikap dengan adanya AI tersebut. Menempatkan diri pada porsi dan wilayah yang tepat.
Manusia bisa mengendalikan harimau bukan karena manusia adalah makhluk terkuat, tercepat atau terbesar, tapi karena manusia cerdas. Manusia lebih pintar daripada Harimau. Kita dibekali akal untuk berpikir, mengolah, mensiasati sesuatu.
Menjaga akal supaya tetap waras, bisa jadi salah satu jalan. Kesadaran juga hal yang penting. Saya kemudian ingat nasehat dari mbah-mbah kita dulu, untukeling dan waspada ,mengingat dan terjaga. Ingat apa yang telah kita lakukan, dan tetap terjaga pada tahap kesadaran bahwa apa yang kita hadapi saat ini adalah kenyataan yang harus kita atasi.",https://miro.medium.com/v2/resize:fit:960/1*rchGC6ySU1CEezcLEMJkzg.jpeg,"Kecerdasan Buatan, AI, Artificial Intelligence"
https://medium.com/s/story/pengenalan-deep-learning-part-4-deep-learning-framework-introduction-tensorflow-keras-b8f00b146f06,Pengenalan Deep Learning Part 4 : Deep Learning Framework Introduction (TensorFlow & Keras),Samuel Sena,2018-03-19T15:14:42.897Z,"Kita sudah sama-sama belajar tentang konsepBackpropagation pada part sebelumnya. Pada dasarnya untuk melakukan training pada neural network, kita melakukan proses pada diagram dibawah ini secara terus menerus hingga loss atau error yang didapatkan memiliki nilai yang relatif kecil.
Kita bisa saja mengimplementasikan algoritma tersebut dan membuat sebuah neural network dengan menggunakan semua bahasa pemrograman yang kita bisa. Namun bagaimana jika masalah yang akan kita selesaikan itu adalah permasalahan yang sangat kompleks? Atau mungkin kita membutuhkan arsitektur yang unik dan kompleks? Atau kita ingin menggunakan GPU untuk mempercepat training?
Semua hal diatas dapat diatasi dengan menggunakan sebuah framework. Sama halnya seperti semua framework, deep learning framework ada untuk memudahkan kita untuk menyelesaikan masalah menggunakan deep learning.
Sebenarnya ada banyak sekali framework untuk deep learning. Bisa dikatakan setiap tech company besar yang ada sekarang memiliki framework masing-masing.
Google mempunyai TensorFlow, Facebook dengan Caffe2, Microsoft dengan CNTK dan masih banyak lagi framework lain seperti Theano dan PyTorch. Kali ini yang akan kita coba sama-sama yaitu TensorFlow (TF).
Ada lagi satu package yang akan kita gunakan yaitu Keras. Sebenarnya TensorFlow sudah cukup jelas cara penggunaannya, tapi kadang dalam riset kita sering sekali untuk mencoba arsitektur lain, mencari optimizer yang paling cepat dan bagus, tweaking hyperparameter, dll.
Dengan menggunakan Keras kita bisa melakukan semua itu dengan relatif lebih cepat dari pada ‘pure’ TensorFlow. Karena jika dibandingkan dengan Keras, TensorFlow serasa lebih “low level” meskipun sudah ada tf.layer yang baru.
Jadi Keras ini sebenarnya adalah wrapper dari TensorFlow untuk lebih memudahkan kita lagi. Oh ya, tidak hanya TensorFlow aja yang disupport, tapi kita bisa mengganti backend yang akan kita gunakan. Saat ini kita bisa gunakan TensorFlow, Theano dan CNTK sebagai backend dari Keras.
Oh ya..untuk instalasi, pada post ini tidak akan dibahas karena proses instalasi relatif mudah dan kalau memang ada trouble kita bisa googling dengan mudah untuk mencari solusinya, karena banyak sekali tutorial diluar sana tentang bagaimana menginstall TF.
Ok, kali ini kita akan mencoba untuk melakukan regresi terhadap sebuah fungsi non-linear seperti berikut :
Sebelumnya kita akan membuat data dengan menggunakan numpy. Input data nya dari rentang -20 sampai 20 dengan step 0.25. Kita juga buat targetnya sesuai dengan persamaan diatas.
Setelah data ada, kita bisa mulai membuat modelnya. Arsitektur yang akan kita coba adalah :
Disini kita juga menentukan optimizer yang akan kita gunakan, disini kita akan menggunakan SGD dan Mean Squared Error (MSE) sebagai loss functionnya. Sebelum kita bisa melakukan training, kita harus meng-”compile” model kita terlebih dahulu.
Setelah model siap, kita bisa mulai melakukan training dengan data yang kita sudah buat diawal. Untuk melakukan training, kita harus memanggil methodfit.
Pada method ini ada param batch_size dengan nilai 20 yang artinya kita gunakan mini-batch SGD. Kalau kita mau gunakan Batch SGD kita bisa set batch_size nya sesuai dengan jumlah data kita. Tapi itu silakan dicoba sendiri ya ..:D
Kita akan lakukan ini hingga 10000 epoch dan menyimpan semua parameter (weights dan bias) kedalam sebuah file.
Epoch, learning rate, batch_size, dll ini adalah hyperparameter yang bisa kita tentukan. Sedangkan nilai hyperparameter yang ideal, sampai saat ini masih belum ada riset yang bisa memecahkan masalah tersebut.
Sebenarnya ada metode seperti Grid Search contohnya untuk mencari hyperparameter, tapi tetap saja tidak terjamin kualitasnya. Kapan-kapan kita bahas masalah ini
Setelah 10000 epoch, saya mendapatkan MSE sebesar 0.0005 untuk training data. Pada tahap ini kita akan lakukan prediksi terhadap angka lain diluar training data yaitu26dan akan membandingkan hasil prediksi seluruh training data dengan target.
Kita bisa gunakan matplotlib untuk membuat dua grafik dan melihat perbandingannya. Line merah untuk target dan line biru untuk hasil prediksi.
Untuk hasil prediksi dari 26, saya dapatkan 36.755 sedangkan kalau dihitung seharusnya 36.783. Masih ada error tapi not bad lah.. Dan grafik prediction vs target untuk semua training data sangat identik sekali.
Karena semua post ke belakang yang dibahas adalah regresi, nanti pada part selanjutnya kita mau coba klasifikasi. Pada prinsipnya semua sama dengan regresi, hanya saja kita akan gunakan loss dan activation function yang berbeda. Detailnya nanti kita bahas lagi..So Stay Tune guys…
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*pidw2QNb9nG5BrcNzbY4NA.jpeg,"Deep Learning, Artificial Intelligence, Neural Networks, TensorFlow, Machine Learning"
https://medium.com/s/story/mengenal-decision-tree-dan-manfaatnya-b98cf3cf6a8d,Mengenal Decision Tree dan Manfaatnya,IYKRA,2018-07-23T09:30:16.891Z,"Building Future Capabilities
Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media sepertiBusiness Intellegenceguna membantu dalam pengambilan keputusan yang tepat.
Namun, hal tersebut tidak akan berarti tanpa adanya konsepdecision tree(pohon keputusan).Decision treeadalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia.Decision treeadalah model prediksi menggunakan struktur pohon atau struktur berhirarki.
Konsep dari pohon keputusan adalah mengubah data menjadidecision treedan aturan-aturan keputusan. Manfaat utama dari penggunaandecision treeadalah kemampuannya untuk mem-break downproses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan.
Nama lain daridecision treeadalah CART (Classification and Regression Tree). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaituclassification treedan jugaregression tree. Untuk memudahkan, berikut ilustrasi dari keduanya.
Untuk gambar diatas merupakan contoh dari classification tree, sedangkan gambar dibawah merupakan contoh dari regression tree.
Decision treejuga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target.Decision treememadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain.
Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan dalam metode ini, misalnya sebuah perusahaandirect mailmembuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja.
Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja.
Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan.Decision treeini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan.
Dalam hal akumulasi,decision treejuga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesaindecision treeyang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metodedecision treesangat tergantung pada bagaimana pohon tersebut didesain.
Terlepas dari kekurangan dan kelebihan daridecision tree, metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan jugamachine learning. Dalam dunia kerja,decision treesendiri sangat berguna untuk penilaiancredit scoring.Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman daridecision tree.
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/da:true/resize:fit:1000/0*MkbCJwBcPbnJsslt,"Data, Decision Tree, Machine Learning, Technology"
https://medium.com/s/story/konferensi-big-data-indonesia-kbi2018-bfbbf25bb4d3,Konferensi Big Data Indonesia (KBI2018),Machine Learning Indonesia (ML ID),2018-05-09T03:43:50.664Z,"Machine Learning Indonesia
Machine Learning Indonesia (ML ID) adalah komunitas yang relatif baru. Melalui kanal komunikasi utama diTelegram, kami terbentuk pada November 2017. Lain dengan kami, komunitasBigdata Indonesia(idBigData) telah terbentuk sejak Desember 2014 dan telah jauh lebih mapan.
Salah satu yang ingin kami contek dari idBigData adalah koneksi dengan kampus. Kampus seperti UI, ITB, UGM, ITS, IPB dan Unhas adalah beberapa university partner idBigData. Kami pernah sekali mengadakan Meetup di Makassar, bertempat diUpana Studiodengan salah satu pembicara adalahDr. Indrabayu, beliau bercerita soal menjadi pembicara dalam Meetup idBigData pada September 2016 di Unhas. Dari situ kami sadar ML ID beririsan dengan idBigData terutama soal niatan yang sama untuk menggandeng kalangan akademisi ke komunitas.
Melanjutkan tradisi, komunitas idBigData bersama denganUniversitas IndonesiadanBekrafdi 12–13 Mei 2018 ini akan menghelatKonferensi Big Data Indonesiayang bertempat di Balai Kartini, Jakarta.
Kegiatan tersebut diadakan pararel dengan International Workshop on Big Data & Information Security (IWBIS2018) dengan pembicara dari dalam (Bekraf, Bukalapak, Universitas Indonesia) maupun luar (Lancaster University, National University of Singapore) negeri. Sungguh menjanjikan seminar/diskusi yang menarik.
Big Data atau konsep yang serupa dengan itu, kami di ML ID belum sepakat dengan definisi Big Data dan relasinya dengan Machine Learning (ML), dapat menjadi sangat efektif saat digunakaan bersamaan dengan ML. Karenanya KBI2018 adalah event yang kami nantikan, beberapa anggota ML ID bahkan secara personal memprotes kami karena tidak mendapatkan tiket untuk event tersebut.
Tapi apa lacur, ternyata memang sulit mendapatkan tiket KBI2018, kami telah mengupayakan melalui panitia dan telah diberikan daftar peserta undangan, antara lain dari lembaga pemerintah dan kampus, dan harus mengakui bahwa jatah reservasi memang kosong (insert sad smiley here).
Di atas itu semua, tidak ada harapan kami selain agar KBI2018 terselenggara dengan sukses dan dapat mendorong Sinergi dan Inovasi diantara para pelaku Big Data dan Machine Learning baik dari akademisi maupun industri. Teriring doa agar kami mendapatkan satu-dua tiket tambahanhahaha.
Pada 8 Mei 2018 kami mendapatkan undangan ekstra yang tentu saja langsung habis dibagikan. Terimakasih idBigData, sampai bertemu diKBI2018.
Machine Learning Indonesia
Machine Learning Indonesian Community",https://miro.medium.com/v2/resize:fit:800/1*ztiNbjVP6eRqAIYPn53YXQ.png,"Big Data, Artificial Intelligence, Kbi2018, Iwbis2018, Machine Learning"
https://medium.com/s/story/5-data-professional-yang-menginspirasi-c16bb6b94513,5 Data Professional Yang Menginspirasi,IYKRA,2018-07-16T09:32:15.282Z,"Building Future Capabilities
Profesi data professional semakin diminati, seiring laju inovasi yang terus terjadi. Kami melakukan kurasi mengenai lima tokoh data professional. Mulai dari ranah data science, analytics, hingga deep learning. Siapa saja mereka? Silahkan simak dibawah ini.
1. Dean AbbottSelain sebagai seorang ahli data, Dean Abboutt sekaligus menjadi salah satu pendiri analytics consultant untuk e-commerce yaitu SmarterHQ dan direktur utama dari perusahaan Abbott Analytics. Dia juga aktif menulis berbagai macam ilmu pengetahuan tentang data di blog pribadinya.
2. Bernard MarrSebagai pendiri sekolah bisnis Advanced Performance Institute, Marr tak ragu untuk membagikan ilmu bisnis datanya kepada khalayak. Atas kontribusinya dalam dunia bisnis data, ia juga menjadi salah satu kontributor dalam forum ekonomi dunia dan kerap mendapatkan penghargaan sebagai salah satu dari 50 influencer bisnis top dunia.
Ia juga aktif menulis buku dan artikel yang salah satunya berjudul Using Smart Big Data, Analytics and Metrics to Make Better Decision and Improve Performance.
3. Andrew NgPendiri dari Baidu Research ini juga menjadi salah satu pendiri proyek Google Brain yang mengembangkan jaringan syaraf tiruan berskala besar. Salah satu idenya adalah mengajarkan dirinya untuk mengenali kucing dalam video. Atas keahliannya tersebut, kini ia sudah berhasil menerbitkan 100 makalah yang bersinggungan dengan data, msin dan bidang bisnis lainnya.
4. Lilian PiersonSiapa bilang wanita tidak bisa menjadi ahli data? Lilian Pierson membuktian kepada dunia bahwa wanita juga bisa menjadi ahli data. Bahkan Pierson kini tercatat sebagai pendiri sekaligus kepala ilmuwan data di Data Mania. Ia seringkali bersinggungan langsung dengan masyarakat dan membagikan ilmu dengan cara mengadakan seminar mengenai bisnis data.
Pierson baru-baru ini juga baru saja merilis sebuah buku berjudul Data Science for Dummies. Ia juga sangat aktif menulis dalam blog pribadinya dan memiliki channel Youtube khusus untuk menjelaskan konsep dasar dalam ilmu data.
5. Kira RadinskySatu lagi wanita yang sangat inspiratif dalam bidang data, dialah Kira Radinsky yang merupakan pendiri dari CTO SalesPredict. Dalam projeknya ini ia dikenal karena telah berhasil merintis proyek Artificial Intellegance atau kecerdasan buatan manusia berbasis aplikasi analitik prediksi yang mengubah cara perusahaa melakukan strategi bisnis.
Atas karyanya, Radinsky berhasil menerima berbagai macam penghargaan dan diakui dalam majalah Forbes sebagai salah satu dari 30 pengusaha baru yang sangat menginspirasi dalan bisang teknologi perusahaan.
Sumber: Berbagai Sumber
Penulis: Aprilia Safitri
Kolase Illustrasi: Rizaldi Adiputra
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*W-QwbzeGWJQq4d13B8sGtQ.png,"Deep Learning, Data, Indonesia, Data Science, Teknologi"
https://medium.com/s/story/memulai-deep-learning-dengan-python-c201124c1c6a,Memulai Deep Learning dengan Python,Ilham Bintang,2018-08-01T14:14:44.878Z,"Python merupakan bahasa pemrograman yang fleksibel, mudah di pahami, rapi dan tentu saja memiliki library yang berlimpah. Mulai dari pengolahan citra, pengenalan pola tradisional bahkan pengenalan pola modern.
Pada tulisan kali ini, penulis menggunakan pengenalan pola tradisional sebagai istilah untukmachine learningdan pengenalan pola modern sebagai istilah untukdeep learning.
Pengenalan pola tradisional/machine learningmerupakan pengenalan pola yang menggunakan teknik-teknik dasar pengolahan citra seperti dilasi, erosi, GLCM, Konvolusi dan lain-lain untuk mencari fitur pada pola kemudian dilakukan klasifikasi dengan metode-metode seperti K-Nearest Neighbor, SVM, Naive Bayes dan lain-lain.
Pengenalan pola modern/deep learningmerupakan pengenalan pola yang memiliki tingkat fleksibilitas dan probabilitas yang tinggi terhadap data latih. Sehingga pada proses ekstraksi fitur, tidak menitikberatkan peneliti untuk memilih metode-metode ekstraksi dan menyesuaikan dengan data latih. Akan tetapi pengenalan pola modern menitik beratkan pada keseragaman data dan jumlah data training yang digunakan.
Perbedaan yang paling signifikan antara pengenalan pola tradisional dan modern adalah pada tahap pengenalan. Pengenalan pola modern menggabungkan proses ekstraksi fitur dan klasifikasi, sehingga peneliti tidak perlu mencari pola data dan memilih metode yang tepat (pada setiap kasus).
Pengenalan pola modern menggunakan Jaringan Saraf Tiruan (Neural Networks) dalam proses ekstraksi dan klasifikasi, untuk lebih jelasnya dapat dilihat pada gambar berikut:
Untuk menggunakan python sebagaimachine learning, maka terlebih dahulu kita harus memasang python di komputer kita. Di tutorial ini akan dijelaskan bagaimana caranya. Di sini digunakan sistem operasi Windows 7. Untuk sistem operasi lain, lebih kurang prosesnya adalah sama. Untuk sistem operasi linux, python sudah terinstall otomatis saat linuxnya diinstall.
Adapun langkah — langkah menginstal python di windows adalah sebagai berikut:
Anaconda merupakan package installer yang cukup handal dan memiliki package yang lengkap dan terupdate. Anaconda sangat direkomendasikan ketika menggunakan python sebagai bahasa untuk machine learning.
PadaAnacondajuga disertakanconda.Condaadalahpackage management systemdanenvironment management systemyang akan memudahkan pemasanganpackagedan mengelolanya.
Proses instalasi anaconda pada windows:
Proses instalasi anaconda pada linux:
2. Instalasi yang dijelaskan pada tulisan ini menggunakanbash, reviewlicense agreementdengan terus menekan 'Enter', lalu ikuti petunjuk selanjutnya. Untuk pengguna biasa bisa memilih 'yes' untuk semua opsi yang ada.
3. Restart terminal setelah instalasi selesai, atau bisa juga dengansource ~/.bashrc.
Pip merupakan package installer bawaan python yang merupakan package install er default python. Untuk menggunakan pip anda cukup melakukan perintah dengan menambahkan pip.
https://www.codepolitan.comwww.pythonindo.com
/dev/ and foo bar fighters",https://miro.medium.com/v2/da:true/resize:fit:1200/0*0saqobF-WMRioOdO,"Deep Learning, Neural Networks, Machine Learning, Python"
https://medium.com/s/story/jenis-data-dan-pengenalan-database-belajar-machine-learning-part-3-c22e6b29b85f,Jenis Data dan Pengenalan Database — Belajar Machine Learning Part #3,Farhan,2018-03-30T04:12:40.876Z,"Dalam bahasa sederhana, data adalahfakta yang berhubungan dengan objek tertentu. Contohnya seperti, nama anda, umur, tinggi badan, berat badan, dan sebagainya adalah beberapa data yang berhubungan dengan anda (objek). Begitu juga dengan gambar, foto, file, dan lain-lain juga termasuk sebagai data.
Database adalah kumpulan data yang sistematis.Karena data pada sebuah database itu terorganisir, manajemen data jadi lebih mudah.
DBMS adalah kumpulan program/codes yang memungkinkanuseruntuk lebih mudah mengakses database, seperti mengubah data, dan membantu dalam merepresentasikan data. Sebuah DBMS juga dapat membantu pengontrolan akses ke database oleh bermacamuser.DBMS dibagi menjadi 2 yaitu:
Jenis DBMS ini berbentuk kumpulan tabel, yang disebut jugarelations.Berbeda denganjaringan DBMS,RDBMS tidak mendukung banyak hubungan. RDBMS biasanyamemiliki tipe data spesifikyang dapat di-support.Ini adalah jenis DBMS yang paling populer yang ada di pasar atau sering dipakai. Contoh RDBMS adalahMySQL, Oracle, dan Microsoft SQL Server.
2.Object-orientedrelational DBMS
Jenis ini mendukung penyimpanan data jenis baru. Jenis ini menyimpan data dalam bentuk objek.Objek yang disimpan memiliki atribut-atribut.Contohnya jenis kelamin atau umur, dan metode pemrosesan datanya.PostgreSQL adalah contohobject-oriented relational DBMS.
SQL adalah singkatan dariStructured Query Language,biasa dibaca “S-Q-L” atau kadang “See-Quel”adalah bahasa standar yang digunakan pada Relational Database.SQL dipakai untuk mencari, mengedit, menghapusrecordsdi database.
Ketika semua data-data disimpan pada Big Data, pada akhirnya kita bisa melihat jenis data apa yang kita perlukan. Jenis data dibagi menjadi 3 yaitustructured, unstructured, dan semi-structured.Jenis datastructureddapat disimpan dalam bentuk tabel, dengan baris dan kolom atau bisa disebutRDBMS. Untuk jenis dataunstructured(seperti video, suara, text, pdf, dsb) harus disimpan dengan format khusus karena tidak memiliki struktur yang spesifik seperti jenis datastructured. Sedangkan untuk jenis datasemi-structuredbisa dimasukan ke dalam sebuah tabel, tetapi skemanya tidak sama dengan tabel biasa yang hanya terdiri dari baris dan kolom. Contohnya dokumen csv, dokumen xml, dan dokumen json.
Dah segini dulu, selanjutnya akan saya bahas apa ituHadoop, SQL, dan NoSQL.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:620/1*46HfsHaaknuHcj9iAFg7aA.png,"Data Science, Database, Machine Learning, Artificial Intelligence"
https://medium.com/s/story/biar-nggak-kalah-saing-sama-robot-c3931ba323ec,Biar Nggak Kalah Saing sama Robot,Nur Khansa,2018-03-31T11:28:48.429Z,"Sahabat berprosesmu.
Ribut-ribut otomatisasi pekerjaan oleh robotArtificial Intelligence(AI) makin sini memang bakal makin ramai, karena teknologi pun semakin canggih dan sains makin berkembang.
Beberapa pekerjaan yang selama ini dikerjakan manusia, digadang-gadang sudah dapat digantikan penuh oleh para robot.
Nggak cuma pekerjaan repetitif seperti nge-pak barang, misalnya. Pekerjaan kreatif yang perlu mikir macem menulis pun sudah ada versi otomatisnya.
Nah trus, gimana nih kira-kira nasib kita? Sudahlah nyari pekerjaan makin susah, ditambah pula harus sikut-sikutan sama robot.
Hm, tapi tenang. Ada sejumlahskillyang belum dapat dikuasai para AI.
Setidaknya, sampai hari ini.
Perdebatan mengenai apakah robot bisa memilki perasaan atau nggak, sebenarnya sudah berlangsung lama. SoftwareArtificial Intelligence(AI) saat ini sudah mampu mengenali kategori emosi yang terdapat pada mimik wajah dan suara manusia, tapi masih sangat jauh untuk mampu memunculkan rasa empati sungguhan.
Hingga saat ini, komputer tercanggih pun belum sedikitpun punya kemampuan merasakan emosi manusia dan berempati terhadap hal tersebut. Bisa mengenali dan mengkategorisasikan jenis emosi dengan benar-benar merasakan apa yang orang lain rasakan adalah dua hal yang berbeda.
Menurut penulisRise of the Robots: Technology and the Threat of a Jobless Future, Martin Ford, salah satu area pekerjaan yang akan sulit digantikan oleh robot adalah pekerjaan yang berhubungan dengan manusia secara kompleks, seperti perawat, pebisnis yang banyak bernegosiasi dengan klien, dan sebagainya.
Walaupun AI di bidang kedokteran sudah dikembangkan, posisi perawat di bidang rehabilitasi medis yang harus menguatkan pasien dengan penyakit parah, misalnya, akan terus dibutuhkan.
Kita juga bisa lho menjadi lebih empatik dalam pekerjaan sehari-hari, seperti menjadi lebih ramah terhadap konsumen, lebih peka terhadap kebutuhan calon pembeli, dan sebagainya. Intinya, mengedepankanhumane touchdalam pekerjaan kita saat ini.
Dalam 20 tahun ke depan, menurut laporan Deloitte, 100.000 pekerjaan di sektor legal memiliki kemungkinan besar untuk diotomatisasi.
Namun, hal-hal terkait aturan dan perjanjian — apalagi menyangkut nilai ‘kebenaran’, nggak selamanya berlaku secara hitam dan putih. Dalam bekerja di bidang legal atau hukum, bakal banyak proses yang melibatkan kemampuan otak manusia dalam memahami argumen dengan motif-motif terselubung, atau menangkap maksud dari alasan nggak rasional. Kemampuan ini akan sulit dimiliki robot.
Hal yang mungkin dilakukan robot dalam bidang ini adalah mengerjakan sesuatu yang lebih bersifat repetitif, seperti menjadi asisten legal dan atau paralegal yang banyak berkutat dengan pencarian dan pengumpulan data.
Martin Ford mengatakan, pekerjaan yang melibatkan kreativitas seperti menjadi seniman, ilmuwan, atau mengembangkan strategi bisnis sejauh ini masih aman dari otomatisasi. Otak manusia mampu memikirkan hal-hal yang ‘gila’ dan ‘nyeleneh’, nggak rasional, radikal, hingga nggak logis sekalipun.
Sementara, cara ‘berpikir’ robot cenderung terstruktur, logis, dan rasional.
Tapi nih, Ford mengatakan, ia nggak yakin apa yang akan terjadi dalam 20 tahun mendatang perihal robot dan kreativitas. Soalnya, saat ini aja, sudah ada robot yang dikembangkan untuk mampu melukis hingga menulis.
Nggak sepenuhnya masa depan bisa dikendalikan. Ada sejumlah pekerjaan yang mengharuskan seseorang untuk menavigasi kondisi darurat dan hasil yang nggak diketahui. Menurut Ford, pekerjaan yang paling mudah digantikan oleh robot adalah pekerjaan yang repetitif dan dapat diprediksi.
Nah, otak manusia punya kapasitas untuk beradaptasi secara dinamis dengan perubahan, dan bereaksi terhadap hal-hal yang bersifat mendadak. Sementara, robot belum dapat diprogram untuk menjadi demikian.
Nggak usah jauh-jauh contohnya, pekerjaan seperti tukang ledeng yang harus siap dipanggil kapanpun dan kemanapun dalam kondisi darurat, misalnya, masih sulit dikerjakan robot, karena hal-hal mendadak tidak sesuai dengan pola kerja robot yang bergantung dengan program.
www.opencolleges.edu.au
Sahabat berprosesmu.",https://miro.medium.com/v2/resize:fit:1024/1*3ivEZhni_PUx6v4IgnPr4A.jpeg,"Sains, AI, Tips, Robot, Teknologi"
https://medium.com/s/story/10-ai-yang-membentuk-masa-depan-c838194d1294,10 AI yang membentuk masa depan,Andi muhammad,2018-04-10T00:47:18.580Z,"Forming tech based society
Di zaman sekarang AI sebentar lagi akan memasuki kehidupan secaramainstream. Mau gak mau kita haus sudah melek dan paham dengan kapabilitas AI untuk membantu kehidupan kita sehari — hari. Untuk mempermudah kalian, kami dari Suar Media memilih produk AI yang akan berkembang dan semakin berpengaruh di masa mendatang.
Siapa yang tidak kenal siri? AI ini dibuat oleh Apple sebagai personal assistent untuk Iphone. Siri berguna untuk membantu hidup orang — orang dengan seakan — akan mempunyai robot sebaguna di genggaman tangan. Siri bisa membuat jadwal, bisa menjawab pertanyaan (dengan bantuan google), mencari arahan, etc. Siri dibangun denganMachine Learning. Itu berarti Siri akan semakin pintar dan semakin bisa berkomunikasi baik dengan penggunanya, semakin pintar Siri untuk mengetahui preferensi penggunanya.
Sekarang, ada juga software baru yang merupakan juga saudari Siri yaitu adalah Cortana. Cortana merupakan saudari beda ibu dari Siri. Kenapa? Karena Cortana adalah AI buatan microsoft yang dibuat untuk menjadi pembantu versi microsoft. Cortana juga bisa membuat jadwal, menjawab pertanyaan, dan mencari arahan. Sama seperti Siri. Tapi keunggulan Cortana adalah dia mempunyai akses yang banyak dari pada Siri. Dia ada di IOs, Android, dan Microsoft sehingga Cortana bisa dipakai siapa saja. Tetapi Cortana memerlukan akun untuk Microsoft agar bisa digunakan.
Alexa merupakan jawaban Amazon untuk Siri dan Cortana. Alexa merupakan AI buatan Amazon yang berbentuk seperti speaker yang bisa diaktifkan dengan suara. Alexa bisa dipanggil dari seluruh penjuru ruangan selama alexa bisa mendengar panggilan tersebut. Karena kecanggihan Alexa untuk bisa menerima perintah suara, Alexa menjadi populer di Amerika pada saat penjualan. Alexa bisa menerima banyak perintah. Contohnya adalah : menjawab pertanyaan,mengatur jadwal, mencari jalan, dan lain sebagainya.
Kalau kamu anak — anak millenial malas baca koran, maka AI dari Recent News bisa membantumu. Sekarang, ada aplikasi yang bernama Recent News. Recent News adalah app yang bertujuan untuk mengumpulkan berbagai macam berita dan menyajikan langsung kepada pembaca menggunakan AI. AI tersebut akan belajar mengenai preferensi mu dalam memilih berita sehingga Recent News akan belajar dan menyusun berita yang benar — benar sesuai dengan preferensi mu.
Bukan namanya anak zaman “now” kalau gak ada seribu foto selfie di dalam memory disk hpnya. Setelah memorinya penuh, akhirnya harus dihapus juga (terutama foto manta). Sekarang bagaimana kalau ada app yang bisa menghapus foto — foto mantan, screenshot gosip, dan lain — lain?
Kalian pasti ingin memakai Stifr Magic Cleaner. Stifr Magic Cleaner adalah aplikasi yang berbasis AI yang bertujuan untuk menghapus foto — foto yang sudah tidak digunakan lagi dan membersihkan memori di dalam memori disk. Siftr gratis dan bisa di download di Iphone dan Anroid
Apakah kalian akan berobat kepada AI? Dengan kemajuan AI yang begitu pesat, tidak bakal heran kalau ada yang namanya AI di bidang kesehatan. Salah satunya adalah Ultromics. Ultromics merupakan AI yang dirancang untuk mendeteksi penyakit jantung yang berada di dalam. AI dari Ultromics diberikan lebih dari ribuan CT scan dan dari hasil yang disampaikan oleh BBC news adalah Ultromics bisa mendeteksi penyakit jantung dan kanker dengan kapasisas yang sama dengan dokter tapi dengan kecepatan yang lebih daripada dokter.
Sekarang AI juga bisa membantu untuk menyelidiki kanker paru — paru. optellum adalah AI yang dirancang untuk mendeteksi kanker paru — paru di dalam tubuh. AI dari Optellum diberikan lebih dari 4000 CT Scan dan dari hasil yang disampaikan oleh BBC news adalah Optelum bisa mendeteksi kanker lebih efisien daripada dokter. AI ini bisa membedakan antara kasus kanker yang serius dan dengan yang tidak, sehingga bisa memotong waktu untuk perawatan dan akhirnya bisa menghemat banyak uang. Dengan begitu, Optellum menghemat sebesar 50% untuk perawatan kanker paru — paru.
Kalau bicara tentang Netflix, pasti kita bicara tentang lebih dari ribuan database film yang mereka miliki ditonton oleh Jutaan manusia yang berlangganan Netflix. Karena Netflix sendiri sudah mengumpulkan lebih dari Milliaran data hasil tontonan para pelanggan mereka, maka mereka menggunakan data tersebut untuk membuat AI yang bisa menampilkan rekomendasi film yang paling cocok dengan selera pelanggan Netflix. Sehingga sekarang Netflix bisa memberikan rekomendasi yang lebih sesuai dan tepat dengan preferensi penontonnya.
Pernah kaga kalian menyalakan Ac (Air Conditioner) di dalam rumah dan kaga pernah dapat temperature yang pas? Dengan Nest maka kalian bisa menyerahkan tugas sepele tersebut kepada AI. Dengan sistem pendeteksi suhu tubuh, maka Nest bisa membuat suhu yang terasa “pas” dengan preferensi tubuhmu. Dengan adanya pengatur seperti nest, maka pengguna nest juga bisa mengatur pengeluaran listrik dengan lebih efisien.
Pandora adalah jasa streaming musik di internet yang menggunakan AI sebagai senjata terbesarnya. Dengan AI Pandora membentukcustomer experiencedengan cara AI membuat rekomendasi sesuai database penggunaan dari cutomer sehingga customer tersebut mendapatkan pengalaman streaming yang menarik. Banyak yang mengatakan pandora bisa menyajikan rekomendasi musik yang terasa “pas”. Sama seperti Netflix. Yang menarik dari Pandora dan Netflix adalah AI bisa merekomendasikan musik atau film yang sesuai dengan selera seakan — akan AI itu mempunyai selera juga (setidaknya selera yang sama dengan setiap penggunanya).
Yang terakhir dari list ini adalah Tesla. Tesla sendiri sudah membuat riset mengenai AI untuk dterapkan di mobil listrik buatan mereka. AI dari Tesla yang bertujuan untuk membantu kontrol automatis pada saat mengemudi. Sehingga para pengemudi tesla bisa bebas dari kendali setir dan menyerahkan semuanya pada AI.
·https://www.forbes.com/sites/robertadams/2017/01/10/10-powerful-examples-of-artificial-intelligence-in-use-today/2/#5931a8a83c8b
·https://futurism.com/ai-medicine-doctor/
·https://futurism.com/ai-diagnose-heart-disease-lung-cancer-more-accurately-doctors/
·https://futurism.com/ai-equipped-microscope-diagnose-deadly-blood-infections/
·https://futurism.com/ai-assisted-detection-identifies-colon-cancer-automatically-and-in-real-time/
·https://www.iqvis.com/blog/top-7-best-artificial-intelligence-apps-for-android-ios/
·http://www.bbc.com/news/health-42357257
Forming tech based society",https://miro.medium.com/v2/resize:fit:316/1*4wKLcEGhBSYadPkWKKYBsw.png,"Artificial Intelligence, AI"
https://medium.com/s/story/deep-learning-dan-manfaatnya-bagi-perkembangan-ai-cab94e20c19a,Deep Learning dan Manfaatnya Bagi Perkembangan AI,IYKRA,2018-07-31T06:51:05.109Z,"Building Future Capabilities
Dalam dunia teknologi informasi, kita mungkin sering mengenal apa yang dimaksud dengan deep learning. Belakangan, deep learning memang sedang trend dan kerap dibicarakan oleh para ahli IT terutama dalam hal artificial intellegence (AI).
Bahkan tak tanggung-tanggung perusahaan sekelas Google berani mengeluarkan biaya sebanyak 400 juta USD demi membeli perusahaan Deepmind yang bergerak dalam bidang deep learning. Lalu, sebenarnya apa yang dimaksud dengan deep learning secara keseluruhan?
Deep learning adalah salah satu cabang machine learning (ML) yang menggunakan deep neural network untuk menyelesaikan permasalahan pada domain ML. Neural network sendiri adalah model yang terinspirasi oleh bagaimana neuron dalam otak manusia bekerja. Tiap neuron pada otak manusia saling berhubungan dan informasi mengalir dari setiap neuron tersebut.
Dengan kata lain, secara sederhana deep learning merupakan sebuah pembelajaran mendalam dalam pengembangan jaringan saraf tiruan atau neuron, yang tentunya memiliki banyak lapisan. Metode ini sangat efektif dan lebih mudah dalam mengidentifikasi pola dari data yang dimasukkan.
Deep learning sangat berdampak pada kemajuan perkembangan yang telah dicapai AI secara bertahap. Tidak hanya untuk perangkat lunak, namun para penggunanya juga telah merambah diberbagai bidang industri.
Contohnya seperti Facebook yang menggunakan sistem deep learning. Lalu pada asisten virtual yang berbasis AI untuk membantu penggunanya melakukan berbagai tugas seperti memesan dan meneliti.
Deep learning bahkan bisa dibilang merupakan teknik AI yang paling baik jika dibandingkan dengan ML. Meski keduanya erat kaitannya dengan data dan algoritma, namun pada ML program pembelajaran yang digunakan berupa saraf neuron yang berfungsi untuk memerintah suatu otak pada komputer.
Sedangkan deep learning lebih kearah kemampuan sistem yang di setting dengan baik demi meningkatkan kerja pada komputer. Untuk itu, wajar adanya apabila deep learning di klaim merupakan teknik AI yang paling baik dibanding ML. Sebab, secara garis besar deep learning bekerja untuk menyempurnakan system kerja dari ML.
machinelearningmastery.com
Penulis: Aprilia Safitri
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*r9w6eJoRQ2MzZNzGjgnfGw.png,"Deep Learning, AI, Technology"
https://medium.com/s/story/mengatasi-missing-data-dalam-dataset-cf5c7657dbf1,Mengatasi “Missing Data” dalam Dataset,R. Kukuh,2018-02-13T03:53:34.116Z,"Missing data dalam dataset tentu akan membuat hasil analisa data menjadi kacau. Berikut adalah contoh missing data dalam dataset:
Simak penjelasan cara mengatasi missing data dalam dataset ini dengan menggunakan Python dan R.
Buka Anaconda Navigator
Buka project sebelumnya tentangData Preprocessing 02
Lanjutkan dengan kode berikut:
Pro Tips: Highlight kata “Imputer” lalu tekan ⌘ + i untuk menampilkan dokumentasinya seperti ini:
Lanjutkan kode diatas hingga menjadi seperti ini:
Eksekusi kode di atas, maka di console akan tampak seperti ini:
Ketik X pada console untuk mengetahui isi dari matrix X setelah missing data-nya di-handle oleh Imputer:
Buka project sebelumnya tentangData Preprocessing 02
Lanjutkan dengan kode berikut:
Eksekusi kode diatas sehingga dalam console akan muncul result seperti ini:
Lihat objek dataset, perhatikan bahwa sekarang missing data pada kolom Age sudah diganti:
Lakukan hal yang sama terhadap missing data pada kolom Salary
Eksekusi kodenya, lalu perhatikan bahwa sekarang dataset sudah tidak memiliki missing data lagi.
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:956/1*YRNtT3UGAfdnHP2ahdNykg.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/beberapa-tahun-lagi-cfc084ed238f,Beberapa Tahun Lagi,Fikri Rasyid,2017-12-07T00:00:17.936Z,"Semakin banyak gue membaca dan memperhatikan diskusi yang ada, kesimpulan gue adalah semakin robotik dan kecerdasan buatan sudah tidak terhindarkan. Konsekuensi signifikan dari ini: Kalau pekerjaan lumenial, repetitif, dan kemungkinan besar bisa digantikan mesin + kecerdasan buatan, sumber penghidupan lu bisa terancam. Buat gue penting untuk menyadari ini karena gue punya anak. Selain gue harus mengamankan sumber penghidupan gue dan keluarga, gue harus bisa memberi nasihat yang tepat untuk anak gue kelak bahwa bidang tertentu kemungkinan besar akan digantikan oleh mesin.Beware of that.
Satu-satunya hal yang gue bisa pikirkan untuk mengantisipasi ini adalah berkecimpunglah di bidang yang melibatkan aspek kreatifitas,mastery, dan semua aspek yang kemungkinan besar belum bisa dipelajari oleh mesin dalam 50–100 tahun kedepan.
Sebagai penutup pembicaraan, ini beberapa artikel menarik mengenai kecerdasan buatan:
Untuk diskusi lebih dalam:
Dan terakhir, gue lupa baca dimana (kalo ngga salah wawancara Elon Musk tapi gue lupa yang mana), tapi substansinya kurang lebih:
Mesin tidak perlu melakukan tugasnya dengan sempurna untuk mengambil alih tugas manusia. Mesin hanya perlu melakukannya secara lebih baik dan lebih menguntungkan secara ekonomis dari manusia.
Google my name for more info.",https://miro.medium.com/v2/resize:fit:1200/1*J9B1VThENjmeK7xKCkCyZw.jpeg,"Robotic, Artificial Intelligence, Culutre, Automation, Future"
https://medium.com/s/story/data-scientist-di-dunia-industry-it-security-cfcfd0316ba5,Data Scientist di Dunia Industri IT Security,MR. Sahputra,2018-06-13T15:58:13.094Z,"Notes: Artikel ini dibuat atas permintaan komunitasngesec.idsebagai donasi dalam rangka pembuatan buku digitalsecstory. Mudah-mudahan semakin banyak profesional IT security Indonesia yang peduli dengan komunitas agar dapat mengarahkan hacker-hacker generasi muda untuk masuk kedalam dunia industri IT security.
Akhir-akhir ini kita sering mendengar tentang istilahdata scientist. Pada salah satu seminar yang diadakan diUniversitas Pakuan baru-baru inisaya diminta untuk menjadi salah satu pembicara membawakan materi tentang peranan data scientist dalam dunia industri IT security.
Sebelum kita membahas lebih lanjut mengenai peran data scientist, kita perlu memahami dulu apa itudata science.
Mengutip dariwikipedia,
Data scienceis an interdisciplinary field of scientific methods, processes, algorithms and systems to extractknowledgeor insights fromdatain various forms, either structured or unstructured,[1][2]similar todata mining.
Data science is a “concept to unify statistics, data analysis, machine learning and their related methods” in order to “understand and analyze actual phenomena” with data.[3]It employs techniques and theories drawn from many fields within the broad areas ofmathematics,statistics,information science, andcomputer science.
Turing awardwinnerJim Grayimagined data science as a “fourth paradigm” of science (empirical,theoretical, computational and now data-driven) and asserted that “everything about science is changing because of the impact of information technology” and thedata deluge.[4][5]
Data science merupakan suatu disiplin ilmu ilmiah yang menggunakan metode, proses, algoritma, dan sistem untuk meng-ekstrak pengetahuan maupun wawasan dari segala macam bentuk data, baik data-data yang terstruktur dengan baik ataupun data-data yang tidak terstruktur dengan baik.
Data science itu sendiri merupakan sebuah konsep yang menyatukan ilmu statistik, analisis data, machine learning, dan beberapa metode lainnya dengan tujuan memahami serta menganalisis fenomena aktual (sesuatu yang sedang, atau telah terjadi) berdasarkan data-data yang tersedia. Artinya, data science mempergunakan tehnik dan teori yang diambil dari berbagai macam disiplin ilmu lain seperti Matematika, Statistika, Information Science, dan Computer Science.
Masih seperti tertulis di Wikipedia, Jim Gray berpendapat bahwa data science merupakan “paradigma ke-4” dari ilmu pengetahuan. Paradigma pertama adalahempirical(berdasarkan pengalaman), kedua adalahtheoritical(berdasarkan teori), ketiga adalahcomputational(berdasarkan perhitungan), dan yang keempat (data science) adalahdata-driven, yaitu berdasarkan data.
Data scientist adalah individu-individu yang profesinya memformulasikan suatuinsight— sebuah wawasan, berdasarkan data-data yang tersedia dengan menggunakan metode pendekatan matematika, statistika, dan computer science.
Insight — atau wawasan seperti apa yang ingin diformulasikan? Nah, ini kembali lagi pada bidang dimana data scientist tersebut mengolah data.
Ketika data science digunakan untuk menganalisis data transaksi e-commerce maka insight-nya bisa jadi masuk dalam scope dunia e-commerce. Ketika data science digunakan untuk menganalisis data logs dengan tujuan untuk menentukan apakah ada atau tidaknya serangan cyber, maka disinilah letak peran serta data scientist dalam dunia IT security.
Saya paling suka memberikan ilustrasi pekerjaan tim Security Operation Center (SOC) menggunakan video dari Deloitte UK berikut ini,
Meskipun kelihatannya seperti aksi dalam sebuah film namun baiktoolsataupun metodologi yang dipergunakan semuanyareal.Aktivitas tersebut benar-benar telah, sedang, dan akan terjadi di dunia nyata.
Setiap organisasi ataupun perusahaan yang terhubung dengan internet akan memiliki resiko seperti dalam video tersebut, apalagi jika organisasi ataupun perusahaan tersebut termasuk kategorihigh-profile. Hacker-hacker dengan niat jelek yang motivasinya hanya sekedar iseng, ataupun memang ingin mengambil keuntungan akan mencoba berbagai macam cara demi mendapatkan akses kedalam perusahaan atau organisasi melalui teknologi.
Ketika hal tersebut berlangsung maka benar-benar terjadi sebuahcyberwarantara hacker penyerang dan tim bertahan — tim bertahan ini biasanya kolaborasi antara engineer, system administrator, security analyst, dan incident response.
Mereka akan beradu cepat. Penyerang akan berusaha sebaik mungkin agar bisa masuk kedalam jaringan ataupun infrastruktur organisasi, sedangkan tim didalam organisasi tersebut akan berusaha sebaik mungkin untuk menangkal serangan. Siapa yang kemampuannya lebih baik — dan lebih cepat akan memenangkan “perang” tersebut.
Ketika berada dalam posisi bertahan maka organisasi dituntut untuk memiliki mekanisme mendeteksi adanya serangan dengan cepat. Saat terdeteksi maka organisasi tersebut kemudian dituntut untuk bisa secepat mungkin menangkal serangan dan melindungi aset serta servis-servis miliknya agar operasional perusahaan tidak terganggu.
Detection, danaction, merupakan dua kata kunci dalam operasional SOC. Seberapa cepat kita bisa mendeteksi serangan, dan seberapa cepat kita bisa merespon serangan tersebut merupakan hal yang sangat penting.
Tiga pilar ini sering kita temukan saat membahas mengenai fondasi SOC:People,process, dantechnology. Saya tidak akan membahas terlalu dalam konsep ketiganya pada artikel ini, namun lebih kepada ilustrasi hubungan ketiganya dengan peran sertadata scientist.
SIEM(Security Information Event Management) merupakan salah satu komponen utama dalam SOC. SIEM merupakan teknologi yang mengumpulkan log-log dari berbagai macam perangkat pada suatu organisasi. Berbagai macam log tersebut kemudian disatukan dan dianalisis untuk mengidentifikasikan apakah telah terjadi serangan terhadap organisasi.
Berbagai macam vendor mengimplementasikan teknologi SIEM dengan arsitektur yang berbeda meskipun fungsionalitasnya sama. Diantara vendor penyedia teknologi SIEM adalahIBMdenganQRadar-nya,MicroFocus(HP) denganArcSight-nya,LogrhythmdenganLogRhyhm-nya,AlienVaultdengan USM ataupun OSSIM-nya, dsb.
Seiring dengan perkembangan teknologi dimana perangkat digital bertambah jumlahnya, dan kebutuhan organisasi untuk terkoneksi dengan dunia internet meningkat maka jumlah perangkat IT perusahaan juga ikut meningkat. Akibatnya terjadi ledakan jumlah data dan log yang masuk ke SIEM untuk dianalisis. Itu sebabnya teknologi SIEM juga ikut berkembang agar tetap dapat diandalkan sebagai teknologi yang dapat menganalisis ledakan data-data log tersebut menjadi suatu informasi security yang akurat.
Berikut ilustrasi arsitektur SIEM salah satu vendor (LogRhythm),
LogRhythm mengumpulkan berbagai macam log dari perangkat-perangkat networking organisasi / perusahaan, server-server, desktop yang dipergunakan oleh staff, bahkan hingga network traffic (dalam bentuk .pcap) yang berseliweran untuk kemudian ditampung dan diproses oleh komponen “Data Processor”.
Diantara tugas data processor adalah mengklasifikasikan log mentah kedalam beberapa kategori, misalnya:audit,operations, dansecurity.
Pada contoh diatas kita bisa melihat bentuk data mentah dari salah satu perangkat kemudian di-proses oleh data processorsehingga menjadi informasi yang memiliki konteks:Log Sourcemerupakan informasi log tersebut berasal dari perangkat vendor mana (cisco),Common Eventmerupakan kategori jenis event (File Access Denied), dsb.
Selain memproses data log mentah menjadi data yang memiliki konteks, dilakukan juga ekstraksi metadata. Metadata inilah yang kemudian akan masuk kedalam komponenAI Engine.
Dalam sesi seminar di Universitas Pakuan ada satu pertanyaan yang ditujukan kepada saya saat sesi diskusi panel.
“Peran data scientist itu dimulai dari fase mana? Apakah mulai dari pengambilan data, ekstraksi data, hingga membuat model matematika? Atau proses ekstraksi data dilakukan oleh data engineer sehingga data scientist fokus pada pembuatan model?”
IMHO, jawabannya bisa beragam. Seperti tertuang dalam definisi Wikipedia, data science melibatkan berbagai disiplin ilmu. Apabila seorang data scientist memiliki kemampuan untuk mengumpulkan data (contoh:data scrapping), ETL (Extract, Transform, Load), hingga membuat model, maka hal tersebut sah-sah saja.
Namun biasanya dalam dunia industri — terutama karena scope-nya luas sekali, seorang data scientist tidak perlu melakukan pekerjaan end-to-end. Artinya, fokus data scientist adalahmemahami konteks data yang akan diproses,memahami insight yang ingin dicapai, untuk kemudiandibuatkan data model-nya.
Proses data collection dan data extraction melibatkan teknologi sehingga tidak perlu dilakukan secara manual. Pada contoh sebelumnya (LogRhythm), fase tersebut ditangani oleh komponendata processor. Data scientist kemudian menganalisis metadata yang dihasilkan oleh data processor untuk dimasukan kedalam suatu model, output dari model tersebut adalah insight / intelligence yang dapat dijadikan referensiaction. Action-nya apa saja? Dalam ranah SOC, action bisa jadi berupa triggeralarm untuk ditampilkan dalam bentuk visualisasi dashboard untuk kemudian dianalisis oleh tim security analyst.
Action lainnya bisa jadi referensi untuk teknologi / tools DevSecOps. Ketika suatu model menghasilkan informasi bahwa telah terjadi serangan DDOS (Distributed Denial of Services) maka secara otomatis akan dilakukan perubahan konfigurasi padafirewall, misalnya. Ataupun ketika di indikasikan bahwa salah satu laptop staff telah terinfeksimalware, maka akan dilakukan mitigasi agar laptop tersebut tidak menginfeksi mesin lain dalam organisasi / perusahaan secara otomatis.
Intinya, model yang dibuat oleh seorang data scientist akan membantu tugas security analyst. Model tersebut dapat membantu mengidentifikasikan telah terjadinya insiden kemanan sehingga dapat dilakukan mitigasi secepatnya.
Jika kita melihat kembali video “Cyber Security, Evolved”-nya Deloitte, tentu kita memahami bahwa identifikasi serangan harus dilakukan dengan cepat dan harus tersedia 24/7. Dengan memilikitechnology,people, danprocessyang tepat maka kebutuhan tersebut dapat dipenuhi dengan baik.
Model yang dibuat oleh seorang data scientist juga akan terus berkembang seiring dengan kebutuhan organisasi / perusahaan karena aplikasi, behaviour, dan faktor-faktor lain yang terjadi pada masing-masing organisasi umumnya juga terus menerus mengalami perubahan.
Seperti itulah kira-kira posisi dan peran serta data scientist dalam dunia industri IT security. Tentunya peran serta data scientist tidak terbatas pada SOC saja, ada banyak contoh lain dimana peran data scientist dibutuhkan untuk memformulasikan suatu keputusan berdasarkan data-data yang tersedia.
Oh iya, untuk SOC itu sendiri tidak harus memiliki akses ke teknologi-teknologi mahal seperti QRadar ataupun LogRhythm agar dapat berinteraksi dengan fitur-fitur AI dan data science. Saat ini ada beberapa teknologi opensource yang telah mendukung fitur serupa, salah satu contohnya adalahApache Metron. Apache Metron saat ini masih terus menerus dikembangkan, source code-nya tersedia, sehingga siapapun bisa ikut berkontribusi. Apabila ada organisasi yang memilikihuman resourcememadai namun memiliki budget terbatas, maka bisa menggunakan Apache Metron sebagai teknologi SIEM dimana didalamnya terdapat komponen bigdata (hortonworks), ETL, Machine Learning, dsb.
Memang belum stabil seperti teknologi yang dikemas oleh vendor-vendor popular lainnya, namun setidaknyadata engineer,data anlyst,data scientist,machine learning engineer,security engineer,security analyst, danincident handlerdapat bekerja bersama-sama menjaga kemanan suatu organisasi.
Nah, dari semua role yang saya sebutkan diatas, bisa dilihat kan ya, jaman sekarang tidak perlu menjadi hacker / cracker terlebih dahulu untuk masuk kedalam dunia IT security.
Sebagai penutup, saya ingin memberikan referensi slide presentasi Jim Geovedi mengenaiMachine Learning for Cybersecurity. Ada banyak hal yang bisa kita pelajari dari slide tersebut.
So, jika profesi kamu sekarang adalah seorang data scientist, tidak perlu berpikir panjang ketika melihat informasi lowongan industri IT security membutuhkan skillset kamu, langsung apply saja, ya?! :)
Die Journale - Vanaf jouself, begin klein dingetjies, en van nou af.",https://miro.medium.com/v2/resize:fit:1200/1*ik-IqfhUfgsJ7tKb0FRYMw.png,"Soc, Data Science, Cybersecurity, Machine Learning"
https://medium.com/s/story/big-data-how-big-you-are-d10678b81bf5,"Big Data, How Big you are?",ProductGO,2017-11-09T07:51:45.550Z,"Setelahintegrated system ,penggiat teknologi kini harus mulai menyadari hal-hal yang mendasar yang lebih penting dari sekedar memberi kemudahan pada penggunanya hal-hal yang bernilai sesungguhnya telah mereka miliki hanya saja mereka tidak menyadari dan tidak tahu bagaimana proses menemukannya.
Seperti dunia pertambangan, sesuatu yang bernilai tidak hadir begitu saja, emas, minyak, mineral lainnya, perlu digali dan dicari. Begitupula dalam industri Technology yang kini mulai disadari pentingnya Big Data. Gua lebih suka menganalogikan Big Data adalah sebuah lahan yang sangat besar, dimana kita gak pernah tau apa sih yang sebenernya ada di dalamnya, sebelum kita benar-benar menggalinya mungkin bisa emas, mungkin bisa besi, tembaga dan lain sebagainya.
Gua kayak di bangunin dari mimpi gua yang hampir gak gua sentuh, jaman gua kuliah gua memulai tentang Big Data dalam memprediksi ramalan cuaca yang sumber datanya di ambil dari BMKG, berkutat dengan variable-variable pendukung, menyusun hipotesa, menguji hipotesa, mengukurnya dan berasumsi dengan hasilnya, ya betul, hanya sampai dengan asumsinya.
Lalu kecintaan gua terhadap data analisis gua terusin di tugas akhir, walau kagak bagus-bagus amat skripsi gua menjelaskan bagaimana mengolah data dengan statistik dan berasumsi berdasarkan hasil perhitungan yang telah di kumpulkan melalui survey.
Lalu beberapa tahun kemudian gua sempat bergabung dengan perusahaan yang mempunyai concern dengan Big Data dan AI, disana gua makin geregetan walau gua bukan team R&D , disana gua belajar banyak tentangdata visualization, ilmu yang gua dapet adalah jika lo gak bisa menjelaskan atau menyederhanakan hal-hal yang sulit kepada orang awam, berarti lo gak bener-bener tau. Ini salah satu keywordsnya si Albert Einstein.
Setelah itu gua terlibat dengan Big Data project KFC buat data forecasting product baru mereka, mengolah lebih dari 1juta record perhari berkutat dengan minimnya infrastruktur.
Gak sampe sana aja gua bergabung dengan digital creative agency, disana ilmu gua di tambahin lagi, gua belajar tentang Big Data untuk kebutuhan market validation dan juga gua belajar sekmetasi pasar, market behavior yang sumbernya tidak lain adalah BIG Data
Product Manager, Digital Product Innovation Enthusiast",https://miro.medium.com/v2/resize:fit:400/1*0A_FhZTTUOCf_X8h_mO6dw.jpeg,"Big Data, Data Scientist, Forecast, AI"
https://medium.com/s/story/pandas-time-series-analysis-d2471f18aa8d,Pandas Time Series Analysis,Rangga Rizky A,2018-03-15T21:58:07.572Z,"Dalam menganalisa data terkadang kita akan menemui data yang berbentuk Time-series. yaitu data yang menpunyai index berupa sebuah datetime. seperti finansial data, jumlah customer perbulan, jumlah produksi perbulan dan lain-lain. dari data tersebut kita bisa melakukan analisa untuk mendapatkan insight dari data tersebut. selain itu juga dapat melakukan prediksi kedepanya sehingga membantu dalam mengambil keputusan bisnis. dengan pandas kita bisa melakukan analisa time-series secara sederhana. mulai visualisasi hingga membuat moving average.
nah. untuk dapat melakukan analisa maka yang pertama dilakukan adalah membuat sebuah date menjadi index data tersebut. dengan pandas dapat melakukan dengan cara seperti dibawah ini.
Setelah itu kita juga dapat menampilkan secara periocly, misal ingin menampilkan nilai rata-rata pada tiap akhir tahun
Lalu kita juga bisa memvisualisasikan dalam bentuk sebuah plot
Selain itu kita dapat menampilkan pergerakan Moving Averagenya. Moving average sendiri merupakan garis yang didapat dari perhitungan harga sebelum hari ini, yang menghitung pergerakan harga rata-rata dari suatu nilai dalam suatu rentang waktu tertentu.
dengan menggunakan pandas maka melakukan analisa akan menjadi lebih mudah. untuk selanjutnya dapat dilakukan prediksi misalnya dapat menggunakan teknik ARIMA dan lain-lain.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:1200/0*ElFFu31Ud9ULdaGt.,"Data Science, Timeseries"
https://medium.com/s/story/berkenalan-dengan-chatbot-part-3-d267835dd4b4,Berkenalan dengan Chatbot (Part 3),Sofian Hadiwijaya,2018-10-17T02:20:06.153Z,"Stories from the people behind the code — the engineers and product designers ofKata.ai
Halo, teman Kata! Di post ini kita akan melanjutkan diskusi kita mengenai cara mengembangkanchatbotdi Kata Platform.Kemarin kita menutup post part 2dengan masalah yang mungkin kita hadapi saat mengembangkanchatbot. Bagaimana cara kita mengakali kata — kata yang sebetulnya memiliki makna yang sama, tapi tidak dikenalichatbotsehingga kita sepertinya harus membuatruleyang berbeda-beda untuk kata — kata yang sebenarnya memiliki maksud sama.
Untuk menghadapi hal ini, Kata Platform memiliki fitur yang namanya NLU (Natural Language Understanding). Fungsi dari fitur ini tentu untuk membuatchatbotyang kita kembangkan bisa memahami lebih baik kata — kata yang ditulis olehuserchatbot
Di sini kita akan coba membuat NLU untuk memecahkan kasus di akhirPart 2(Chatbottidak paham bahwa kata “Helo” sebenarnya ucapan salam, sehingga ia tidak memunculkan balasan yang seharusnya dikeluarkan saat user mengucapkan salam).
Pada Kata Platform, NLU ada di menu kiri, tepat di bawah Flows. Pertama kita buat NLU baru ((Create new NLU)).
Akan muncul tampilan sebagai berikut :
Ada beberapa bentuk yang dapat diolah oleh Kata Platform, Namun kali ini kita akan membahas tentang Keyword terlebih dahulu. Keywords ini sekelompok kata — kata kunci dengan makna sama yang bisa dikenali olehchatbot.
Di sini kita akan tambahkan beberapakeywordsyang menyatakan bahwa kata kata tersebut atau frase yang termasuk dalam kategori Salam. Kemudian ((Save NLUs)).
Selanjutnya, kita akan ubah Intent kita sebelumnya yang hanya dapat mengenali kata ‘hi’ menjadi lebih luas.
Pertama kita delete terlebih dahulu. Lalu untuk menambahkan NLU yang telah kita buat, kita klik tombol ((Add classifier)).
Kemudian pilih intentNLU dan match terhadap greetingKey, sesuai dengan nama NLU dan Keywords sebelumnya.
Dan mari kita coba di emulator kita.
Yes, problem solved.
Kata Platform juga bisa menyimpan atribut yang dapat kita ambil dari pesan yang masuk. Misal setelah menjawab salam, si chatbot bisa saja menanyakan age sex and location (ASL). Kita cukup membuat sebuah intent dan action setelah state greetingHandling serta aslHanding. Kurang lebih tampilan State akan seperti ini.
Dan kita juga dapat menggunakan NLU untuk mengekstrak atribut yang kita inginkan dari sebuah pesan yang dikirmkan.(Kita akan bahas lebih detail di part berikutnya)
Nah sekarang mari kita coba deploy keTelegram. Semoga para pemirsa sudah terbiasa dengan messaging app telegram 😬. Pertama mari kita buat Bot kita dulu, dengan cara cari akunBotFather.
Tekan tombol start, kemudian ketik /newbot untuk membuat bot baru, masukkan nama dan username bot. Hasilnya BotFather akan membalas dengan pesan yang berisikan token untuk kita gunakan dalam Platform Kata.
Mari kita kembali ke Kata Platform.
Sebelum kita deploy Bot kita ke telegram, marilah kita publish dulu versi terakhir yang sudah kita update.
Setelah selesai marilah kita ke menu Deployment dan kita Create Deployment
Klik ((View)), nah disini nanti kita bisa menambahkan berbagai channel untuk membuat bot kita dapat diakses di beberapa chat apps berbeda. Namun kali ini kita akan mencoba ke telegram terlebih dahulu. Untuk membuat channel, kita pilih Create Channel.
Klik View, nah disini nanti kita bisa menambahkan banyak channel ke messengger apps. Namun kali ini kita akan mencoba ke telegram terlebih dahulu. Untuk membuat channel kita Create Channel.
Beri nama channelnya agar lebih mudah untuk dikenali kemudian pilid Telegram, lalu isikan access token dengan token yang kita dapat sebelumnya dari BotFather. jangan lupa Add Channel.
Setelah selesai, mari kitaset webhook dari telegramke url webhook kata. Caranya gampang tinggal buka browser kemudian ikutin format berikut
Jika sukses kurang lebih tampilannya seperti diatas. Webhook was set!!!
Mari kita coba bot kita.
Nah, sekarangchatbotbuatan kita sudah bisa diajak ngobrol di Telegram.
Untuk part selanjutnya, kita akan coba buat contoh flowchatsederhana sesuai dengan request dari pembaca blog ini. Apabila ada teman — teman yang punya saran mengenai flowchatapa yang ingin kita bahas, silakancommentdi bawah ya!
Sampai ketemu di Part 4 artikel kita
Stories from the people behind the code — the engineers and product designers ofKata.ai
AI Enthusiast, Data Enthusiast, Technology Evangelist, Robotic Enthusiast, Public speaker, Mentor, Doing epic shit!http://id.linkedin.com/in/sofianhw",https://miro.medium.com/v2/resize:fit:1200/1*uQ31wAmzeUfq1ljwtN2w5Q.jpeg,"Bots, Artificial Intelligence, NLP, Chatbots, Machine Learning"
https://medium.com/s/story/intro-to-descriptive-statistic-memahami-tipe-data-d3555757f9eb,Intro to Descriptive Statistic : Memahami Tipe Data,Rangga Rizky A,2018-05-18T02:21:33.916Z,"Sebelum melakukan analisa pada data ataupun membangun sebuah model machine learning tentunya penting untuk kita mengetahui berbagai macam tipe data pada dunia nyata. setiap data memiliki measurement level sendiri-sendiri dan juga memiliki cara yang berbeda untuk melakukan visualisasi maupun pengolahanya.
secara umum tipe data dibagi dua kategorikal dan numerikal. untuk kategorikal sendiri merupakan data yang terpisah secara kategori. contohnya kelas pada pesawat ekonomi, bisnis dan first class. data sedangkan untuk numerikal merupakan data yang berbentuk angka numerik. untuk numerikal sendiri dibagi menjadi diskrit dan kontinu. untuk diskrit contohnya adalah nilai ujian. yang membedakan diskrit dan kontinu adalah bahwa pada data kontinu nilainya berupa infinite tak peduli beberapa banyak angka dibelakang koma. contohnya dalah berat badan. nilai berat badan sebenarnya tidak diskrit. misalnya berat seseorang 60 kg bukan berarti berat sebenarnya 60 kg tetapi mungkin bisa jadi 60.9897979 kg.
untuk tingkat pengukuran sendiri dibagi menjadi dua macam kualitatif dan kuantitatif. kualitatif sendiri digunakan untuk tipe data kategorikal dan kuantitatif untuk tipe data numeriak. pada kualitatif dibagi menjadi nominal dan ordinal. nominal untuk kategori data yang berdiri sendiri contohnya hujan , cerah ,berawan sedangkan ordinal untuk kategori yang memiliki urutan seperti rendah , sedang dan tinggi. sedangkan untuk kuantitatif terdiri dari interval dan ratio. yang membedakan adalah ratio memilii 0 true value jadi jika nilainya 0 berarti memang 0 contohnya tinggi badan. 0 berarti tidak memiliki tinggi. untuk interval contohnya adalah suhu ruangan. pada suhu 0 derajat bukan berarti titik terendah. pada skala celcius titik terendahnya -273.15.
untuk data kategorikal sendiri untuk visualisasi frekuensi umumnya menggunakan bar plot.
Sedangkan untuk data numerik visualisasi frekuensi dapat menggunakan histogram
dengan mengetahui tipe data kita dapat membangun model yang tepat. dan dapat menentukan langkah selanjutnya untuk melakukan analisa data.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:891/1*odq3Y8A5ml1xvYEQzAg1VQ.jpeg,"Data Science, Statistics"
https://medium.com/s/story/what-is-data-science-about-d3bec4f92b18,What is Data Science About?,AC,2018-05-05T15:04:40.806Z,"Tulisan ini tentang perjalanan karir selama setahun di dunia perdataan. di titik ini gue sering berpikir apa yang selama ini gue kerjakan, bermanfaat atau tidak, apakah ini yang gue inginkan atau citakan, apakah gue sudah tidak mencari zona nyaman lagi?
kebayang banget waktu dulu masih jadi programmer level receh, sehabis pulang kerja malah utak atik R Studio, bikin segala plotwithout meaningbikin ratusan model dan features buat akurasi, hingga akhirnya lupa gimana cara bikin module login website -____-
ada kesempatan dimana judulnya data scientist tapi bukan data scientist, cuma dapetin mean aja meaningful banget .__. padahal waktu skripsian on fire banget ngejelasin ke manager buat strategy businessnya. tapi waktu kerjaan kemarin kemarin ini tidak berasa valuenya. in a nutshell, data science is not about model, statistics, visualization yang canggih bener, bukan fancy model machine learning, tapi sebuah value yang dihasilkan dari data. apa yang bisa didapat dari data, strategy apa yang bisa membuat lebih baik dengan banyaknya data. tentu yang saya maksud bukan hanya statistika deskriptif belaka, kejadian dibeberapa level managerial cuma minta mean, median dan modus, sedangkan statistik inferensialnya ntah berantah.
memang, untuk membuat strategy ataudecision makingyang lebih baik bukan berarti harus membuat model yangngejelimet,ada kok cerita tentang revenue perusahaan tidak bertambah hanya karena karyawannya lupa pasang sticker sale di pintu.
sering kali waktu kumpul dengan teman teman data, kita hanya fokus dengan hal hal yang menurut kita krusial tapi bagi orang lain ga penting, seperti koefisien, R Squared, dll. butwhat does it mean for us as business people.sering kali kepikiran kalo ada beberapa variable signifikan terhadap varibel prediksi, terus kenapa? kalo korelasinya tinggi terus kenapa? hal-hal yang seperti ini yang terlewatkan saat kita fokus ke MSE atau R-Square.
setahun belakangan ini juga banyak banget baca buku yang judulnya ada data sciencenya, dibuku buku tersebut banyak bahas data preparation, data cleansing, data transformation, modeling, evaluation, sampai deployment. sedikit sekali penjelasan bagaimana model tersebut dipakai. padahal data science bukan hanya tentang machine learning, tapi juga analisis yang mendalam dari model tersebut yang pada akhirnya bisa memprediksi apa yang akan terjadi di masa yang akan datang.
buku buku yang saya sebut diatas adaintroducing data science by Davy Cielen et al, Data Science and Big Data Analytics by David Dietrich et al, Agile Data Science by Russell Jurney,dan masih banyak lagi. kebanyakan buku buku itu lebih banyak bahas teknikal bagaimana menghandle missing values, interpretasi model evaluasi, dan sebagainya, begitu juga textbook, lebih menjelaskan kepada konsep algoritmanya.
seringkali diawal awal belajar data analysis, problem definition itu sangat penting, bagaimana kita membuat limitation dan definition disetiap masalah yang ingin kita selesaikan, membuat metrics atau index baru dari beberapa variabel, sehingga dapat membuat model yang lebih kompleks, hal hal seperti ini yang sering kali terlewatkan.
memang model regresi berbeda dengan model machine learning, di machine learning tidak ada koefisien tapi feature importance, namun tidak bisa ditreat sebagai koefisien karena perhitungannya seberapa banyak model tersebut melewati feature tersebut yang banyak terjadi di decision tree.
pada akhirnya model tinggalah model jika tidak memberikan manfaat kepada business strategy atau user experience. hal yang membuat saya takjub sewaktu mengerjakan skripsi, algortima apriori yang sederhana itu hasilnya bisa diinterpretasikan bermacam macam, hasilnya bisa kita buat untuk strategy product bundling, product A dan B, selain itu masalah user experience juga buku buku yang kategori A bisa berdekatan dengan B sehingga penggunjung tidak perlu mencari jauh jauh dan memudahkan pengunjung, ada lagi waktu gue membuat skripsi ini bisa membantu menjual barang barang yang tidak laku dengan paket bundling itu. pada akhirnya membantu perusahaan dalam strategy businessnya.
belajar model dan teknikal itu penting, tapi interpretasi hasilnya juga penting, dan bagaimana kita sebagai orang analyst bisa membantu membuat keputusan yang lebih baik. setelah model evaluation selesai dan mendapatkan akurasi yang tinggi balik lagi kepertanyaanwhat does it mean for us as business people",,"Data Science, Career, Machine Learning, Business"
https://medium.com/s/story/belajar-data-science-memahami-layout-rstudio-d3d46f9f955c,Belajar Data Science : Memahami Layout RStudio,Amanda Pratama Putra,2018-05-25T09:36:09.260Z,"Pada story sebelumnya, saya telah sedikit mengenalkan tentang apa itu bahasa pemrograman R serta menjelaskan langkah-langkah instalasinya. Juga, saya sudah merekomendasikan penggunaan RStudio untuk memudahkan proses coding. Bagi yang belum membaca,storytersebut bisa dikunjungi melalui link dibawah ini:
Kali ini, saya akan menjelaskan tentang fitur-fitur dan layout dari tampilan aplikasi RStudio. Dengan memahami fungsi dan fitur, RStudio dapat digunakan dengan lebih mudah dan efektif.
RStudio merupakanintegrated development environment (IDE)khusus bagi bahasa pemrograman R. Software ini menyediakanR console,code editordengansyntax highlighting,code completiondandirect execution,environment,history, connections, dan fitur-fitur tambahan lainnya sepertifile manager,packages manager, help, plot viewer, hinggaproject versioningmenggunakan git. RStudio sebenarnya memiliki dua versi, yaituopen source(gratis) dancommercial edition(berbayar). RStudio juga tidak hanya terbatas dalam bentuk aplikasi dekstop, melainkan terdapat versi RStudio Server, yaitu RStudio yang dapat diakses melalui browser yang terhubung dengan suatu jaringan komputer. Untuk saat ini, versi RStudio yang akan dijelaskan hanyalah RStudio open source berbasis dekstop saja.
Agar kita dapat menggunakan IDE secara tepat dan mudah, maka sebaiknya kita harus memahami fungsi dari panel-panel yang ditampilkan pada IDE. Pada dasarnya, layout RStudio terbagi kedalam empat (4) bagian, yaitu:Source, Console, Environment/History/Connections,fitur lainnya. Gambaran umum untuk masing-masing panel dijelaskan sebagai berikut.
Source/Editor window adalah jendela yang dapat digunakan untuk membuat, mengedit, dan menyimpanscriptR. Pada jendela ini, tersedia fitur autocomplete yang akan memudahkan kita dalam membuatscript. Apabila jendela tersebut tidak muncul saat pertama kali menginstall RStudio, langkah yang dapat dilakukan adalah sebagai berikut:
Klik tab File -> New File -> R script.
Console window adalah jendela tempat mengeksekusi command dari script R yang dibuat. Script tersebut dapat dijalankan dari Source/Editor window maupun dengan langsung mengetikkan script pada jendela ini. Jendela ini akan menampilkan output dari setiap satu proses/baris command R yang dijalankan. Jendela ini sebetulnya merupakan tampilan langsung dari setiap proses yang dikerjakan oleh R.
Jendela ini terdiri dari beberapa tab, yaitu Environment, History, dan Connections. Tab environment akan menampilkan daftar data dan nilai yang sedang aktif tersimpan didalammemory(RAM). Kita dapat melihat data atau nilai tersebut dengan mengklik nama data tersebut. Tab History akan menampilkan daftar command yang telah dijalankan sebelumnya dalam satusessionyang aktif. Adapun tab Connection merupakan tab khusus yang berkaitan dengan koneksi ke database seperti mySQL, postgreSQL, Spark, dll.
Jendela ini merupakan fitur tambahan yang bermanfaat khususnya dalam manajemen file, menampilkan outputcommandberupa plot, informasi dan bantuan dalam penulisanscript, dan web viewer. Khusus pada tab Packages, kita dapat melihat daftarlibraryR yang telah terinstall pada PC kita. Apabila kita ingin menambahkanlibrarybaru, cukup dengan menekan tombol install, kemudian mengetikkan nama dari library yang ingin diinstal.
Demikianlah penjelasan dari layout aplikasi RStudio. Masing-masing jendela memiliki fungsi yang berbeda-beda, dengan mengetahui fungsi dan penggunaannya, maka prosescodingmenggunakan R akan menjadi jauh lebih mudah dan praktis. Selain itu, RStudio juga menyediakan fitur manajemen Project yang dapat membantu kita untuk menyusun script dengan rapi dan dapat diakses dari PC manapun, khususnya ketika sedang mengerjakan suatuprojectpengolahan data yang cukup kompleks dan membutuhkan waktu yang cukup lama.
Untuk pembahasan selanjutnya, kita akan membahas tips & trik menggunakan RStudio, meliputi:
Data Scientist | Artificial Intelligence | E-commerce | Consultant",https://miro.medium.com/v2/resize:fit:1200/1*7EwYr95ImX01VXZvmO2TzQ.png,"Belajar, Rstudio, Belajar R, Data Science, R Programming"
https://medium.com/s/story/simple-linear-regression-teori-d4abebd1ade2,Simple Linear Regression: Teori,R. Kukuh,2018-02-16T18:42:51.212Z,"Dalamtutorial sebelumnyaAnda telah men-download dataset yang dibutuhkan, sekaligus paham business problem yang akan dibuatkan model Machine Learning-nya.
Sebelum kita masuk pada fase coding, akan saya jelaskan sedikit tentang teori Simple Linear Regression ini.
Berikut adalah rumus dari Simple Linear Regression:
Mari kita bahas satu per satu:
Jika susah memahami Simple Linear Regression dalam bentuk rumus, semoga dalam bentuk grafik ini bisa memudahkan Anda.
Gambar di atas adalah bentuk grafik Simple Linear Regression atas suatu kasus bertema: “Perbandingan Besar Gaji Terhadap Lama Kerja”
Penjelasannya sebagai berikut:
Berikut akan dijelaskan apa dan bagaimana trend line atau best fitting line pada Simple Linear Regression ini.
Masih menggunakan kasus dengan tema yang sama seperti diatas, yaitu: “Perbandingan Besar Gaji Terhadap Lama Kerja”.
Mari saya jelaskan gambar di atas:
Dari penjelasan diatas, maka sebenarnya trend line ini adalah model Machine Learning dari Simple Linear Regression itu sendiri 😄
Atribusi Anda berupaclapping,sharing, ngasihkomentar, danfollowingblog ini dapat menimbulkan efek samping berupa semangat menulis yang menggebu bagi saya
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*7tcfpzsXbrxROhVCB8BkBA.png,"Regression, Data Science, Machine Learning, Simple Linear Regression"
https://medium.com/s/story/big-data-dengan-hadoop-membuat-aplikasi-untuk-hadoop-part-4-dd445a71a46c,Big Data dengan Hadoop (Membuat Aplikasi untuk Hadoop) — Part #5,Farhan,2018-05-24T18:58:35.900Z,"Setelah berhasil meletakan file di HDFSdirectoryselanjutnya adalah membuat aplikasi sederhana untuk mengerjakan Job di Hadoop yang kali ini saya beri nama WordCount. Tanpa bertele-tele langsung saja berikut langkah-langkahnya:
Setelah pembuatan aplikasi selesai selanjutnya akan saya praktekan bagaimana menjalankan Job atau aplikasi yang sudah kita buat ini di Hadoop.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:1200/1*EN1mTXsaKV4Ibwe4Hss6KQ.png,"Big Data, Hadoop, Cloudera, Data Science, Hdfs"
https://medium.com/s/story/aplikasi-machine-learning-dalam-media-digital-ddce1f8a092f,Aplikasi Machine Learning dalam Media Digital,AC,2021-01-22T07:11:01.377Z,"Artikel tentang seluruh yang berkaitan dengan data mulai dari machine learning, data analisis, data engineering, data science, business intelligence
Machine learning adalah salah satu disiplin ilmu yang saat ini banyak digemari oleh kalangan saintek, menurut salah satu orangAmerican Computer ScientistMachine learning adalah
A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. — Tom Mitchell
Machine learning digunakan untuk memprediksi klasifikasi atau memprediksi pengkelompokan berdasarkan atributnya, saat ini di Indonesia machine learning banyak diterapkan pada industrie-commerce dan fintech.Dalam industri media digital yang memproduksi konten artikel, gambar dan video belum banyak diterapkannya machine learning untuk memproduksi konten, sedangkan machine learning bisa diterapkan keberbagai industri darie-commercesampaimedical.alasan mengapa media digital menerapkan machine learning adalah untuk memahami kebutuhan pembacanya, … read moreAplikasi Machine Learning dalam Media Digital — Data Folks Indonesia (datafolksid.xyz)
Artikel tentang seluruh yang berkaitan dengan data mulai dari machine learning, data analisis, data engineering, data science, business intelligence",https://miro.medium.com/v2/resize:fit:1024/1*ZkZS46p7Lbw-PDBtPMfEEw.jpeg,"Digital Media, Supervised Learning, Content, Data Science, Machine Learning"
https://medium.com/s/story/big-data-dengan-hadoop-memasukan-file-ke-hdfs-part-4-dfdc628186ae,Big Data dengan Hadoop (Memasukan File ke HDFS) — Part #4,Farhan,2018-05-24T16:34:40.133Z,"Setelah berhasil menginstall Hadoop menggunakanpackageCloudera dicatatan sebelumnya, kali ini saya akan mempraktekan bagaimanamengakses dan meletakan file di Hadoop.
Seperti yang kita ketahui, ada bagian bernamaApplicationyang berfungsi memberiJobkepada Hadoop, lalu adaMasterdanSlave.SeorangBig Datadeveloper membuat sebuahapplication,application sendiri sebenarnya sebuahprogramyang dibuat dengan bahasa pemograman Java yang berisiSet of instructionsuntuk memproses suatu dataset. Ketika application atau program yang dibuat oleh Big Data developer tersebut dieksekusi ini disebut sebagaiJob. Kemudian Hadoop memprosesnya secaraparalelataudistributedmenggunakan mekanisme Master dan Slave.
Untuk lebih jelasnya berikut langsung saja saya praktekan bagaimana cara mengakses HDFS dan meletakan file di HDFS:
Setelah file sudah ada di HDFS selanjutnya adalah diproses dengan aplikasi. Di catatan selanjutnya akan saya praktekan bagaimana membuat aplikasi dan menjalankan Job sederhana menggunakan bahasa pemograman yang digunakan Hadoop yaitu Java.
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:1200/1*7uGiNvqaUH5yN8CgjAc4ZQ.png,"Big Data, Hadoop, Cloudera, Data Science, Hdfs"
https://medium.com/s/story/memperkenalkan-connect-job-ceo-yoni-assouline-e264f9dc19e7,Memperkenalkan Connect Job CEO: Yoni Assouline,Unggah Rizki,2018-01-07T09:14:39.984Z,"Teknologi otomatis seperti pengetahuan mesin dan robotika menjadi bagian integral dari kehidupan kita sehari-hari, mengambil alih fungsi yang secara tradisional dikaitkan dengan perwakilan manusia. Namun, terlepas dari kekhawatiran Manichean tentang tujuan otomasi ini, tidak dapat disangkal kemudahan yang mereka bawa ke fungsi mereka.
“Mesin dengan cepat menggantikan manusia” kata Yoni Assouline, salah satu pendiri dan CEO ConnectJob, “dan mereka menyelesaikan pekerjaan dengan harga murah”
Perdagangan AI dapat menghitung miliaran dolar perdagangan dan bertindak sesuai skenario yang paling sesuai dalam sepersekian detik tanpa “kecerdasan emosional” yang ditakuti dan melekat pada manusia. Robot produksi dapat merakit dan mengemas apa saja yang telah diprogramkan ke dalam protokol mereka tanpa pengawasan.
“Seperti yang telah saya katakan sebelumnya, manusia digantikan oleh mesin” Yoni mengulangi
Otomasi terikat untuk menghilangkan beberapa pekerjaan dalam dekade berikutnya dan tentunya akan sangat berpengaruh pada pasar tenaga kerja.
“Inilah sebabnya, misalnya, beberapa negara berbasis UE sedang mengerjakan ‘pendapatan universal untuk semua’. Tapi saya tidak yakin ini akan terjadi “
Sebagai kepala pengembangan pemasaran, yang bertanggung jawab atas penyebaran aplikasi di seluruh dunia dan memastikan aktivitas bisnis sehari-hari, taman bermain Yoni Assouline adalah pasar tenaga kerja global. Pemahamannya yang rumit tentang dinamika pasar dan wawasan unik tentang bagaimana komponen individual saling berhadapan, telah memberi kepadanya kemampuan untuk melihat kesalahan dalam sistem.
“Nah, masalahnya dengan pasar tenaga kerja saat ini sangat kompleks dan terfragmentasi, tapi saat Anda memecahkannya, ada satu hal; faktor manusia “
Sebagai manusia, kita terus-menerus dibombardir oleh apa yang kita lihat, rasakan, sentuh, dan indra. Stimulus ini mendorong dan menarik kita ke segala arah. Keputusan kita didasarkan pada seberapa kuat dorongan atau tarikan tertentu.
Tidak begitu dengan mesin.
Mesin dapat mengambil dan memproses data lebih cepat dari kemampuan kita dan mampu belajar lebih cepat dari pada kita. Mereka tidak membutuhkan tidur atau nutrisi seperti yang kita lakukan, memungkinkan mereka bekerja sepanjang waktu. Kapasitas memori mereka yang terus berkembang memungkinkan mereka memanfaatkan informasi yang dibutuhkan secara instan. Siapa yang tidak mau menyewa karyawan seperti itu dan kapan itu terjadi ….
“Coba pikirkan. Anda bermain gitar dan Anda kehilangan pekerjaan di pabrik. Tapi Anda bisa mengajari musik. Bagaimana menemukan murid? Dan bagaimana murid akan menemukanmu? “
Meski mesin memiliki kelebihan otomasi, manusia tetap tetap unggul dari mesin dalam banyak hal. Sementara mesin dapat multitask dan menjalankan proses paralel tanpa kehilangan ketukan, mereka kurang memiliki penilaian, naluri, dan sifat kreatif yang membuat manusia sangat serbaguna. Mereka juga tidak memiliki kemampuan bawaan untuk menciptakan cara manusia melakukannya.
Tapi bagaimana kalau manusia dan mesin bisa hidup berdampingan? Bagaimana jika otomasi dapat membantu manusia menghilangkan “kecerdasan emosional” itu, yang memungkinkan kita membuat keputusan cerdas dan mengamankan pertunangan finansial.
“Kita perlu membantu orang untuk memonetisasi keterampilan mereka dan untuk mendapatkan kehidupan yang layak. ConnectJob dikembangkan untuk tujuan ini “
Dengan memanfaatkan teknologi blockchain dan otomasi smart contract, ConnectJob memfasilitasi gelombang baru keterlibatan layanan tanpa tanding.
“Konsepnya adalah untuk menghubungkan pencari job di seluruh dunia dan pencari layanan dan pencari layanan di lokasi mereka. Dengan menggunakan geolokasi, pengguna akan dapat menemukan Anda, untuk membaca tentang Anda, memesan layanan Anda dan membayarnya dengan menggunakan kriptourrency. Riff gitar yang mengagumkan yang telah Anda buat mungkin adalah semua yang Anda butuhkan untuk mencari nafkah. Bisakah mesin rusak seperti itu? “
ConnectJob menguji versi alfa platformnya di Odessa, Ukraina selama tiga bulan. Tes ini berfokus pada dinamika industri pariwisata yang sedang booming. Hasilnya sangat banyak. “Tes ini sangat menarik. Kami telah belajar banyak tentang apa yang telah kami lakukan dengan benar dan bagaimana memperbaiki diri. Kami sekarang 100% siap untuk memulai penyebaran global “
Dan titik nol untuk penyebaran global dimulai dengan penawaran pre-sale token ke publik. ConnectJob menawarkan token utilitas pelayanan terdesentralisasi, CJT, kepada publik pada 2400CJT per ETH. CJT menghilangkan masalah terbesar di pasar tenaga kerja global; pria tengah
“Menggunakan cryptocurrencies akan aman, cepat dan meningkatkan pengalaman pengguna”
Penawaran ConnectJob berjalan dari 12/01/2017 sampai 02/11/2018.
Untuk informasi lebih lanjut silahkan kunjungi:http://ico.connectjob.io/
Kontak Media:media@connectjob.io
Hassans International Law Firm, 57–63 Line Wall Road, Gibraltar / +44 203 936 2998",https://miro.medium.com/v2/resize:fit:1200/1*_rHYkpTXZn0xRcCMaMnZxw.jpeg,"Smart Contracts, Blockchain, AI, Geolocation, Machine Intelligence"
https://medium.com/s/story/love-food-hate-waste-delicia-memanfaatkan-teknologi-blockchain-untuk-menghilangkan-food-wastage-e37743a012,DELICIA — Memanfaatkan teknologi Blockchain Untuk Menghilangkan Pemborosan Makanan (Food Wastage),Aisyah Qenot,2018-05-17T18:40:29.429Z,"Assalamulaaikum semuanya…Hai-hai senang sekali dapat berbagi cerita dengan teman-teman semua. Dan saya sangat bersemangat memulai cerita saya pada hari ini yaitu Tentang memanfaatkan teknologi blockchain untuk menghilangkan food wastage. Penasaran kan apa itu food wastage dan bagaimana teknologi blockchain bisa menangani masalah tersebut..! Mari kita simak bersama penjelasannya check it out!!
Pemerintah di seluruh dunia bangun untuk masalah pemborosan makanan. Parlemen Perancis mengkriminalisasi membuang makanan oleh restoran dan toko kelontong dalam beberapa tahun terakhir, yang lain mengikuti. Meskipun mungkin ada yang akan diperbarui dalam memecahkan masalah ini, sarana dan prasarana tidak memadai. Oleh karena itu, setiap upaya untuk menghilangkan pemborosan makanan tidak mencapai hasil yang diinginkan. Meskipun niat terbaik semua orang, dunia masih menghabiskan 1,3 miliar ton makanan setiap tahun. Jika disalurkan kepada mereka yang membutuhkan, itu bisa menghapus kelaparan!
Delicia berencana melakukan hal itu. Kami sedang membangun teknologi yang akan membuat perubahan struktural pada cara rantai nilai Makanan saat ini bekerja. Jawabannya terletak pada AI, pelacakan lanjutan, perkiraan permintaan/penawaran dan di atas semua menghubungkan bisnis dan konsumen dalam rantai nilai makanan ini. Ini akan menghasilkan ekonomi yang lebih baik, efisiensi yang lebih, dan perutean ulang makanan berlebih yang paling dibutuhkan.
Menggunakan teknologi blockchain dan AI canggih, Delicia membuat koneksi penting antara para pemangku kepentingan dalam rantai nilai makanan. Melalui platform kami, bisnis mendorong penjualan makanan hampir kadaluwarsa dan transaksi lainnya di jaringan Delicia sementara pembeli potensial mendapatkan akses ke makanan berkualitas dengan harga diskon.
Dengan demikian, kami meningkatkan usaha kecil dengan memberi mereka cara yang lebih baik untuk memasarkan kepada calon pelanggan mereka di dekatnya. Dari sisi konsumen, kami memberi mereka akses ke inventaris bisnis yang berpartisipasi yang juga mencakup makanan berkualitas mendekati kedaluwarsa yang tersedia dengan diskon besar. Ini membantu mencegah bisnis dari membuang makanan ini. Tapi itu tidak hanya terbatas pada makanan yang kedaluwarsa. Delicia juga menyediakan bisnis cara untuk mengkomunikasikan transaksi reguler dan promosi mereka dan mencocokkannya dengan preferensi konsumen. Ini menang-menang.
Dengan memperbaiki rantai nilai makanan, platform kami membantu dunia bergerak lebih dekat untuk menghilangkan pemborosan makanan dengan memaksimalkan pengambilan makanan dengan kualitas hampir kadaluwarsa dengan menggunakan teknologi canggih AI dan teknologi Blockchain.
Platform Delicia tidak hanya jembatan hiper-lokal antara penjual dan pembeli makanan tetapi juga merampingkan rantai nilai makanan dengan menyediakan statistik akurat waktu nyata pada permintaan makanan, konsumsi dan pasokannya. Dilengkapi dengan pengetahuan ini, pemain di industri dapat memperkirakan pola permintaan dengan lebih baik, mengisi kesenjangan pasokan dan mengurangi faktor-faktor yang mengarah pada pemborosan makanan. Tujuan kami adalah untuk merevolusi bisnis makanan sedemikian rupa sehingga mengurangi pemborosan dan mempertahankan diri pada model ekonomi yang jauh lebih sehat. Yang tidak termasuk pemborosan sebagai produk sampingan.
Kami telah mulai merevolusi rantai nilai makanan dengan memanfaatkan kekuatan Blockchain dan AI. Ini bukan hanya tentang penyebab mulia penghapusan pemborosan makanan tetapi juga menghadirkan peluang ekonomi yang sangat besar, hingga $ 18.2 miliar. Peluang ekonomi ini akan terwujud hanya ketika pasokan makanan dan konsumsinya lebih sinkron. Itulah yang teknologi kami lakukan!
Tujuan utama DELICIA adalah untuk menciptakan jaringan pangan universal yang terdesentralisasi, untuk memaksimalkan penggunaan produk dan mengurangi kerugian makanan.
DELICIA menawarkan, secara real time, memantau kelebihan makanan di restoran dan toko. Dan mengalihkan makanan berlebih kepada orang-orang yang membutuhkan, dengan harga lebih murah.
Ekosistem ini akan didasarkan pada teknologi Blockchain. Aplikasi ini akan tersedia di seluruh dunia. Saat ini, tim sedang bekerja untuk menandai ekosistem ini menggunakan teknologi Ethereum berbasis teknologi dengan utilitas Token DFT multifungsi.
Kami sangat senang memperkenalkan tim kami kepada Anda. Tim Delicia dan tim Penasehat menggabungkan individu dari pengalaman industri jangka panjang dalam Teknologi Informasi bersama dengan pola pikir kewirausahaan yang berbeda. Inti dari visi tim adalah sentrisitas pengguna dan kesuksesan di setiap fitur yang tergabung dalam Delicia. Dengan keahlian ini, Delicia dapat memberikan makanan, inovasi dan nilai keuangan.
Berlangganan whitepaper kami hari ini di delicia.io untuk menjadi bagian dari revolusi pangan yang terdesentralisasi ini.Informasi selengkapnya kunjungi kami :
Website||Whitepaper||Facebook||Twitter||Reddit||Medium||ANN BTT||Telegram||Telegram announcement||
0x896bdd4564C5C12A45452bE6D8cC6E4b15F1c78F",https://miro.medium.com/v2/resize:fit:1200/0*MmCAXQc0BnXVaw2x.,"Cryptocurrency, Ethereum, AI, ICO, Blockchain"
https://medium.com/s/story/kebun-kurma-sleman-telp-wa-0822-4069-7469-kebun-kavling-kurma-sleman-e4e12cc3408b,"Kebun Kurma Sleman, Telp / Wa 0822–4069–7469, Kebun Kavling Kurma Sleman",basirul akbar,2018-08-14T23:06:46.802Z,"Hari ini banyak sekali hal yang bisa Anda temui di berbagai macam berita, baik di dunia maya maupun media cetak hingga elektronik lainnya. Salah satu bahan pemberitaan yang tidak habis — habisnya ini adalah tentang investasi.
Coba saja Anda lihat di televisi, pasti familiar sekali dengan acara yang mempromosikan investasi property yang dikelola oleh developer properti terkenal. Di berita lainnya seperti media cetak atau berita di internet, berbagai macam tawarantempat investasi terbaikpun bermunculan menawarkan kelebihannya masing — masing.
Memang benar adanya jika investasi menjadi sebuah kepentingan saat ini mengingat persaingan ekonomi yang begitu berat, kemajuan teknologi setiap hari yang juga berdampak pada segala macam kebutuahn hidup yang terus meningkat sementara tidak akan selamanya Anda bekerja, pun tidak akan selamanya kita memimpin sebuah perusahaan atau membuat sebuah usaha.
bersama di Kavling Taman Kurma.
Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Anda pun tidak akan rugi jika beli kavling diKavling Taman Kurmakarena harga tanah selalu naik dan ini akan jadi investasi yang terbaik untuk Anda.
Tempat Investasi Terbaik adalah Investasi Properti Syariah
Sebagai seorang muslim, segala macam kegiatan khususnya dalam kegiatan perekonomian harus dilakukan sesuai dengan ketentuan hukum islam. Hal yang sama juga berlaku saat Anda akan mencari sebuah investasi masa depan.
Carilah investasi berbasis syariah yang bebas riba dan semua tata cara pelaksanaan dilakukan sesuai dengan syariat Islam.Kavling Taman Kurmasebagai salah satu contoh investasi syariah paling baik karena sudah pasti bebas riba, DP dan Angsuran juga tidak akan dibuat ribet, dan juga sudah memilikilegalitas.
Dengan memilih investasi berbasis syariah, maka hasil yang akan Anda dapat pun sudah pasti terjamin halal. Dengan demikian, kita juga akan hidup jauh lebih tenang di masa depan bersama anak cucu Anda.
Selagi masih muda dan memiliki penghasilan cukup dari bekerja, mulailai untuk sisihkan sedikit demi sedikit untuk kita putarkan ditempat investasi terbaikseperti Membeli Kavling Taman Kurma.
Dengan mulai berpikir untuk melakukan investasi, artinya Anda sudah peduli dengan kehidupan di masa depan terutama juga untuk kelangsungan Hidup anak, istri, hingga cucu dan buyut Anda di masa depan.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:1200/1*SUlGwscgBB0i_ljv8BTYbg.jpeg,"Management, Marketing, Machine Learning, Maintenance, Marriage"
https://medium.com/s/story/simple-linear-regression-python-e541ed030e40,Simple Linear Regression: Python,R. Kukuh,2018-02-17T18:09:29.955Z,"Setelah kita tahuteoridibalik Simple Linear Regression ini, sekarang waktunya untuk coding! Finally!
Pada bagian ini mari kita coding pembuatan model Machine Learning menggunakan rumus Simple Linear Regression, dalam Python 🐍.
Buka Anaconda, lalu buka Spyder
Gunakantemplate berisi kode untuk Data Processing yang sudah kita buat pada seri tutorial sebelumnya.
Ubah beberapa bagiannya, seperti ini:
Eksekusi bagian kode diatas
Akan muncul 3 variabel baru dalam window Variable explorer:
Ketiga variabel tersebut adalah: (1) dataset: berisi keseluruhan data, (2) X: berisi data sebagai independent variable, dan (3) y: berisi data sebagai dependent variable.
Lanjutkan template kode tersebut, ubah dibagian ini:
Eksekusi kode tersebut, maka akan muncul variabel baru lagi
Beberapa variabel baru yang muncul adalah: X_train, X_test, y_train, dan y_test.
Sampai disini selesailah proses pembuatan variabel-variabel yang kita butuhkan untuk proses selanjutnya.
Catatan: Pada data preprocessing template memang ada langkah selanjutnya untuk melakukan feature scaling. Namun pada kasus ini, library yang akan kita gunakan untuk melakukan fitting (dibahas di bawah ini) tidak memerlukannya.
Tambahkan kode berikut:
Eksekusi kode tersebut
Demikianlah kita sudah melakukan fitting Simple Linear Regression ini pada training set (X_train dan y_train), dan menciptakan model Machine Learning yang kita butuhkan.
Pro Tips: Tekan ⌘+i pada fungsi atau keyword apapun untuk kemudian memunculkan help / dokumentasi mengenai kegunaannya.
Tambahkan kode berikut, lalu eksekusi:
Buka isi dari y_pred dan y_test
Apa yang bisa kita baca dari keduanya?
Seperti yang sudah kita ketahui bahwa isi dariy_testadalah data asli daftar gaji yang diperoleh oleh para karyawan, sedangkan isi dariy_predadalah prediksi daftar gaji para karyawan yang seharusnya menurut model Machine Learning kita ini.
Coba bandingkan secara manual isi dari keduanya. Akan tampak bahwa data hasil prediksi tidak akan jauh berbeda dengan data asli. Ini membuktikan bahwa model Machine Learning kita ini berhasil memprediksi daftar gaji karyawan (yang seharusnya didapatkan oleh mereka).
Pada langkah berikutnya, akan saya tunjukkan grafik visualisasi model Machine Learning ini terhadap data aslinya.
Tujuan dari visualiasi ini adalah memudahkan kita melihat hasil prediksi model Machine Learning ini: bahwa hasil dari training-set dan test-set tidaklah jauh berbeda.
Tambahkan kode berikut:
Eksekusi kode diatas, jika sukses akan muncul suatu grafik seperti ini:
Cara membaca grafik diatas:
Ambil contoh titik merah yang berada pada proyeksi x-axis 4 tahun lama kerja. Menurut grafik, karyawan ini mendapat gaji sekitar $55k. Namun menurut model Machine Learning, karyawan ini seharusnya mendapat gaji sekitar $65k
Tambahkan kode berikut:
Eksekusi kode diatas, jika sukses akan muncul suatu grafik seperti ini:
Cara membaca grafik diatas:
Pada grafik ini jugalah terdapat prediksi Machine Learning terhadap kasus ini, yaitu: Berapa besar gaji yang seharusnya diterima oleh karyawan baru jika dia berada pada lama waktu kerja tertentu?
Catatan: Tentu saja tidak ada hasil prediksi yang 100% tepat, dan hal ini wajar. Dalam contoh diatas terlihat ada 2 proyeksi yang berada agak jauh dari trend line.
Atribusi Anda berupaclapping,sharing, ngasihkomentar, danfollowingblog ini dapat menimbulkan efek samping berupa semangat menulis yang menggebu bagi saya
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:870/1*76EgX9rVwEc5hNAxnkCk8A.png,"Regression, Simple Linear Regression, Python, Data Science, Machine Learning"
https://medium.com/s/story/sebuah-irdrop-e5d8b5c507f3,Tanpa Judul,Nuurur Azhari,2018-02-13T21:50:46.363Z,Sebuahirdrophttps://docs.google.com/forms/d/e/1FAIpQLSeowgMGxnfKEbVYxhgC1sbWHsi0WYWM2YEOk04NdoFI-XTeqw/viewform,,Artificial Intelligence
https://medium.com/s/story/kecerdasan-buatan-ai-dalam-lingkup-internal-audit-e737c80986b3,Kecerdasan Buatan (AI) Dalam Lingkup Internal Audit,Debrian R. Saragih,2017-11-28T17:32:34.483Z,"Mahasiswa Online merupakan media sharing pengetahuan antar mahasiswa dari berbagai universitas di Indonesia. Konsep yang ditawarkan yaitu berbagi bersama bahan kuliah yang sudah tidak dibaca lagi agar tidak hanya menumpuk di laptop atau komputer. Kunjungiwww.mahasiswaonline.com
Publikasi berkala dengan tema GPI (Global Perspectives and Insight) yang dikeluarkan oleh IIA baru-baru ini mengangkat suatu topik yang agak berbeda. Publikasi yang dimuat tanggal 12 Oktober 2017 ini mengangkat judul “Potensi untuk menerapkan AI dalam lingkup pekerjaan Internal Audit” (Artificial Intelligence — Considerations for the Profession of Internal Auditing).
https://global.theiia.org/knowledge/Public Documents/GPI-Artificial-Intelligence.pdf
Sebagai pembahasan pembuka dalam publikasi ini disebutkan bahwa implementasi kecerdasan buatan (AI) bisa diterapkan dalam berbagai bentuk diantaranyadeep learning,machine learning, image recognition, naturallanguage processing, cognitive computing, intelligence amplification, cognitive augmentation, machine augmented intelligence, and augmented intelligence.Macam-macam penggunaan AI lebih lanjut dapat Anda baca pada artikel sebelumnya mengenaiSistem Pakar untuk Audit Pengadaan.
Penerapan AI dalam dunia internal audit, harus dipahami secara kontekstual, artinya tidak semua kecerdasan buatan bisa diimplementasikan ke semua lingkup pekerjaan internal audit. AI sangat bergantung padabig datadan algoritma yang digunakan. Belakangan ini kita sering mendengar kalimatbig data.Jika kita pernah mengerjakan laporan keuangan dengan transaksi yang sangat banyak, itu belum masuk dalam kategori big data.
Big data means more than just large amounts of data — big data refers to data (information) that reaches such high volume, variety, velocity, and variability that organizations invest in system architectures, tools, and practices specifically designed to handle the data.
Dikatakan bahwa big data tidak hanya sekedar jumlah data yang besar, namun lebih menekankan pada informasi beserta variabelnya; volume, jenis data, kecepatan perubahan data, dan variabilitas data dimana perusahaan membutuhkan suatu sistem dan alat bantu yang spesifik untuk mengelola data tersebut.
Hubungan Big Data, Algoritma dan AI
Big data akan menjadi input dalam melakukan proses berpikir suatu mesin cerdas. Algoritma berfungsi sebagai metode mengolahbig datatadi sehingga bisa menghasilkan informasi yang bernilai tambah. Sedangkan AI adalah gabungan kedua proses tersebut, sehingga suatu mesin dapat dikatakan pintar karena bisa meniru cara manusia berpikir dan mengambil keputusan.
Pemanfaatan Big Data, Algoritma dalam Pengambilan Keputusan
Pembahasan berikut ini akan memberikan Anda pemahaman yang lebih nyata bagaimana Big data, dan algoritma bermanfaat dalam pengambilan keputusan. Big Data adalah sekumpulan data yang belum bisa memberikan informasi karena kategorinya masih tersebar dan tidak normal. Data yang tersebar ini harus kita olah menjadi cluster informasi yang memiliki persamaan.
Dengan menggunakan konsepMachine Learning, data yang sudah memiliki hubungan tadi dapat dilakukan proses regresi untuk membentuk suatu model statistik yang dapat dimanfaatkan untuk mencari nilai prediksi. Semakin besar data, maka hasil yang dicapai akan semakin akurat, untuk itulah big data menjadi sangat penting keberhasilan suatu model dalam membuat keputusan.
Dalam bahasa program algortima di atas dapat digambarkan sebagai berikut:
x_train = [1,2,3,4,5,6,7]
y_train = [1.5,2.4,3.7,4.8,5.2,6.9,7.3]
variablex_traindany_trainmerupakan variable dari hasil input big data. Variable ini dilakukan regresi untuk mendapatkanmodelyang paling sesuai.
Jika model regresi sudah didapat misal Y = 2 + 3X. Maka model ini dapat diaplikasikan untuk melakukan prediksi nilai dengan nilai variable x yang lain. Nah, kelebihan konsep machine learning ini nantinya lebih tinggi lagi, yaitu tanpa perlu memasukkan nilai X, kita sudah dapat memprediksi arah grafik regresi berikutnya, karena sistem sudah memiliki pengetahuan dan kepintaran sendiri.
Bagaimana Peluang Pemanfaatan AI dalam Internal Audit
Sampai saat ini penggunaan AI dalam lingkup internal audit dimanfaatkan dalam pengendalian risiko kegiatan intenal audit. Peran internal audit justru bukan pada sisi pemanfaatan teknologi AI, namun bagaimana menciptakan framework yang sesuai menghadapi perubahan metodologi kerja sesuai dengan perkembangan teknologi. Peran internal audit menghadapi kemajuan teknologi ini diharapkan dapat membantu institusi untuk mengevaluasi, memahami dan mengkomunikasikan level dimana AI memiliki dampak, baik positif maupun negatif untuk menciptakan nilai tambah bagi institusi dalam jangka pendek, menengah dan jangka panjang.
Internal audit setidaknya memiliki 5 area kritis yang terkait dengan aktifitas AI dalam suatu organisasi atau institusi:
Membangun AI Auditing Framework
Bayangkan apabila semua pekerjaan dalam kantor sudah menerapkan teknologi yang modern dengan AI didalamnya. Mulai dari hal terkecil, membuat spreadsheet, otomatisasi pekerjaan rutin, mail merge, mesin penjawab telpon otomatis, big data dalam mengelola data kepegawaian organisasi, dan lain-lain. Mau tidak mau kita akan memasuki dunia yang penuh dengan AI, sehingga sebagai internal auditor kita harus membangun suatu framework yang mengatur semua itu agar setiap aktifitas dapat dinilai dampaknya. Secara singkat, AI Auditing Framework terdiri dari 6 komponen yaitu
Penjelasan lebih rinci dapat Anda baca dalam publikasi tersebut. Semoga artikel ini memberikan pencerahan.
Mahasiswa Online merupakan media sharing pengetahuan antar mahasiswa dari berbagai universitas di Indonesia. Konsep yang ditawarkan yaitu berbagi bersama bahan kuliah yang sudah tidak dibaca lagi agar tidak hanya menumpuk di laptop atau komputer. Kunjungiwww.mahasiswaonline.com
I write code // IT enthusiast // web developer. Learning by practicing & googling.https://www.linkedin.com/in/debrian-ruhut-saragih/",https://miro.medium.com/v2/resize:fit:1024/1*JNB7YXq6bLJKN_WJOlDHDA.png,"Internal Audit, Artificial Intelligence, Iia, Kecerdasan Buatan, Machine Learning"
https://medium.com/s/story/bagaimana-sistem-rekomendasi-berkerja-e749dac64816,Bagaimana Sistem Rekomendasi Berkerja?,Rangga Rizky A,2018-09-28T10:18:24.308Z,"Pernah merasa penasaran, kenapa youtube bisa mengerti video apa yang kita suka? atau pernah penasaran mengapa bisa amazon merekomendasikan buku yang tepat untuk kita.After all magic behind them is just sequence of logic and statement. yah sistem rekomendasi merupakan million dolar investment. karena sistem rekomendasi sangat berdampak pada perilaku konsumen sehingga semakin tepat sistem rekomendasi konsumen juga akan semakin nyaman. nah berdasarkan featurnya terdapat dua macam tipe yaitu dengan content-based atau collaborative Filtering
Berdasarkan namanya, pada sistem rekomendasi content-based, sistem menggunakan attribut kontent untuk menentukan rekomendasi untuk mu. seperti genre,judul,author,cast dan lain-lain. untuk sistem yang masih kecil (belum punya banyak user) metode ini merupakan pilihan yang tepat. yang paling sederhana adalah menghitung kemiripan antar data film yang terakhir dilihat dengan data semua film pada database. untuk menghitung kemiripan ada berbagai cara mulaiecludian distancehinggacosine similarity
pada collaborative filtering attribut yang digunakan bukan konten tetapi user behaviour. contohnya kita merekomendasikan suatu item berdasarkan dari riwayat rating dari user tersebut maupun user lain.
Untuk melakukan rekomendasi dapat menggunakan item-based maupun user-based. yang dimaksud dari item-based yaitu mencari berdasarkan kemiripan berdasarkan suatu item. untuk langkah langkahnya seperti dibawah ini
pada item based kita berangkat dari item sebagai bari dan user sebagai kolom, kemudian hitung similaritynya setalah mendapat nilai similarit kita sorting descending, nah disini kita bisa improvisasi dengan melakukan filter pada film-film yang sudah pernah ditonton. lalu kita dapat memilih Top N atau kita dapat menggunakan threshold. misalnya menghilangkan item yang ratingnya dibawah 3. setelah kita mempunyai kandidat item langkah selanjutnya adalah scoring. setelah melakukan scoring kita jumlah semua score tiap item. dan dilakukan sorting untuk menampilkan berdasrkan kemiripan.
sedangkan pada user-based kurang lebih sama tetapi kita berangkat menggunakan matriks user sebagai baris dan film sebagai kolom.
lalu bagaimana cara melakukan evaluasi?. salah satu cara untuk mengevaluasi top N recommender system dapat menggunakan hit rate. Bagaimana mengukur hit rate?
2. pisahkan satu item dari daftar
3. gunakan daftar untuk sebagai data testing pada sistem rekomendasi d
4. Jika satu item yang dipisahkan tadi muncul pada hasil Top N it is a hit. If not, it’s not a hit
selain itu kita juga bisa mengujinya menggunakan A/B Testing
Sistem rekomendasi mempunyai banyak manfaat di sekitar kita. untuk membangun sistem rekomendasi yang bagus banyak hal yang perlu di perhatikan. dan masih ada banyak cara untuk improvisasi. diluarsana masih banyak berbagai cara seperti menggunakan deep learning atau menggunakan Matrix Factorization. untuk perusahaan seperti amazon dan youtube dapat memiliki hasil yang bagus karena mereka memiliki banyak data. bagaimana pun penilitian sistem rekomendasi belum berhenti hingga sekarang.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:866/1*WRPOvhjrU66RbZr9nEEIUw.png,"Data Science, Data Mining"
https://medium.com/s/story/jaminan-keamanan-wa-62-852-1630-6730-jual-kawat-duri-dan-kawat-silet-di-serdang-e7fb6e2fcb8c,Jaminan Keamanan!!! WA+62 852 1630 6730 Jual Kawat Duri Dan Kawat Silet Di Serdang,kawat Silet Duri,2018-02-17T06:08:40.612Z,"Anda mencari Jual Kawat Duri Dan Kawat Silet di Serdang, membutuhkan Jaminan Keamanan!!! Hubungi : Tel./WA +62 852 1630 6730. Kawat duri Paling banyak dipakai pada rumah tinggal (untuk mencegah orang jahat memanjat pagar), peternakan (mencegah predator masuk), padang rumput, jalan, atau proyek konstruksi untuk mengamankan property.
Jual Kawat Duri Dan Kawat Silet Di SerdangKami adalah SUPPLIER KONTRAKTOR INDONESIA. Kami menjual geotextile,jual geomembrane,jual geogrid,jual geobag,jual plastik cor,jual drainage cell,jual pipa hdpe,jual kawat bronjong,jual kawat tambang,jual kawat duri,jual kawat silet,jual kawat galvanized wire mesh,jual peredam suara,jual peredam panas,jual kayu lantai,jual kayu dinding,jual, jual wallpaper, jual stainless plate,jual hot dip galvanized coil,danjual aluminium plate.Untukharga seluruh produk tersebut, kita tentukan dan kita sepakati bersama saja. GAMPANGKAN!.
Hemmm…, apa ada kebingungan di dalam benak
Tidak usah bingung, karena itulah yang sebenarnya. Hal ini merupakan salah satu sensasi pelayanan yang akan anda dapatkan dari perusahaan kami.
Dengan begini, Anda tidak perlu menghabiskan waktu dan kesal menego penawaran harga dari pen jual geotextile lain, karena kita bisa langsung bersepakat untuk menentukan harga jual geotextile dan harga jual produk lainnya.
Jual Kawat Duri Dan Kawat Silet Di Serdang
Hemmm…, apa Anda masih bingung juga?, Pleaseee … nggak usah pake bingung lagi dong… hehehe…
Gini, semua itu berangkat dari prinsip bisnis yang kami pegang teguh di perusahaan kami, dan kami memegang teguh prinsip tersebut, yaitu :
PERTAMA, PEMBELI ADALAH SAUDARA
Prinsip pertama ; pembeli adalah saudara. Dengan pengertian bahwa kami akan berusaha untuk selalu menggembirakan Anda dalam segala hal. Terutama dalam hal mutu dan harga sebuah produk. Pada konteks pelayanan, mulai dari pengiriman penawaran harga, hingga pengiriman barang, kami pun berusaha sekuat tenaga untuk membuat Anda gembira. Ini merupakan sebuah kejujuran dan ketulusan kami yang notabene sebagai manusia biasa yang berharap, serta mengharapkan kebaikan dari orang lain. Seriusssss looo, buka ngecap… hehehe.
Saking seriusnya, kami merancang management pelayanan dalam sebuah modul yang berjudul SENSASI PELAYANAN yang MENGGEMBIRAKAN untuk setiap item pekerjaan pelayanan…
Hayooo … mau modulnya yaa buat perusahaan tempat Anda sekarang sedang bekerja, boleh aja siiihh, asallll .. hahahh… (sstt.. khusus bab ini, koling-kolingan aja kita). Kami siap berbagi kok. Kan Saudara.
KEDUA, MENJUNJUNG TINGGI KEMULIAAN MARTABAT MANUSIA
Prinsip kedua adalah menjunjung tinggi kemuliaan martabat manusia. Dengan prinsip tersebut maka pada aspek mutu atau spesifikasi produk, kami akan selalu sampaikan apa adanya, bukan ada apanya…hehehe. Tak ketinggalan untuk mutu dan spesifikasi geotextile serta mutu produk kontruksi kami yang lainnya. Bagi kami berbohong adalah suatu tindakan yang akan menghinakan martabat kami.
Boleh jadi dengan cara “utak-atik spesifikasi atau mutu” kami bisa men jual geotextile atau produk yang sedang Anda cari dengan harga lebih murah. Lalu kami memperoleh sejumlah keuntungan dari situ, yang boleh jadi angkanya lumayan besar. Namun kami yakin, ketika Anda mengetahui hal itu, pastilah Anda akan memprotes atau bahkan marah kepada kami. Selanjutnya, ketika hal itu diketahui oleh orang lain, sebut saja misalnya mereka merupakan kompetitor kami, pastilah hal itu akan menjadi celah untuk menjatuhkan eksistensi perusahaan kami. Itulah kenapa ada pepatah lama yang berbunyi, kalau usaha udah kagak jujur, nggak lama pasti ancuurr…
Seriuusss bangets yaah …!!! Doorr.. emang itu siiii prinsip yang kami pegang teguh, bukan hanya untuk menjalan bisnis seperti men jual geotextile dan produk kontruksi seperti yang Anda kenal saat ini. Prinsip-prinsip tersebut juga kami pegang teguh untuk kehidupan kami sehari-hari di rumah dan juga di pergaulan lainnya
Ssssttt.. ada lagi yang lebih ngeri, yaitu ketika rejeki yang kami dapat dari ketidak jujuran ini dimakan oleh orang tua kami, anak dan istri-istri kami … iiihhh ngerinya, terbayang oleh kami bagaikan menyuapkan belatung untuk makan minum orang tua dan anak sendiri … iiiihhh ,,, ngeri.. asli ngeri.
Eh lagiankan, Tuhan nggak pernah tidur untuk melihat dan mendengar setiap tindak tanduk semua hambanya. . Anda, Competitor, Orang tua, istri dan anak-anak kami boleh jadi tidak mengetahui kelakuan kami, namun Tuhan, selalu ada di dekat kami. Karena itu tidak ada pilihan bagi kami untuk jujur dalam segala hal. Kami menyakini, jujur adalah satu-satunya pilihan jalan yang dapat menjunjung tinggi kemuliaan martabat manusia.
Karena itu, berikan kesempatan kepada kami untuk memuliakan Anda dengan mengedepankan kejujuran dalam segala hal.
KETIGA :KEUNTUNGAN USAHA HANYALAH TITIPAN.
Yah… men jual geotextile atau pun produk kontruksi lainnya merupakan pilihan bisnis yang kami sadari akan mendatangkan keuntungan yang lumayan. Namun kami pastikan kepada Anda, keuntungan tersebut kami akan manfaat sebaik mungkin untuk membantu saudara-saudara kita (sesama manusia) agar dapat merasakan pendidikan yang mencerdaskan, menapaki jalan ekonomi yang mesejahterakan, serta memperoleh konsumsi yang menyehatkan.
Jadi … Men jual geotextile dan men jual produk kontruksi lainnya adalah cara kami pilih untuk bekerjasama dengan Anda untuk mendidik, melatih, dan memfasilitasi generasi penerus bangsa ini agar cerdas, kreatif, sehat, dan kuat. Sehingga mereka bisa tampil sebagai generasi yang tangguh untuk menghadapi kompetisi dunia yang semakin ketat dimasa yang akan datang
Kami siap melayani pemesanan skala besar maupun kecir, untuk seluruh wilayah indonsia Untuk contact follow up lebih bisa melalui : Bapak Adrian Tel./WA 085216306730 (TELKOMSEL) EMAIL adrianrenovadi@gmail.com
Originally published atjualkawatsiletditangerangselatan.blogspot.co.id.",https://miro.medium.com/v2/resize:fit:320/0*j_s0EK28Ed-npFXe.jpg,"Artist, Artificial Intelligence, Asia, Articles, Arsitektur"
https://medium.com/s/story/business-intelligence-qna-session-e87f7a1e8044,Business Intelligence — QnA Session,IYKRA,2018-08-16T03:55:41.094Z,"Building Future Capabilities
Dunia business intelligence memang sangat menarik untuk dibahas. Oleh karenanya, kami mengangkat sesi Tanya jawab seputar business intelligence bersama Data Science Indonesia dan kemudian dijawab oleh industry expert.
Dalam sesi ini, Budiman Rusly akan menjadi tokoh industry expert yang akan membahas pertanyaan yang terpilih. Saat ini, ia merupakanChief Data Analytic Officerdi salah satu perusahaan asuransi jiwa. Dengan pengalaman dan juga pengetahuannya, Pak Budi menjelaskan latar belakang mengenai Business Intelligence supaya kita lebih mudah mencerna setiap jawaban.
“Pertumbuhan yang sangat cepat didalam dunia data serta ditambah dengan semakin meningkatnya volume, kompleksitas dan variasinya, hal tersebut membuat data science memberikan sentuhan tersendiri terhadap dunia business intelligence. Data Science dan Business Intelligence fokus terhadap data itu sendiri dan keduanya mempunyai tujuan untuk memberikan keluaran dengan hasil terbaik dimasing-masing bidangnya. Selain itu juga, Kedua bidang ini saling berkaitan erat dan merupakan suatu rantai panjang dari sebuah siklus analisa data”
Supaya lebih mudah, beliau menampilkan grafis ini
Selain itu, ia juga menjelaskan definisi yang menjelaskan perbedaan dari Business Intelligence dan Data Science agar kita tidak terkecoh antara keduanya.
Kalau,Business Intelligenceitu kumpulan methodologi, proses, strategi dan teknologi yang digunakan oleh organisasi untuk keperluan transformasi data mentah menjadi sebuah informasi yang berharga. Dengan definisi tersebut maka ruang lingkup business intelligence terdiri dari manajemen data (data integrasi, data quality, data warehouse, OLAP, master-data management, Data Lake, report dan data visualisasi.
Gambar diatas bisa membantu menggambarkan definisi dari Business Intelligence (BI). Sedangkan,
Data Scienceadalah bidang yang menggunakan metode scientific, proses dan sistem algoritma untuk mendapatkan insight dari data. Data science menggunakan teknik dan teori dari bidang matematika, statistik , information science dan computer science.
Nah, jadi sampai sini jelas ya. Setelah memahami latar belakang dari BI dan juga perbedaannya dengan Data Science, mari kita bahas pertanyaan terpilih yang akan dijawab oleh Pak Budi.
Bagaimana kaitan antara Business Intelligence dan Data Science?Business Intelligence merupakan langkah awal dari perjalanan menuju penerapan data science dalam suatu organisasi.Business intelligence dibuat untuk membantu memetakan arus masuk dari berbagai macam data source, baik structured maupun unstructured, dimana data tersebut akan ditransformasi menjadi keluaran yang lebih baik dan siap untuk dikonsumsi untuk analisa bisnis. Kegiatan yang termasuk didalamnya adalah data scrubbing, data enhancement dan standarisasi value.
Data yang sudah matang akan dipakai akan digunakan sebagai input untuk exploratory data analysis dan pembentukan model & algoritma dalam prosess data science. (Untuk lebih memudahkan, silahkan lihat gambar no 1)
Apa saja yang menjadi cakupan dari bidang Business Intelligence?Ruang lingkup BI terdiri dari manajemen data. Termasuk data integrasi, Data Quality, Data warehouse, Online Analytical Processing atau OLAP, Master-Data Management, Data Lake, Report dan Data Visualisasi. (Silahlkan lihat gambar no 1 sebagai referensi)
Apa saja yang diperlukan untuk mempelajari Business Intelligence, bagaimana dan dimana kita bisa menjadi BI expert?Business Intelligence merupakan bidang yang cukup kompleks untuk digeluti namun bukan berarti tidak mungkin dilakukan. Seseorang harus memiliki “Bird-eye-view” terhadap keseluruhan arsitektur BI untuk bisa disebut sebagai BI Expert. Setidaknya ada empat keterampilan yang diperlukan, yaitu:1. Listening Skills untuk memahami proses bisnis serta memahami masalah apa yang hendak dipecahkan.2. Penjabaran konseptual dari setiap kebutuhan yang dituangkan melalui Database design, ETL-ELT (Extract, Transform, Loading) Design , Data Warehouse Design, Multidimensional Model Design, Data Visualization Design3. Technical Knowledge untuk implementasi dari semua yang sudah didesain dalam konseptual proses diatas dengan menggunakantoolsdan memahami setidaknya SQL (Structured Query Language)4. Mengaplikasikan best practice design dan implementasi
Apa saja tantangan yang harus dihadapi dalam menekuni BI?Tantangan utama adalah komitmen untuk mempelajari konsep dan teori BI dengan baik lalu mencoba menerapkannya dalam kegiatan sehari-hari. Hal ini dikarenakan beberapa konsep yang harus diserap memiliki tingkat kesulitan yang cukup tinggi dan abstrak dimana konsep tersebut harus diikuti dengan pengulangan secara terus menerus serta menerapkannya setiap hari. Setelah selesai menerapkannya,maka harus diuji kembali denganbest practice implementation and design.Tantangan lainnya adalah banyaknyatoolsyang harus digunakan untuk implementasi BI dimana setiaptoolsmemilikilearning curve timeyang berbeda serta kompleksitas tersendiri.
Untuk saat ini, itulah pertanyaan terpilih yang sudah dijawab oleh Industry Expert IYKRA. Pertanyaan lain yang terpilih akan kami sambung padablog postberikutnya ya.
Ilustrasi: Rizaldi Adiputra
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*U6D-LTFsKb6_RMIoYZLSZQ.png,"Business Intelligence, Expert, Technology, Data, Data Science"
https://medium.com/s/story/pengenalan-deep-learning-part-2-multilayer-perceptron-e8f98d625b09,Pengenalan Deep Learning Part 2 : Multilayer Perceptron,Samuel Sena,2018-03-19T15:14:56.950Z,"PadaPart 1kita sudah mengenal apa itu neural network, activation function dan sudah mencoba implementasi forward propagation untuk melakukan regresi terhadap fungsi linearf(x) = 3x + 2
Fungsi linear diatas adalah fungsi yang sangat simple sehingga dengan menggunakan 2 layer (Input dan Output) saja kita sudah bisa menyelesaikan permasalahan tersebut. Lalu bagaimana dengan fungsi non-linear? Tentu saja kita tidak bisa menggunakan arsitektur 2 layer tersebut.
Sehingga untuk non-linear regression kita membutuhkan setidaknya 3 layer neural network atau yang biasa disebut Multilayer Perceptron (MLP) atau Fully-Connected Layer dengan menggunakan non-linear activation function pada seluruh neuron di hidden layer.
Kita akan mencoba melakukan forward pass pada MLP masih dengan Numpy saja. Untuk contoh kasusnya adalah kita akan melakukan regresi untuk data yang sebenarnya adalah sebuah fungsi non-linear sebagai berikut:
Sedangkan arsitektur neural networknya terdiri dari :
Neural network diatas sudah saya train dan nanti kita akan melakukan forward pass terhadap weight dan bias yang sudah didapat pada saat training.
MethodforwardPassyang kita pakai di part sebelumnya akan dimodifikasi sedikit dengan menambahkan argument baru untuk memilih activation function.
Pada percobaan non-linear regression kali ini kita akan melakukan perdiksi nilai dari -2, 0dan 2. Output yang dihasilkan seharusnya adalah 3, 1, 3dan hasil prediksi adalah 2.96598907, 0.98707188 dan 3.00669343. Masih ada sedikit error tapi paling tidak hasil diatas menunjukkan bahwa MLP dapat melakukan regresi terhadap fungsi non-linear dengan cukup baik.
Pada part selanjutnya kita akan sama-sama coba untuk melakukan training pada neural network untuk mendapatkan weight dan bias yang optimal.
Dibawah ini adalah series Pengenalan Deep Learning yang bisa kamu ikuti :
Deep Reinforcement Learning Student",https://miro.medium.com/v2/resize:fit:1200/1*BVx4X8_eZQJMqwdfNDjLeg.jpeg,"Deep Learning, Artificial Intelligence, Machine Learning, Neural Networks"
https://medium.com/s/story/exploratory-data-analysis-kenali-datamu-sebelum-membangun-sebuah-model-e907d9dfea1d,Exploratory Data Analysis : Kenali Datamu sebelum membangun sebuah model,Rangga Rizky A,2018-03-08T12:55:09.441Z,"Agar dapat menemukan metode yang tepat membangun model machine learning kita harus mengetahui karateristik dari data yang akan kita analisa. dengan memahami sifat-sifat data yang diolah maka kita dapat membangun model yang tepat. sebelum menentukan algoritma apa yang digunakan ada baiknya kita mempelajari pola dari data tersebut. selain itu kita dapat mencoba menghitung besar korelasi fitur-fitur yang ada. dan mempertimbangkan fitur yang akan digunakan. dengan mengeksplorasi data kita juga dapat mendeteksi missing value dan noise data.
disini saya akan mencontohkan eksplorasi data pada data titanic. data titanic sendiri merupakan data supervised dari penumpang titanic dengan label survived atau tidak survived.
pertama tama akan dilakukan pengecekan missing value
Dari data diatas disimpulkan bahwa banyak mising value pada kolom cabin dan Age. lalu kemudian kita coba melihat distribusi dari Age
Lalu kita coba lihat distribusi Age pada Pclass
ternyata pada pclass1 rata2 umur di nilai 37 dan pclass2 di nilai 29 sedangkan pclass3 di nilai 24. insight tersebutlah yang akan digunakan untuk mengisi missing value pada Age. lalu bagaimana dengan Cabin? melihat dari banyaknya jumlah missing value sehingga yang akan kita lakukan adalah menghapus kolom kabin dari fitur.
Untuk selanjut dapat dilakukan eksplorasi untuk menilai suatu pengaruh suatu fitur terhadap label. dengan mengetahui karateristik data yang akan diolah akan dapat membantu kita untuk menentukan model yang tepat untuk melakukan analisa.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:1200/0*nZs0xrKZd2EB7RP4.,Data Science
https://medium.com/s/story/simple-linear-regression-r-ea42aed17c52,Simple Linear Regression: R,R. Kukuh,2018-02-17T18:19:46.905Z,"Hore! Selamat! Anda telah menyelesaikan pembuatan model Machine Learning menggunakanSimple Linear Regression dalam bahasa Python.
Kini waktunya R untuk beraksi!
Gunakantemplate berisi kode untuk Data Processing yang sudah kita buat pada seri tutorial sebelumnya.
Ubah beberapa bagiannya, seperti nama dataset yang di-load:
Eksekusi kode tersebut. Jika sukses akan muncul objek dataset yang bisa Anda lihat isinya:
Dari dataset diatas bisa kita tentukan bahwa kolomYearsExperienceadalah jenisindependent variable, sedangkan kolomSalaryadalah jenisdependent variable.
Goal kita adalah dapat memprediksi besar Salary (yang seharusnya diterima karyawan) berdasarkan informasi data YearsExperience asli hasil observasi.
Lanjutkan template kode tersebut. Ubah dibagian ini:
Eksekusi kode tersebut. Jika sukses, akan tercipta 2 objek baru: training_set dan test_set.
Sampai disini selesailah proses pembuatan objek-objek yang kita butuhkan untuk digunakan pada proses selanjutnya, yaitu:
Catatan: Pada data preprocessing template memang ada langkah selanjutnya untuk melakukan feature scaling. Namun pada kasus ini, library yang akan kita gunakan selanjutnya tidak memerlukannya.
Tambahkan kode berikut:
Pro Tips: Sekedar mengingatkan kembali, bahwa kita bisa melihat dokumentasi dari suatu fungsi dengan cara tekan F1 didekat fungsi tersebut.
Mari kita bahas satu per satu arguments dari fungsi lm() ini:
Eksekusi kode diatas. Jika sukses akan muncul seperti ini:
Isi (dengan format tampilan yang berbeda) dari objek regressor ini juga bisa dilihat dari console dengan cara seperti ini:
Dari beragam informasi yang muncul, mari kita fokus dibagian Coeficients-nya. Disini tidak hanya terdapat informasi tentang nilai coefisient dalam persamaan Simple Linear Regression ini, tapi juga tingkat kesignifikannya.
Perhatikan bahwa dataset kita mendapat 3 bintang (***), yang berarti bahwa variabel YearsExperience ini dinilai sangat signifikan (punya hubungan linear yang kuat) terhadap perubahan Salary.
Kita akan menggunakan fungsi predict() untuk memprediksi hasil observasi test-set ini.
Tambahkan kode berikut, lalu eksekusi.
Anda bisa melihat isi dari vektor y_pred ini melalui console
Untuk lebih memudahkan arti dari vektor y_pred ini, silahkan melihat kembali isi dari test_set:
Sekedar mengingatkan:
Maka tujuan dari proses ini adalah: Bagaimana Simple Linear Regression memprediksi Salary yang seharusnya (didapat oleh karyawan) berdasarkan model Machine Learning yang nanti terbentuk.
Gambar di atas adalah perbandingan antara data asli hasil observasi dalam test_set VS hasil prediksi dalam vektor y_pred.
Coba bandingkan secara manual isi dari keduanya. Akan tampak bahwa data hasil prediksi tidak jauh berbeda dengan data aslinya. Ini membuktikan bahwa model Machine Learning kita ini berhasil memprediksi daftar gaji karyawan (yang seharusnya didapatkan oleh mereka) pada suatu waktu lama kerja.
Dibagian ini akan dijelaskan cara membuat grafik dari hasil prediksi diatas, agar kita lebih mudah dalam memahami. Grafik yang dibuat akan ada 2 jenis:
Dari grafik-grafik itu kita akan mengetahui bahwa hasil prediksi yang sudah dibuat mendekati data asli, sehingga bisa dianggap kita sukses membuat model Machine Learning yang benar untuk kasus ini.
Hal pertama yang harus dilakukan adalah meng-install library ggplot2:
Tambahkan kode berikut:
Eksekusi kode tersebut. Jika benar, maka akan muncul grafik seperti ini:
Cara membaca grafik:
Ambil contoh titik merah yang berada pada proyeksi x-axis 5,1 tahun lama kerja. Menurut grafik, karyawan ini mendapat gaji sekitar $71. Namun menurut model Machine Learning, karyawan ini seharusnya mendapat gaji sekitar $74k.
Tambahkan kode berikut:
Eksekusi kode tersebut. Jika benar, maka akan muncul grafik seperti ini:
Cara membaca grafik diatas:
Pada grafik ini jugalah terdapat prediksi Machine Learning terhadap kasus ini, yaitu: Berapa besar gaji yang seharusnya diterima oleh karyawan baru jika dia berada pada lama waktu kerja tertentu?
Catatan: Tentu saja tidak ada hasil prediksi yang 100% tepat, dan hal ini wajar. Dalam contoh diatas terlihat ada 3–5 proyeksi yang berada agak jauh dari trend line.
Atribusi Anda berupaclapping,sharing, ngasihkomentar, danfollowingblog ini dapat menimbulkan efek samping berupa semangat menulis yang menggebu bagi saya
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*2l5LRyr-Pt9QVbrgOODK3g.png,"R, Regression, Simple Linear Regression, Data Science, Machine Learning"
https://medium.com/s/story/dont-panic-keep-it-organic-ea555de1dff8,Don’t Panic Keep It Organic,Akmal Muhammad,2018-08-27T16:25:57.794Z,"Nama lengkapnya adalah Muhammad Rizki Duwinanto dan panggilannya adalah Kak Rizki. Dia merupakan mahasiswa tingkat 4 yang sedang menyusun tugas akhirnya untuk wisuda di jurusan Teknik Informatika karena pada eraIndustry4.0 ini perkembangan ilmu Informatika sangat pesat sehingga menarik untuk dipelajari. Alumni SMAN 8 Jakarta ini masuk Divisi Manajemen Proyek Departemen Inkubator IT di HMIF dikarenakan ingin mengerjakan proyek agar bisa mengasah kemampuanngodingnyawalaupun magangnya bukan di Divisi ManPro, singkatan Manajemen Proyek, melainkan di Divisi Kaderisasi. Kakak yang mensyaratkan kita yang mewawancara agar tahu isu kampus ini juga mengikuti salah satu unit yang bergerak di bidang dokumentasi dan media yaitu Liga Film Mahasiswa yang biasa disebut LFM. Sebenarnya alasan utama Kak Rizki masuk LFM bukanlah karena pada saat SMA dia ikut salah satu subseksi atau yang biasa kita kenal ekstrakurikuler di bidang yang sama juga yang bernama Media Siswa, namun lebih kepada hal-hal seru dan menarik yang diproyeksikan akan diikuti oleh Kak Rizki yang tidak disebutkan secara rinci apa saja.
Saat naik ke tingkat 2, Kak Rizki cukup tertarik untuk menjadi Panitia Lapangan OSKM yaitu Mentor karena ingin melatih skill komunikasi menjadi lebih baik dan lancar, menambah kenalan samamaba-mabaserta menambah wawasan serta pengetahuan tentang ilmu kementoran. Awanya Kak Rizki sempat merasa bosan dengan yang namanya Diklat Terpusat dan Diklat Divisi namun lambat laun perasaan itu berubah menjadi rasa bersyukur karena Kak Rizki mendapat banyak manfaat seperti tujuan awal yang diinginkannya. Di angkatan HMIF 2015 yang bernama ENIGMA, total seluruh panitia lapangan OSKM berjumlah 50-an orang. Nilai ini berbeda cukup signifikan jika dibandingkan dengan jumlah panitia lapangan OSKM di angkatan 2017 yaitu UNIX yang berjumlah 33 orang.
Kakak yang memilikicodenamePanic at The Disco ini ketika SPARTA sebenarnya secara resmi ikut di Divisi Materi dan Metode serta Mentor — tentu karena Kak Rizki pernah jadi Mentor OSKM, namun dia juga sempat diberikan amanah untuk membantu di Divisi Dokumentasi serta Lapangan yaitu Keamanan. Jadi, kakak Daemon yang satu ini memang multitalenta, sudah jago ngoding, komunikasi, dokumentasi, juga untuk menentukan nilai-nilai yang dibawa saat SPARTA pun bisa. Bercerita tentang SPARTA, Kak Rizki merasa diuntungkan karena walaupun turut berpartisipasi dalam Mentor OSKM, namun MSDM SPARTA dengan MSDM OSKM cukup kooperatif untuk bekerjasama mendiskusikan masalah perizinan sehingga jika dirasa Sekolah Mentor saat itu lebih penting maka otomatis teman-teman ENIGMA yang ikut Sekolah Mentor diberikan izin untuk tidak mengikuti SPARTA
Beralih ke masalah keprofesian yaitu tentang Informatika, Kak Rizki cukup tertarik dengan topik Artificial Intelligence yang secara rinci berupa Machine Learning dan Natural Language Processing atau yang biasa disebut NLP. Dikatakan begitu karena topik ini sangat ramai diperbincangkan di dunia teknologi dan memang benar-benar menarik untuk dipelajari. Artificial Intelligence saat ini bisa dibilang merupakanelectricitybaru bagi umat manusia. Banyak sekali hal yang diproyeksikan ke depannya dari 10 hingga 20 tahun ke depan akan tergantikan dengan Artificial Intelligence entah berupa robot, sistem atau yang semacamnya. Namun, untuk saat ini memang rasanya kurang lengkap jika kita bicara teknologi namun tidak mengaitkannya dengan perkembanganArtificial Intelligenceitu sendiri. Walaupun mengambil Kerja Praktik sebagaiSoftware Development EngineerpadaFront-End, kakak yang tinggal di Jakarta ini memiliki rencana untuk mengambil topik Machine Learning dan NLP untuk tugas akhirnya dikarenakan hal-hal yang telah dijelaskan sebelumnya. AI merupakan suatu solusi untuk banyak permasalahan di dunia ini, namun AI juga banyak diperbincangkan terkait masalah yang akan diciptakannya di masa depan, salah satunya adalah akan banyak pekerjaan manusia yang tergantikan oleh AI, misalnya industri. Tentu kita tidak dapat menyalahkan AI karena suatu hal dibalik kehebatannya pasti ada kelemahan yang tidak bisa kita pungkiri. Masalah ini pun tentu dapat merembet ke masalah-masalah lain yang dapat menyebabkan keresahan di masayarakat yaitu kejahatan seperti perampokan, penculikan, pembunuhan hingga hal-hal lain yang tentunya kita sebagai manusia akan sangat khawatir. Namun, dibalik itu semua Kak Rizki memberitahu bahwa telah muncul suatu pemikiran yang berbasis komunis yang dicetuskan oleh Jacquest Fresco yaituresource-based economy. Jadi, Fresco mencetuskan ide yang mungkin bagi sebagian dari kita cukup mencengangkan bahwa secara simpel orang-orang di masa depan tidaklah perlu untuk bekerja lagi, melainkan robot yang akan bekerja untuk kita semua mulai dari bercocok tanam, mengolahnya, hingga mengantarkannya hasil olahannya yang tinggal dikonsumsi oleh manusia. Jadi ide ini bisa diklasifikasikan sebagai komunis namun bukan pada manusia, melainkan robot. Tentu hal-hal ini sangat membuka wawasan kami selaku mahasiswa yang baru menginjak tingkat 2 bahwa perkembangan dunia teknologi sangatlah pesat dan diluar dugaan dari kami.
Masih bersambung dengan perkembangan teknologi yang begitu pesat, perkembanganstartupdi dunia ini pun begitu pesat. Ide-ide yang muncul berubah demikian cepat sehingga bisa sajastartupyang berkecimpung di dunia jasa hari ini, besoknya meluncurkane-commercebahkan ­fintech. Hal ini dapat dijelaskan dengan teori yang bernamaPorter’s Five Forcesyang dipelajari di semester 6 kata Kak Rizki. Jadi, singkatnya persaingan Industri itu sangatlah dipengaruhi oleh 2 ancaman yaitu ancaman untuk tergantikan dan ancaman untuk datangnya penghambat baru serta oleh 2 kekuatan penawaran yaitu oleh penyuplai serta pembeli. Analoginya seperti jika Gojek terlambat 1 atau 2 tahun saja untuk memasuki bidang jasa, maka ancaman Gojek untuk tergantikan sangatlah besar dikarenakan kekuatan penyuplai dan pembeli bisa jadi telah didominasi oleh Grab serta inovasi-inovasi baru dari Grab dapat menghambat berkembangnya Gojek di Indonesia. Keadaan seperti ini nyata terjadi di era sekarang, era yang menuntun setiap manusia untuk berubah bahkan bisa dibilang “memaksa” untuk berubah, era di mana peraduan ide berlangsung secara cepat dan masif sehingga jika kita terlambat untuk mengeksekusi –yang tentunya harus secara sangat baik- walau hanya beberapa tahun bahkan bulan saja, ide tersebut dapat diambil dan dieksekusi oleh pihak lain. Maka, jika dikaitkan dengan keprofesian, Kak Rizki sangat menyarankan agar kita sebagai mahasiswa Informatika tidak melulungodingsaja, melainkan kita juga harus memahami ilmumanagerialsecara menyeluruh. Kasarnya, jangan jadikan 4 tahun menuntut ilmu kita hanya menjadikan kita sebagai “budak korporat” saja, melainkan kitalah juga yang seharusnya bisa mengatur dan menyalurkan ide kita secara menyeluruh ke depannya.
Setelah cukup lama berbincang tentang teknologi, tak lupa Kak Rizki juga memberi nasihat kepada kami agar jangan lupakan dan tinggalkan lingkungan sekitar kami, yaitu HMIF. Salah satu alasannya adalah karena lingkungan ini yang akan selalu kita temui selama 3 tahun ke depan di jurusan, maka dari itu tetaplah untuk berusaha aktif dimanapun divisi tempat kami berkecimpung bahkan walaupun divisi yang kita masuki bukanlah yang sebenarnya kita inginkan. Kakak Daemon yang berencana wisuda pada Oktober di tahun 2019 ini menyarankan agar kita fokus terlebih dahulu untuk mebenahi internal HMIF, baru setelah itu melebar ke kemahasiswaan secara terpusat dikarenakan kita juga harus tahu isu-isu kampus sebagai tanda bahwa kita merupakan mahasiswa yang aktif dan kritis. Salah satu isu yang kami perbincangkan dalam wawancara ini yang juga merupakan isu paling hangat di kampus tercinta adalah tentang kesalahan ketiga kalinya yang dilakukan oleh Presiden Kabinet “Senurani” KM ITB. Kami meluruskan pemikiran terhadap kesalahan ini karena bisa banyak sekali pendapat tentang kesalahan ini yang bisa berujung pada kesalahpahaman yang tentunya kami tidak mengharapkannya. Kak Rizki menyatakan bahwa dia bersikap netral terhadap kasus ini, lain halnya dengan pernyataan sikap HMIF yang memberikan permintaan maaf bersyarat kepada Presiden KM yang berjurusan di Teknik Pertambangan 2014 itu.
Pada sesi terakhir, kami satu per satu diminta untuk mengajukan pertanyaanrandom.Jadi, Kak Rizki mengganti seprai tempat tidurnya sebulan sekali, lalu dia juga tidak pernah mematikan gawainya dalam modeshut down.Jika Kak Rizki memiliki kemampuan finansial untuk bisa pergi ke suatu tempat, dia akan kerja serta menjadi seorangbackpackerdi Eropa. Untuk masalah perjodohan dan kuantitas anak yang akan dimiliki, Kak Rizki berencana untukKeep It Organic ,suatu frasa yang mulaiboomingsaat Forum Bebas yang diadakan awal Agustus lalu. Kak Rizki juga memberi tahu sedikit dari sifatnya yaitu orangnya jail serta bisa dibilang “beo”, yaitu ikut-ikutan sama yang dilakukan orang lain. Terakhir, Kak Rizki memberikan sepatahquoteyang terdengar tidak asing di telinga kami yaitu “Don’t Panic Keep It Organic”.",https://miro.medium.com/v2/resize:fit:599/1*44_a0fJsEej-QxjBJHPkPA.png,"Startup, AI, Technology"
https://medium.com/s/story/berkenalan-dengan-chatbot-part1-ea92147e0c5d,Berkenalan dengan Chatbot (Part 1),Sofian Hadiwijaya,2018-10-17T02:17:38.517Z,"Stories from the people behind the code — the engineers and product designers ofKata.ai
Masih ingat ga pada tahun 2015 bermunculan startuppersonal assistant? ya saat itu di indonesia sendiri ada Yessbosnow dan Halodiana, persaingan ini cukup sengit. Mekanisme dari keduanya hampir mirip, dikala itu semua pesan yang masuk diolah oleh customer service. Dari situ gw berpikir kayaknya akan susah jika perkembangan usaha berbanding lurus dengan penambahan karyawan diera startup yang mencoba mengefisienken segala sesuatunya dengan bantuan teknologi.
Nah ditahun yang sama, saat gw main ke kantor om Jim, doi tiba-tiba ngajak ketemuan dengan salah satu perusahaanpersonal assistanttersebut, disinilah gw bertemu dengan irzan dan rizqi, founder Yessbosnow. Ternyata mereka mulai merancangchatbot, Dan kita pun mulai membahas gimana mekanisme untuk membuatchatbot. Sepulangnya dari pertemuan itu, om Jim coba menjelaskan ulang tentang NLP (Natural Language Processing).
Mari kita liat bagaimana sebenarnya Bot bekerja.
Sebenernya sangat sederhana Bot menerima pesan, kemudian mengolahnya, dan membalas pesan tersebut. Pertanyaannya, bagaimana Bot mengerti isi dari pesan yang kita kirim padanya? dan setelah mengerti apa yang harus dibalaskan?, masih ingatkah kita ketika kita belajar bahasa baik indonesia, kita dikenalkan dengan SPOK ( Subjek, Predikat, Objek, Keterangan ) dalam sebuah kalimat, dan kalimat biasanya dibentuk oleh beberapa kumpulan kata. Dan kata sendiri banyak sekali jenisnya seperti pada tautan berikuthttps://id.wikibooks.org/wiki/Subjek:Bahasa_Indonesia/Materi:Kata
Dengan demikian kitapun harus mengajari Bot kita bagaimana supaya mengerti konteks dari setiap pesan yang masuk. Namun terkadang tidak segampang itu, mari kita perhatikan kalimat berikut:
Saya ingin makan malam
Jika kita tidak hati-hati dalam melatih Bot untuk mengerti konteks, bisa saja Bot menerjemahkan, malam yang mau dimakan sama orang yang mengirim pesan tersebut. Setelah mengerti konteks-nya, kita juga siapkan jawaban apa yang cocok dengan konteks tersebut.
Wah cukup panjang ya perjalanan membuatchatbot, dan kita harus tekun juga nih, hehehee..
Nah bagaimana seandainya kita ga punya kemampuan untuk membuat algoritma supaya Bot mengerti konteks dan kita juga ga punya perbendaharaan kata yang banyak untuk membalas pesan tersebut?, sedikitflashbackkebelakang, dulu ada Bot twitter yang bisa kita aja ngobrol, namanya@begobet, Bot ini ternyata cara kerjanya sangat sederhana, jadi setiaptwityang diterima sama Bot ini,twittersebut akan secara otomatis dicarikan di forumKasKus, contohnya seperti ini:
secara acak akan diambilreplydari sebuah thread. Kalo mau dicoba monggo, kunjungi reponya om Jim atau bisa ke repo om Jim yang sudah sayaforkhttps://github.com/sofianhw/botgan, begini sekilas kelucuan chat sama botgan
Namun terkadang sungguhrandomya 😆? Salah satu perusahaanpersonal assistant , Yessbosnowyang sudah saya mention di atas, sekarang berevolusi menjadiKata.aiperusahaan yang mempermudah untuk membangunchatbot. Jadi kita bisa fokus terhadapflowdanexperienceuser/customerketika beinteraksi sama Bot kita.
Mari kita coba sekilasPlatform Kata.ai,berikut contoh Bot sederhana yang hanya mengerti pesanHi.
Pertama mari kita install kata-cli (kebetulan gw menggunakan macOs tampilan berikut merupakan step by step yang ada di terminal macOS)
$ npm install -g kata-cli
Whoalaa.. tenang begitulah cara mesinkata.aiuntuk mengerti pesan yang dikirim kepada Botnya.
Hhmm.. sepertinya masih lumayan ribet ya 😬? bagaimana kalo kita bisa membuatchatbotdengan menggunakan GUI (Graphical User Interface), dimana kita ga perlu install dan ga perlu terlalu paham dengan bahasa pemrograman?Bersambung ke part2
Stories from the people behind the code — the engineers and product designers ofKata.ai
AI Enthusiast, Data Enthusiast, Technology Evangelist, Robotic Enthusiast, Public speaker, Mentor, Doing epic shit!http://id.linkedin.com/in/sofianhw",https://miro.medium.com/v2/resize:fit:1200/1*S5S_OBECjIzmGtPZq1Vxsg.jpeg,"Kata Ai, Chatbot, Bot, Artificial Intelligence"
https://medium.com/s/story/mengapa-harus-rahasia-bila-memang-tulus-eb48e37830a0,Mengapa Harus Rahasia Bila Memang Tulus?,Gema Mahardhika,2018-07-08T02:51:12.409Z,"Pernahkah kita membayangkan hidup seperti di dalam film Her (2013) karya sutradara Spike Jonze, dimana kehidupan sudah serba modern dan dipenuhi jaringan internet, bahkan hingga ke tubuh dan jiwa seseorang? Pada film tersebut, tokoh utamanya, Theodore, mulai merasakan kesepian yang sangat ketika berpisah dengan istrinya, Catherine, belum lagi ditambah tekanan kerja yang membuatnya bingung harus kemana pelampiasan stress itu dibawa. Akhirnya ia menemukan solusi terhadap kesepiannya itu. Tepat diwaktu sepulang kerja, di jalan menuju rumah ia menemukan iklan teknologi berupa Artificial Intelligence, sebuah kecerdasan buatan yang dimuat dalam bentuk Operation System (O.S) dan menarik perhatian Theodore karena kecanggihannya yang bahkan mampu mendeteksi kondisi psikis seseorang. Theodore pun menjadi pengguna setia dari O.S tersebut. Di akhir film Theodore bahkan jatuh cinta dengan suara di balik O.S tersebut. Karena sejak awal ia membeli, O.S itu selalu paham akan keperluan Theodore. Salah satu alasan di film ini mengapa Theodore bisa jatuh cinta terhadap mesin itu karena ia tidak lagi merasakan kesepian, banyak teman di dunia nyata mencoba mengingatkannya agar untuk tidak jatuh cinta terhadap mesin itu tetapi Theodore mengabaikan. Demikian canggih teknologi O.S itu hingga mampu membuat Theodore tertawa, menangis, bahkan memenuhi kepenuhan seksualnya. Sejak bercerai Theodore menjadi sangat kesepian, banyak teman-teman kerja yang peduli terhadapnya tetapi tidak dihiraukan. Disaat seperti itu seharusnya faktor utama agar Theodore mampu melanjutkan hidup seperti biasa adalah dengan mendengarkan dan mengikuti pendapat teman. Bagaimana pendapat kalian? namun sebelum itu tonton dulu filmnya,hehe!
Sekarang seseorang bisa mengakses internet dimanapun dan kapanpun. Dulu kita bisa memarahi orang yang mengabaikan pembicaraan kita karena bermain handphone, tetapi sekarang jika kita menegur orang yang asik dengan handphone-nya malah semakin membuat kita jauh dari bagian temannya. Dunia pun sudah terbagi dua. Kita mungkin mengenal orang lain lewat media sosial. Di instagram orang tersebut telihat atraktif, namun saat bertemu ia hanya diam dan tidak sesuai dengan apa yang tampak di instagram. Itu semakin mendukung bahwa sebenarnya dunia menjadi dua di zaman seperti yang sekarang ini. Kita menilai orang lain berdasarkan apa yang ia tunjukkan di akun media sosialnya dan mengabaikan keutuhan dari makna perkenalan yang seharusnya berujung saling membantu dan pada akhirnya untuk menyayangi sesama manusia. Kita menjadi lebih sensitif ketika seseorang tidak mengomentari dan memberi likes di postingan kita dibanding seseorang tersebut tidak datang memberikan kue atau minuman kerumah kita.
Saya percaya semua media sosial itu dibuat berdasarkan tujuan yang baik. Hanya saja seringkali yang membuat tujuan itu tercemar adalah si pengguna. Kita melakukan suatu hal hanya untuk bersenang-senang sehingga melukai tujuan positif dari media sosial. Media sosial apapun itu pasti memiliki dampak positif dan negatif. Kita boleh memiliki sebanyak mungkin media sosial sesuai keperluan kita, namun tugas kita sebagai masyarakat dan manusia adalah dengan tidak berlebihan menggunakannya sehingga melupakan teman disamping kita yang bisa jadi sangat memerlukan kita pada saat itu. Juga sebisa mungkin untuk berusaha menghapus dampak negatif itu bersama-sama.
Blood makes you related. Loyalty makes you family. Darah membuat kita berhubungan. Kesetiaan membuat kita menjadi keluarga.
*gema mahardhika(2018)",https://miro.medium.com/v2/resize:fit:1200/1*w1JtgVbEClqbKJjTxFYrcg.jpeg,"Artificial Intelligence, Technology, Indonesia, Articles, Esai"
https://medium.com/s/story/jual-tanah-balikpapan-kota-telp-wa-0822-4069-7469-jual-tanah-balikpapan-kota-kavling-taman-ee29453b724e,"Jual Tanah Balikpapan Kota, Telp / Wa 0822–4069–7469, Jual Tanah Balikpapan Kota Kavling Taman Kurma Samboja Tenggarong Kutai Kartanegara Kalimantan Timur",basirul akbar,2018-08-11T05:25:57.622Z,"Hari ini banyak sekali hal yang bisa Anda temui di berbagai macam berita, baik di dunia maya maupun media cetak hingga elektronik lainnya. Salah satu bahan pemberitaan yang tidak habis — habisnya ini adalah tentang investasi.
Coba saja Anda lihat di televisi, pasti familiar sekali dengan acara yang mempromosikan investasi property yang dikelola oleh developer properti terkenal. Di berita lainnya seperti media cetak atau berita di internet, berbagai macam tawarantempat investasi terbaikpun bermunculan menawarkan kelebihannya masing — masing.
Memang benar adanya jika investasi menjadi sebuah kepentingan saat ini mengingat persaingan ekonomi yang begitu berat, kemajuan teknologi setiap hari yang juga berdampak pada segala macam kebutuahn hidup yang terus meningkat sementara tidak akan selamanya Anda bekerja, pun tidak akan selamanya kita memimpin sebuah perusahaan atau membuat sebuah usaha.
Pasti akan ada masa dimana semua diganti oleh Orang yang baru dan di masa tua Anda, apa yang bisa Anda lakukan?
Masa tua adalah masa dimana harusnya Anda beristirahat dari segala macam hiruk pikuk dunia. Namun, tidak menutup mata pula jika kebutuhan itu pasti dan akan selalu ada hingga Anda tutup usia.
Karena itulah jika Anda sudah punya investasi yang tepat, maka Anda tidak perlu khawatir lagi untuk menjalani sisa hidup Anda di masa yang akan datang.
Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Anda pun tidak akan rugi jika beli kavling diKavling Taman Kurmakarena harga tanah selalu naik dan ini akan jadi investasi yang terbaik untuk Anda.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:1200/1*SUlGwscgBB0i_ljv8BTYbg.jpeg,"Passion, Passwords, Machine Learning, Parenting, Pakistan"
https://medium.com/s/story/a-i-artificial-intelligence-ancaman-atau-idaman-eebfa68d2910,A.I. Artificial IntelligenceAncaman atau Idaman,Ron Ashrovy,2017-11-18T02:51:26.841Z,"Mungkin saat ini artificial intelligence (AI) atau kecerdasan buatan bukan lagi hal yang asing di telinga kita, ditambah lagi salah satu search engine idola rakyat kita; mbah Google meluncur smartphone Google Pixel 2 yang empowered dengan AI, salah satunya yang teramat jelas adalah dimana Google Pixel 2 hanya menggunakan satu lens kamera dan dapat memfoto dengan efekbokeh mirip seperti semua smartphone dengan dual kamera.
Lalu bagaimana perkembangan AI saat ini? bagaimana perkembangan AI masa depan nanti? apakah nantinya AI tetap seperti saat ini? Membantu dan menolong kita atau kah sebaliknya?
Tahun 2018 prediksi saya, seluruh smartphone akan berlomba-lomba menanamkan AI seperti layak Google Pixel 2 atau mungkin lebih canggih ke perangkat mereka. Efek yang kita rasakan berikutnya adalah smartphone berteknologi tinggi akan semakin murah dan kita akan semakin dimanjakan dan tergila-gila dengan fitur² yang makin canggih kedepannya.
Bahkan prediksi saya tidak hanya Google, melainkan Facebook, kedepannya mereka akan ikit berkecimpung. Saat ini mereka tengah mengembangkanAI dimana kita mampu mengetik tulisan dengan pikiran.walaupun mereka baru berhasil mengetik 8 kata permenit, target mereka mencapai 100 kata permenit. Menariknya teknologi ini sama sekali tidak menggunakan implan apapun di dalam otak kita hanya cukup melihat layar ponsel anda maka gelombang otak terbaca.
Fenomena perkembangan AI sebenarnya tidak hanya tertuntuk smartphone. AI sudah banyak digunakan untuk hal-hal yang teramat teknis, dari mulai memverifikasi ikan segar hingga meniru gaya menulis manusia.
Beberapa AI yang telah berkembang juga ada yang kontroversial, diantaranya munculnya AI bernama gayradar yang mana AI ini dapat mengetahui apakah anda gay atau bukan hanya dengan scanning foto!Bukan dari pemaparan genetikyang menjadi kekhawatiran ialah dimana AI² ini disalah gunakan oleh pihak-pihak yang tidak bertanggung jawab.
Hingga saat ini perkembangan dan penelitian teknologi informatika yang sedang gencar-gencar membuat tidak sedikit dari pakar yang meyakini (seperti Stephen Hawking dan Elon Musk) bahwa khayalan fiksi seperti The Terminatordimana robot menguasai manusia. Hal ini bukan lagi mustahil di era AI yang sudah mampu melakukan berbagai macam hal, cukup menunggu waktu.
Untuk menghalau kejadian yang tidak diharapkan ini, alih-alih membatasi perkembangan AI. Elon Musk justru merasa kalau kita harus berintegrasi dengan AI, dengan kata lain ditubuh kita (lebih tepatnya otak) ditanami mesin.
Bercanda? Sama sekali tidak.
Pernyataan ini terkesan hanya eksis di khayalan dan layar, tetapi Elon Musk sendiri yang menegaskan bahwa masa depan nanti (setelah robot mampu di implan ke dalam otak) kita tidak perlu lagi berbicara atau kesulitan di dalam mengirimkan email dan video, bahkan kktatidak perlu berbicara untuk berkomunikasi dengan seseorang. Segala sesuatu bisa dilakukan dengan pikiran.
Seakan meng-amini ucapan Elon. Google sendiri menyatakan bahwa mereka sedang mempersiapkan cloud untuk menampung memori anda di secara online sehingga bisa dibilang kedepannya nanti kita berpotensi untuk tidak lupa dan sekaligus ingatan kita bisa saja di manipulasi atau tidak nyata. Dan project ini akan di launching30 tahun mendatang rencananya
Saya memprediksi kedepannya akan ada layanan ilusi dan imajinasi sebagai santapan otak. Contoh, anda tidak punya waktu berlibur dan menikmati keindahan raja ampat, dengan dukungan integrasi otak dan cloud memungkinkan kita bisa menikmati liburan dan nuansa alam disana dengan hanyamemejamkan mata.
Memang, dan yang terpikirkan oleh saya ialah resiko dari kemajuan ini; privacy, psikologis, politik, sosiologi masyarakat, hingga esensi hidup.
Di China, fenomena menangkap seseorang dariidentifikasi dan face recognition sudah dilakukan. Hal ini menunjukkan bahwa tanpa kita sadari ada pihak-pihak tertentu yang mungkin saja memata-matai kita tanpa kita sadari, mencuri, mempelajari biodata kita dari sosial media, menggunakannya untuk kepentingan pribadi.
Dan ini bukanlah hal yang langka.
Mengapa Google menyediakan mesin pencari tanpa meminta sepeser pun dari pengguna? Apa untungnya untuk Google? Bagaimana cara dia menghidupi karyawan? Apa yang mereka jual?
Hal yang sama juga dilakukan Facebook, dia tidak pernah meminta apapun dari user dan selalu memberikan layanan terbaik. Lalu bagaimana Facebook menghidupi karyawan perusahaan sebesar itu? Apa saja yang mereka jual?
Selama kita masih menggunakan sosial media, sejatinya privacy kita terjajah tanpa kita sadari. Saya tidak menyarankan anda untuk berhenti bersosial media, tetapi lebih ingin memberi tahu, bahwa saat ini di balik sosial media terdapat AI yang berfungsi mempelajari anda dan mengerti anda jauh lebih dalam, hal tersebut tidak dapat di hindari yang anda dapat lakukan hanyalah berhati-hati didalam mengakses konten dan melakukan posting, apapun yang terjadi jangan memposting sesuatu yang menjadi boomerang untuk anda, demi kebaikan anda sendiri
Founder at Dialogika",https://miro.medium.com/v2/resize:fit:1200/1*sejIK_1N2wtjEbkqpxKpsA.jpeg,"Artificial Intelligence, Threat, Penemuan, Bahasa Indonesia, Inventions"
https://medium.com/s/story/teknologi-facial-recognition-untuk-memberantas-kriminalitas-eedd28c383ea,Teknologi Facial Recognition untuk Memberantas Kriminalitas,Fadel Nararia Rahman,2018-07-17T16:29:44.534Z,"TeknologiFacial Recognitionuntuk Memberantas Kriminalitas
Fadel Nararia Rahman / 16517204 / Sistem & Teknologi Informasi
Teknologi adalah keseluruhan sarana untuk menyediakan berbagai macam peralatan atau sesuatu yang dapat digunakan manusia untuk membantu kehidupan menjadi lebih nyaman dan mudah. SedangkanFacial Recognitionsendiri adalah salah satu teknologi biometrik yang ditemukan digunakan untuk mendeteksi dan juga membedakan wajah orang yang dipindai dan membeda-bedakannya berdasarkan data personal orang yang telah telah disimpan sebelumnya.
AplikasiFacial Recognitionsendiri ada beberapa tahapan atau proses kerja. Pertama tentunya dengan memanfaatkan kamera untuk menangkap wajah orang tersebut, kemudian perangkat lunak yang bekerja dibelakangnya akan membedakan mana bagian gambar yang merupakan wajah manusia dan juga mana bagian gambar yang merupakan latar ataubackgrounddari gambar tersebut. Kedua, setelah didapat gambar wajah dari orang tersebut maka perangkat lunak tersebut akan membandingkannya dengan data personal wajah yang telah disimpan dalam suatudatabase. Apabila dalam pencarian data tersebut ditemukan kecocokan, maka perangkat lunak tersebut akan menampilkan pesan benar atau ditemukan, lalu di beberapa keperluan dapat ditampilkan data-data personal orang yang dipindai tersebut.
Lalu apa hubungan dengan upaya menggunakan teknologi ini untuk memberantas kriminalitas? Tentu ada banyak sekali keuntungan yang akan didapatkan bila pemerintah kita menggunakan teknologi ini untuk memberantas kriminalitas. Pertama, memudahkan kepolisian yang bekerja dalam pencarian pelaku kriminalitas dengan lebih cepat, efektif, serta tanpa adanya campur tangan manusia yang tidak netral. Bila data penduduk Indonesia yang direkam sangat lengkap mulai dari data administrasi hingga data biometrik personal, dan juga perangkat kamera pengintai (CCTV) dipasang secara merata di seluruh daerah Indonesia, maka proses pencarian orang hilang, pelaku kejahatan, bahkan orang yang dicari akan sangat mudah dan efisien untuk dilakukan. Karena dasarnya setiap gerak-gerik orang tersebut dapat terpantau kamera dan secara langsung dapat dilihat oleh pihak berwajib. Kedua, bila terdapat suatu kejadian kriminal misalkan pencurian, perampokan, pembunuhan, dll maka pihak berwajib akan langsung mengetahui persis lokasi & waktu kejadian, serta siapa pelaku dari tindakan kriminal tersebut. Pihak berwajib dapat memilih untuk langsung datang ke TKP dan menindaknya apabila sempat, atau bila tidak sempat setidaknya pihak berwajib dapat langsung menangkap pelaku tersebut dengan cepat menggunakan teknologi tersebut. Ketiga, selain dapat diaplikasikan oleh pihak berwajib atau pemerintah, teknologi ini dapat diterapkan pada seluruh gedung / ruangan yang hanya boleh dimasuki oleh pemilik ruangan teresebut, atau secara sederhana penguncian suatu ruangan hanya dapat dibuka dengan pemindaian wajah sang pemilik menggunakan teknologi tersebut. Sehingga tiap kali orang lain berusaha membuka ruangan tersebut, maka sistem akan menolak membukanya, berkebalikan jika sang pemilik yang berusaha masuk maka sistem akan secara otomatis membuka ruangan tersebut. Dengan begini, bila proses penguncian seperti ini diterapkan pada rumah pribadi maka dapat mencegah pencurian atau perampokan terjadi.
Berbagai contoh keuntungan yang mungkin bisa didapatkan dengan menerapkan teknologiFacial Recognitionyang telah disebutkan diatas memang masih agak sulit bila diterapkan di negara kita sendiri, Indonesia. Satu sisi, dari segi kesiapan infrastruktur baik itu koneksi internet, maupun keuangan yang akan menanggung semua sarana dan prasarana yang dibutuhkan dapat dikatakan kurang siap. Hal ini disebabkan masih banyak daerah di Indonesia yang kurang tersentuh jangkauan sinyal Internet, bahkan ada area yang benar-benarblankdari sinyal Internet. Tapi masih ada harapan dan kemungkinan teknologi ini benar-benar dapat diaplikasikan di Indonesia, kita hanya perlu memperjuangankannya dan berdoa agar di masa depan teknologi ini bisa dimanfaatkan secara maksimal dan membawa manfaat yang besar dalam rangka memberantas tingkat kriminalitas di Indonesia.",https://miro.medium.com/v2/resize:fit:870/1*2K4MrZYsqlOQ-HdhVLPwUw.jpeg,"Artificial Intelligence, Technology, Facial Recognition, Crime, Innovation"
https://medium.com/s/story/tambang-minyak-baru-di-2017-eff1213a5652,Tambang Minyak Baru di 2017,Incircle,2017-11-21T03:22:45.549Z,"Sebuah komoditas baru telah menumbuhkan industri dengan cepat dan menguntungkan, yang akhirnya mendorong para pembuat kebijakan untuk masuk dan mengendalikan orang-orang yang mengendalikannya selama ini.
Seabad yang lalu, sumber daya yang dimaksud adalah minyak. Sekarang kekhawatiran serupa diajukan oleh raksasa-raksasa yang menangani data, minyak di-era digital.
Raksasa-Alphabet (induk perusahaan Google), Amazon, Apple, Facebook dan Microsoft-terlihat sudah tak terbendung. Mereka adalah lima perusahaan yang paling berharga di dunia. Keuntungan mereka secara kolektif melonjak lebih dari $ 25 miliar dalam laba bersih di kuartal pertama 2017. Amazon sendiri menangkap setengah dari semua dolar yang dibelanjakan secara online di Amerika. Sedangkan Google dan Facebook menyumbang hampir semua pertumbuhan pendapatan iklan digital di Amerika tahun lalu.The Economist.
Tetapi, Apakah kamu sendiri tau jika kita juga merupakan salah satu penghasil data terbaik di dunia?
Kontribusimu dalam membuat sumber daya di bidang data sangatlah nyata.
Dengan adanya data yang dihasilkan saat ini, diprediksi data yang terkumpul pada tahun 2020 dapat berlipat menjadi empat kali lebih banyak daripada butiran pasir di bumi.
Every Second, we create data.
Setiap hal yang kita lakukan akan tercatat dan tersimpan sebagai data. Apa yang kita lakukan melalui ponsel kita, browsing, typing, uploading, downloading, streaming dan lain sebagainya akan selalu tercatat.Setiap menitnya, terdapat 31,3 juta pesan yang terkirim lewat facebook chat, sehingga facebook dapat mengolah hampir 350 GB data per menit.
Setiap menitnya, 277.000 pengguna Twitter mengirimkan cuitan mereka.
Setiap menitnya, terdapat 300 video yang di upload ke Youtube.
Setiap menitnya, Google memproses 2 juta pertanyaan.
Setiap menitnya, terdapat lebih dari 100 juta email baru dibuat.
Dengan begitu banyaknya data yang tercipta setiap menit bahkan setiap detik, bukan suatu hal yang tidak mungkin jika di tahun 2020, banyaknya data dapat berlipat menjadi empat kali lebih banyak daripada butiran pasir di bumi.IBM Analytic.
Data are starting to touch nearly every aspect of our lives, sometimes in unexpected ways.
Ketika berbelanjan online, penjual dan pengiklan menggantungkan big data dalam menyarankan barang-barang ke pembeli yang berdasarkan pada riwayat pencarian dan riwayat pembelian si pembeli.
Di sektor kesehatan, berbagai macam aplikasi kesehatan berbasis data membantu memudahkan kita dalam mengecek/melacak kesehatan kita seperti tingkat glukosa, tekanan darah sampai pola makan kita.
Melalui perhitungan data real-time, aplikasi penunjuk jalan (maps) dapat memberikan saran rute tercepat menuju tempat destinasi kita lengkap dengan estimasi waktunya. Cara kerja yang sama digunakan aplikasi reminders dan calendar yang meberikan pemeritahuan di tanggal tertentu.
Aplikasi sosial media seperti facebook dan twitter menggunakan big data dalam merekomendasikan berita yang muncul di halaman beranda (newsd feed). Hal yang sama dilakukan oleh apikasi penyedia berita.Seperti contoh-contoh di atas, tidak diragukan lagi bahwa adanya data telah membawa impact positif yang meresap di semua aspek kehidupan kita. Sangat jelas bahwa kehidupan kita dikemudian hari juga akan semakin bergantung pada data dan dengan demikian, maka tidak menutup kemungkinan bahwa jumlah data yang tercipta dan tersimpan terus berlipat.
Incircleadalah perusahaan analisis data teknologi pertama yang menemukan nilai tersembunyi dari sebuah project. Kami membantu creator dalam membangun project yang inovatif dan efektif dengan menggunakan data, analitik dalam cloud base.
TujuanIncircledibangun untuk menganalisis data dengan volume besar tanpa memindahkannya dan juga mengisi kesenjangan antara kebutuhan perusahaan dan sumber daya project. Kami bekerja dengan menggabungkan pengetahuan vertikal untuk membantu sebuah proyek memenuhi kebutuhan bisnis yang unik dari sebuah perusahaan.
Mohammad Arkham Chadiar Jantra
Source :Keywebmetrics,Botanalytics,EconomistandIBM Analytics
Incircle is an Analytical Data Technology Company. We provide Customer Information and Insight to the Company using data and analytics by entering into customer",https://miro.medium.com/v2/resize:fit:1200/1*iY_DQSswstpHhj-u-5vX2w.jpeg,"Mining, Data, Business Strategy, Indonesia, Data Science"
https://medium.com/s/story/apakah-pekerjaanmu-bakal-tergantikan-ai-f07198357bab,Apakah Pekerjaanmu Bakal Tergantikan AI?,Nur Khansa,2018-03-26T16:00:46.644Z,"Sahabat berprosesmu.
Belakangan ramai dibicarakan kalau pekerjaan manusia di sejumlah bidang bakal tergantikan oleh robot atau program kecerdasan buatan aliasArtificial Intelligence(AI) di masa yang akan datang.
Nggak jarang orang mulai merasa was-was bila bidang pekerjaannya lama kelamaan nggak lagi dibutuhkan.
Malah, saat ini sudah adaportal webyang khusus memberi informasi apakah pekerjaan seseorang akan aman (atau tidak) dari otomatisasi.
Kira-kira, apa aja sih pekerjaan yang bisa — atau sudah mulai tergantikan oleh AI?
Meksipun sekolahnya susah dan perlu ‘berdarah-darah’ untuk lulus dari Fakultas Kedokteran, tapi ternyata, sudah diciptakan sebuah robot yang punya kemampuan memberi diagnosa bermacam-macam penyakit.
Robot tersebut bernama Babylon, yang dikembangkan oleh perusahaanstart-upkesehatan asal Inggris.
Babylon juga mampu melakukan komunikasi sederhana dengan pasien, hingga melakukan perawatan yang dibutuhkan. Ia bisa digunakan sebagai pencegah penyakit tertentu dengan memberi diagnosa awal melalui rekaman kebiasaan harian dan mengkombinasikan data detak jantung dengan rekam medis penggunanya.
Saat ini, sektor hukum juga sudah memiliki AI bernama Ross yang mampu melakukan riset, menyediakan teori hukum yang dibutuhkan, hingga mencari data-data yang berhubungan dengan kasus yang tengah ditangani. Pekerjaan-pekerjaan tersebut biasanya banyak menyita waktu pengacara. Ross bisa diandalkan untuk memudahkan pekerjaan para pengacara, sehingga perannya lebih mirip seorang asisten. Tapi, bukan nggak mungkin di kemudian hari peran Ross berkembang menjadi lebih signifikan.
Agen perjalanan juga saat ini sudah banyak digantikan lewat situs pemesanan tiket, penginapan dan perencanaan perjalanan lewat internet, karena kemudahan yang ditawarkan.
Bahkan, urusan masak pun sudah ada AI-nya lho.
Moley, sebuah robot AI, mampu memasak macam-macam jenis makanan dengan cekatan. Robot berbentuk dua buah tangan otomatis ini bisa mereplikasi resep yang diinginkan. Tapi, ia belum mampu meracik resep sendiri.
Pekerjaan kreatif seperti menulis juga ternyata lambat laun sudah bisa digantikan AI. Saat ini, di bidang agensi dan periklanan, ada sebuah program yang mampu melakukanpitchinguntukbranding,kampanye, dan iklan di hadapan klien. Program yang dinamai Persado ini juga bisa membuat email penawaran personal untuk meyakinkan klien memakai jasa mereka.
Ada juga Wordsmith, program pintar yang telah melewati berbagai tahap uji coba untuk menulis berita. Program ini bisa melakukan riset dan membuat laporan yang efisien dari data yang besar.
Program Bold juga konon bisa menggantikan peran editor, karena ia bisa diandalkan untuk memberi revisi dan perbaikan terhadap struktur kalimat.
Posisi perekrut staff kantor yang biasa dilakukan oleh pekerja di divisi Human Resouces Development (HRD) bisa tergantikan oleh aplikasi AI nih.
Pasalnya, FlatPi mampu membaca, memilah dan kemudian mengkategorisasikan lamaran kerja untuk mendapatkan kandidat yang paling sesuai dengan kriteria yang diinginkan sebuah perusahaan — hanya dalam hitunagn detik. Perusahaan konsultan karier, Firstjob, juga telah membuat chatbot bernama Mya yang mampu berinteraksi dengan pelamar kerja untuk memberi informasi terkait status lamaran kerja mereka.
Pekerjaan administratif juga punya tantangan nih. Resepsionis virtual sekarang mulai banyak digunakan di berbagai kantor, untuk menjawab panggilan secara otomatis, hingga menjadi asisten. Jepang bahkan telah mengganti peran resepsionis dengan robot.
Serupa dengan resepsionis, peran juru ketik dan notulensi juga mulai digantikan AI. Saat ini, software pengenal suara sudah mampu mentransfer suara menjadi tulisan dengan efisien.
Nggak hanya itu, peran supervisor juga mulai dikerjakan oleh robot lho. Betty, sebuah robot pintar memiliki kemampuan untuk menyapa tamu dan karyawan, memantau jam kerja dan lembur karyawan, berkeliling menginspeksi kantor, hingga mengelola inventaris kantor.
Pekerjaan repetitif dengan hasil yang konstan saat ini sudah mampu dikerjakan robot, seperti yang dikerjakan pekerja pabrik untuk memasang kemasan sebuah produk, menempel label, dan sebagainya. Penggunaan robot untuk pekerjaan repetitif diyakini mampu mengurangi human error akibat kelelahan atau konsentrasi berkurang.
Selain untuk pabrik, robot serupa juga mampu bekerja di bidang pertanian sebagai penyemai benih, pemberi pupuk, dan sebagainya.
Hm, berfaedah, atau mulai merasa tersaingi nih?
https://tekno.kompas.com/read/2014/01/29/1727369/10.Profesi.yang.Terancam.Punah.Digantikan.Mesin/
*semua berita diakses pada 25 Maret 2018
Sahabat berprosesmu.",https://miro.medium.com/v2/resize:fit:1024/1*pc5jwzR0XgHDXvoDGWwwyw.jpeg,"Artificial Intelligence, Otomatis, Millenials, Kerja, Teknologi"
https://medium.com/s/story/kenalan-dengan-core-ml-dari-apple-f0b87b2119dc,Kenalan dengan Core ML dari Apple,Cahyanto Setya Budi,2017-11-27T11:59:52.823Z,"Codelabs is a division of digital experts. We created an application to help the people. And we’re from Indonesia Computer University.
Sebelumnya pada WWDC 2017 yang lalu Apple resmi memperkenalkan Core ML, tapi buat apa sih Core ML ini?
Mmmm singkatnya Core ML merupakan framework dari Apple yang bisa digunakan untuk mengintegrasikan model ML(Machine Learning) yang sudah di training sebelumnya agar bisa digunakan pada aplikasi iOS dengan menggunakan resource yang ada pada perangkat iOS itu sendiri. Apple sendiri yakin telah mengoptimasi Core ML sehingga bisa digunakan pada perangkat iOS yang ada di pasaran.
Terus apa saja sih yang bisa dilakukan Core ML ini? Hmmm mungkin gambar ini bisa sedikit menjawab pertanyaan:
Oke cukup dengan teorinya, kayaknya lebih rame kalo langsung demo aja. Let’s go!
Pertama, buat project dan pilih “Single View App” dan jangan lupa bahasanya swift ya. Selanjutnya buat layout seperti ini pada Main.storyboard
Setelah kita membuat UI lanjutkan dengan implementasinya. Pada ViewController.swift tambahkan code menjadi seperti ini:
Setalah itu lanjutkan dengan mengunjungi link inihttps://developer.apple.com/machine-learning/dan scroll kebawah dan cari Models lalu pilih Inception v3 sebagai modelnya dan download.
Selesai didownload masukan model tersebut ke dalam project dengan drag file dan lepaskan pada Project Navigator di Xcode. Setelah selesai menambahkan file lanjutkan dengan generate model dengan klik tanda panah pada Inceptionv3 pada Model Class, lihat gambar untuk lebih jelasnya:
Tahap selanjutnya, tambahkan code pada ViewController.swift menjadi seperti ini:
Sekarang waktunya kita build dan run projectnya. Masukan foto yang akan di identifikasi lalu apps akan menampilkan hasilnya pada label yang telah dibuat sebelumnya. Oya pada saat pengetesan, simulator menggunakan iOS 11.1 dan Xcode 9.1.
Untuk yang masih bingung mengenai code lengkapnya bisa download langsung project nya dari siniGithub Project
Codelabs is a division of digital experts. We created an application to help the people. And we’re from Indonesia Computer University.",https://miro.medium.com/v2/resize:fit:800/1*uVu0ZWQxeodVuLTePgVHJw.png,"Core Ml, Machine Learning, IOS, Artificial Intelligence"
https://medium.com/s/story/convolutional-neural-net-untuk-deteksi-objek-f14d72f11ba6,Convolutional Neural Net untuk Deteksi Objek,Richard Dharmadi,2022-02-22T03:26:29.637Z,"Seperti telah dibahas pada introductory postsebelumnya, CNN dapat digunakan sebagai pendekatan untuk problem-problem di bidangcomputer visionselain klasifikasi gambar. Secara lebih khusus, post ini akan membahas penggunaan CNN pada implementasiobject detector.
Beberapa teknik populer yang memanfaatkan CNN, diantaranya:
Salah satu arsitektur pertama yang menggunakandeep learninguntuk mendeteksi objek adalah OverFeat. Dipublikasikan pada tahun 2013 oleh peneliti dari New York University, OverFeat memanfaatkan algoritmamulti-scale sliding windowdan CNN [1].
Permasalahan dengan menggunakan tekniksliding windowadalahbanyaknya potongan-potongan gambar yang harus diproses oleh CNN. Setiap potongan tersebut akan melalui proses konvolusi, untuk kemudian diklasifikasikan menjadibackgroundatau objek. Artinya, dengan banyaknya lokasi gambar serta ukuransliding windowyang digunakan, komputasi dari keseluruhan proses tersebut akan sangat berat![1]
Maka muncul lah teknikRegions with CNN features atau R-CNNyang diperkenalkan oleh peneliti dari UC Berkeley,Ross Girshick, et al. pada tahun 2014.
Proses R-CNN sendiri terdiri dari 3 tahap[1];
As you might already notice, instead of processing a whole bunch of cropped images like in OverFeat, R-CNN “reduces” the number of things to process to some regions.
Dengan begitu, R-CNN berhasil meningkatkan performa dari OverFeat hingga hampir 50%![1].
Meskipun peningkatan nya cukup signifikan, prosestrainingdari R-CNN memiliki banyak kendala. Kita perlu menghasilkan region sebelum dapat memulai prosestraining! Selain itu, tidak hanya kita perlu melatih CNN itu sendiri, kita juga perlu melakukantraininguntuk SVM yang digunakan [1].
Oleh karena itu, setahun berikutnya, yaitu pada tahun 2015, Ross Girschick mengembangkan R-CNN menjadiFast R-CNN.Alur kerja dari Fast R-CNN sedikit berbeda dari sebelumnya, dimana bila sebelumnya tiap region-region dariregion proposal method(Selective Search) akan memiliki CNN untukfeature extractionmasing-masing, Fast R-CNN hanya menggunakan 1 CNN.Feature mapyang dihasilkan kemudian “dicocokan” dengan Region of Interest (ROI) yang didapat dari Selective Search tadi, untuk kemudian diklasifikasi kelas nya dan dideteksibounding boxnya. Dengan kata lain,Fast R-CNN melakukanfeature extractionsebelum mengajukan regions.[1,2]
R-CNN juga meninggalkan penggunaan SVM sebagai classifiers, menggantinya dengan ROI pooling danfully-connected layers.Pendekatan dengan 1 CNN, ROI pooling layer dan feed forward network ini tidak hanya mempercepat performa R-CNN menjadi lebih cepat, tetapi juga menambah kapabilitas R-CNN menjadiend-to-end differentiabledan juga mempermudah prosestraining(karena tidak perlu lagi melakukantrainuntuk SVMs dan hanya melatih 1 arsitektur CNN ketimbang banyak CNN seperti sebelumnya).
Penggunaan region proposal method seperti Selective Search masih menjadi “bottleneck”dalam proses deteksi objek menggunakan R-CNN [1], karena metode seperti ini membutuhkan waktu yang relatif lama untuk menghasilkan regions [2].
Tidak lama setelah Fast R-CNN,Faster R-CNNhadir untuk mencoba menyelesaikan masalah region proposal method diatas. Dikembangkan oleh Shaoqing Ren dan dibantu oleh Girshick (pelopor R-CNN dan Fast R-CNN), Faster R-CNN memperkenalkanRegion Proposal Network (RPN).
RPN adalah sebuahneural networkyang menggantikan peran Selective Search untuk mengajukan region (bagian-bagian mana dari sebuah gambar yang perlu “dilihat” lebih jauh). RPN menghasilkan beberapabounding box,setiap box memiliki 2 skor probabilitas apakah pada lokasi tersebut terdapat objek atau tidak.
Region-region yang dihasilkan tersebut akan menjadi input untuk arsitektur yang mirip seperti Fast R-CNN. Arsitektur Fast R-CNN dapat dilihat pada gambar dibawah ini.
Penggunaan RPN untuk mengganti Selective Search ini mengurangi kebutuhan komputasi yang cukup signifikan, dan membuat keseluruhan model dapat di-trainsecaraend-to-end(karena tidak lagi ada metode yang digunakan terpisah seperti dengan region proposal method sebelumnya).
Selain itu, Faster R-CNN juga menghasilkan performa yang lebih cepat dan lebih akurat bila dibandingkan dengan Fast R-CNN [2], dan saat ini telah menjadi pilihan model yang umum digunakan untuk deteksi objek dengan solusi berbasisdeep learning[1].
Begitulah sekilas perkembangan teknikdeep learninguntuk deteksi objek. Beberapa model terkini memang terbukti dapat meningkatkan kecepatan deteksi, tapi tidak dalam margin yang besar bila dibandingkan dengan Faster R-CNN [2].
Secara natural, teknik atau algoritma yang menggunakan proses terpisah mulai ditinggalkan karena umumnya akan meningkatkan waktutrainingdan berkurangnya akurasi. Tren perkembangan arsitekturdeep learninguntuk deteksi objek mulai mengarah kepada arsitektur yang dapat mendeteksi objek lebih cepat serta sistem deteksi yang lebih efisien. Arsitektur “baru” yang memperhatikan hal-hal tersebut diantaranyaSingle Shot MultiBox Detector(SSD) danYou Only Look Once(YOLO). Kita akan membahas YOLO secara lebih khusus pada post berikutnya.
Most of my professional work has been related to how technologies can help people - first as an engineer, and later as a product and growth strategist.",https://miro.medium.com/v2/resize:fit:700/0*Fr5Zvo4INCaGS22Y.jpg,"Deep Learning, Artificial Intelligence, Machine Learning, Computer Vision"
https://medium.com/s/story/word-clustring-from-twitter-search-data-f6c0067fc5ac,Word Clustring from twitter search data,Rangga Rizky A,2018-06-02T08:08:20.388Z,"Clustering adalah mengelompokan data berdasarkan kesamaan nilainya dan memisahkan dari kelompok yang nilainya berjauhan. Clustering meruapakn unsupervised learning. tujuan dari word clustering adalah mencari kelompok kata yang mirip dan saling berhubungan satu sama lain. dengan begitu kita dapat mengetahui pola dari kelompok kata yang ada pada dokumen kita. dengan memanfaatkan twitter search data saya akan mencoba melakukan word clustering pada data hasil dari query search kata “infrastruktur”. pertama-tama kita crawling terlebih dahulu data dari twitter
tahap pertama adalah melakukan pre-processing text. dimana tahapan tahapan dari pre-processing text sendiri terdiri dari tokenisasi, penghapusan kata lebih kecil dari 3 , penghapusan tanda baca, dan penghapusan username twitter. setelah data text dibersihkan. langkah selanjutnya adalah membentuk term-document binary matix. dimana daftar distinct term menjadi baris dan dokumen menjadi kolomnya. pada matrix ini bernilai 1 dan 0. 1 jika dokumen tersebut terdapat kata tersebut dan 0 jika sebaliknya.
untuk mendapat kan kata-kata yang lebih bermakna maka dilakukan agregasi dan filtering. sehingga kata yang diambil hanya kata yang memiliki frekuensi kemunculan diatas 5.
kemudian langkah selanjutnya adalah dilakukan clustering disini saya akan menggunakan cluster kmeans dengan jumlah 4 kluster
Kemudian saya lakukan dimensi reduksi hingga menadi 2 dimensi saja menggunakan singular value decomposition sehingga dapat divisualisasikan ke scatter plot.
Dari hasil visualisasi diatas menggunakan 4 cluster dapat diambil sebuah knowledge. bahwa cluster 1 merepresentasikan macam proyek-proyek infrastruktur. untuk cluster 2 merupakan kumpulan kata-kata yang berasal dari tweet yang berisi link gambar di twitter dan cluster 3 merupakan infrastruktur yang berhubungan dengan persiapan lebaran. untuk cluster 4 sendiri hanya berisi satu kata yaitu infrastruktur dimana cluster ini dapat dikategorikan sebagai anomali.
What cannot be proven is wrong. Cannot be proven correctly",https://miro.medium.com/v2/resize:fit:943/1*nC79sn030gTD-7d7r_vvkw.jpeg,"Data Science, Text Mining, Machine Learning"
https://medium.com/s/story/data-preprocessing-template-f76326393721,Data Preprocessing Template,R. Kukuh,2018-02-13T03:55:45.328Z,"Halo semua, selamat datang dibagian terakhir dari seri tutorial Data Preprocessing.
Jika Anda mengikuti tutorial ini dari awal, berarti Anda telah melakukan berbagai macam teknik data preprocessing untuk kebutuhan Machine Learning modelling nantinya, seperti:
Namun dalam kebanyakan kasus, kita akan jarang menemui suatu dataset yang perlu diproses sebanyak langkah-langkah diatas. Seperti misalnya:
Oke, seperti judul artikel ini. Mari kita membuat suatu template untuk urusan data preprocessing. Template ini akan berisi kode-kode yang telah kita tulis, dengan optimasi (pengurangan kode yang tidak perlu) seperlunya.
Buka Anaconda Navigator, lalu buka Spyder
Lanjutkan project sebelumnya tentangData Preprocessing 06
Ubah kode yang sudah ada secara keseluruhan menjadi seperti ini:
Lanjutkan project sebelumnya tentangData Preprocessing 06
Ubah kode yang sudah ada secara keseluruhan menjadi seperti ini:
Sr. Software Dev Learning Facilitator at Apple Developer Academy @UC",https://miro.medium.com/v2/resize:fit:1200/1*a2GnvT-2IlmWuCqUcVPV7w.png,"R, Data Preprocessing, Python, Data Science, Machine Learning"
https://medium.com/s/story/tips-membuat-portofolio-yang-bakal-bikin-profilmu-outstanding-f7acfae4c7b3,Tips Membuat Portofolio yang Bakal Bikin Profilmu Outstanding,IYKRA,2018-07-26T04:48:51.913Z,"Building Future Capabilities
Menjadi seorang data scientist memang merupakan salah satu pekerjaan yang cukup menjanjikan pada jaman modern seperti sekarang. Hal ini terkait dengan perkembangan data dan teknologi yang terus meningkat setiap tahunnya.
Seperti yang telah kita ketahui, profesi data scientist selain menjanjikan dalam jenjang karier, gaji yang ditawarkan juga cukup besar. Meskipun saat ini, tenaga ahlinya masih sangat terbatas.
Lalu, skill apa saja yang harus dimiliki oleh seorang data scientist? Dan bagaimanakah membuat portofolio yang bagus agar dapat menarik perhatian sebuah perusahaan terhadap kemampuan kita?
Seorang data scientist harus mampu menguasai pemrograman atau coding dalam mengolah suatu data. Pemahaman yang menyeluruh mengenai statistika juga merupakan hal yang paling mendasar bagi seorang data scientist. Selain untuk menentukan algoritma yang akan digunakan, ilmu statistika diperlukan untuk mengembangkan software machine learning yang berfungsi sebagai pusaka seorang data scientist.
Machine learning pun digunakan untuk mengolah banyaknya data tak terstruktur. Data scientist juga harus menguasai kalkulus dan aljabar, pemetaan data, komunikasi dan visualisasi data, menguasai software engineering serta intuisi agar dapat memecahkan masalah dengan mudah.
Sementara itu, bagaimanakah cara untuk membuat portofolio untuk seorang data scientist? Salah satu caranya adalah memiliki sebuah blog pribadi. Ya, setiap kali Anda mendapatan project udahakan simpan dokumentasinya dalam blog tersebut. Hal ini berguna agar manager perekrutan dapat dengan mudah menemukan proyek-proyek karya Anda dan mudah untuk mengevaluasi keterampilan yang Anda miliki.
Berikut adalah contoh jika anda ingin menampilkan olahan data yang sudah anda kerjakan di situs pribadi. Felipe Hoffa, seorang developer di Google membuat tulisan dari World Cup 2018 silam. Ia menceritakan most valuable player selama pertandingan lewat visualisasi.
Lainnya, adalah Ravi Suhag. Data Engineer dari GO-JEK. Ia membuat situs dimana ia berbagai mengenai berbagai proyek yang ia kerjakan.
Proyek harus terdokumentasi dengan baik dengan file yang disusun rapi yang isinya menjelaskan setiap detail tentang data. Jika ingin menunjukkan keahlian Anda, mungkin dapat menggunakan Kaggle, LinkedIn, Twitter, Facebook untuk mengisi resume pada profile Anda.
Selain itu, banyak juga data scientist yang menggunakan situs coding Github untuk menyimpan portofolio mereka. Meski mereka mengaku masih sulit menggunakan situs ini, namun Github cukup menjadi andalan bagi para data scientist.
Sebagai contoh, gambar diatas adalah contoh portofolio dari Github. Lewat penggunaan Github, developer bisa bekerja solo ataupun berkolaborasi dengan pihak yang dikehendaki. Baik 2 orang atau bahkan lebih.
Untuk anda yang baru memulai karier, jangan panik. Anda bisa mengambil berbagai dataset di situs berikut untuk berlatih:
Demikianlah hal-hal yang bisa ditempuh agar portofolio anda sebagai seorang data scientist bisa dilirik. Jangan lupa mencoba ya!
medium.freecodecamp.org
Penulis: Aprilia Safitri
Building Future Capabilities
Crafting Technology Capabilities, IYKRA’s vision is to build an Artificial Intelligence (AI) Talent Ecosystem in Indonesia.www.iykra.com",https://miro.medium.com/v2/resize:fit:1200/1*RDlSyWFYApylBSAhgwaiCw.png,"Data Science, Portfolio, Developer, Professional"
https://medium.com/s/story/aplikasi-dan-penggunaan-artificial-intelligence-sebagai-sarana-pendidikan-karakter-moral-dan-fb550c50b0e2,"Aplikasi dan PenggunaanArtificial IntelligenceSebagai Sarana Pendidikan Karakter, Moral, dan Mental Anak-Anak, Remaja, Maupun Dewasa di Indonesia",David Petra,2018-07-17T14:21:16.203Z,"(Tugas Karya — David Petra Natanael — 16517168)
Sebagaimana kita ketahui, kita sudah berada dan hidup di zaman milenial yang berdasarkan zamannya digitalisasi di tiap hal. Mungkin bagi kita yang lahir di era 90-an masih bisa merasakan perbedaan yang cukup besar dibanding generasi yang lahir tahun 2000-an keatas. Perbedaan yang sangat signifikan berasal dari aspek teknologi. Teknologi membawa pengaruh besar bagi orang-orang terutama bagi generasi kita dan generasi sekarang. Kita yang lahir terlebih dulu masih bisa merasakan begitu sederhananya teknologi di zaman kita, namun bagi anak-anak yang lahir di zaman ini, sudah disuguhkan oleh kemajuan teknologi yang sangat pesat.
Memang kemajuan teknologi ini punya sisi baik dan sisi buruk. Sisi baiknya adalah sangat memudahkan generasi sekarang ini untuk belajar dan berinovasi karena semua hal dapat dilihat dimana dan kapan saja. Tapi, yang saya lihat dibalik itu semua yaitu adanya sisi buruk atau negatifnya yang menurut saya cukup mempengaruhi kehidupan kita di zaman ini. Sisi negatif ini berhubungan dengan karakter, moral, dan mental generasi yang lahir tahun 2000-an keatas. Banyak anak yang baru berumur 4 sampai 5 tahun keatas sudah mengerti cara bermain dismartphone. Selain itu, anak-anak SD sudah memilikismartphonepribadi bahkan sudah mengerti dan ikut membuat akun di berbagai media sosial tanpa adanya pengawasan orang tua. Dan kita juga telah tahu bahwa media sosial dan internet memiliki banyak konten yang kurang mendidik. Orang tua seringkali bisa menjadi korban berita hoax. Beberapagamesjuga kurang cocok untuk anak yang masih dibawah umur.
Maka dari itu, saya memiliki ide berupa solusi atas permasalahan ini. Solusi yang bisa mendidik karakter, moral, dan mental mereka menurut saya ialah dengan menggunakan teknologi juga yang berbasisArtificial Intelligence.Artificial Intelligenceatau kecerdasan buatan (AI) bisa memudahkan kita mendidik karakter, moral, dan mental anak-anak ketika orang tuanya tidak bisa mengawasi mereka. Mungkin bisa suatu saat nanti dilakukan pengembangan sebuah alat yang dipasang disebuah rumah. Alat ini merupakan sebuah AI yang juga bisa memancarkan sinyal khusus berupa data ke setiap perangkat elektronik yang ada di rumah. Mungkin alat ini akan mirip seperti cara kerja Wi-Fi atau bahkan perangkat IoT (Internet of Things).
Kemudian tentunya sebelum bisa diterapkan di masyarakat, perlu ada uji khusus. Jadi AI ini juga yang akan mengontrol sendiri penggunaan internet dansmartphoneyang ada dirumah sesuai dengan umur penggunanya berdasarkan data yang sebelumnya diuji dan sudah dianggap benar. AI ini otomatis ada dismartphoneatau tablet yang dimainkan apabila AI yang dipasang dirumah juga aktif. Misalnya, ketika anak kecil ingin bermaingamesmaka AI ini akan memilah-milahgamesyang tentunya sesuai umur dan memiliki nilai pendidikan sehingga karakternya bisa berkembang. Contoh lainnya yaitu untuk anak-anak dan remaja yang memiliki media sosial, AI ini bisa berperan sebagai pemilah konten yang baik dan buruk, tentunya sesuai umur juga agar anak-anak dan remaja tidak terpengaruh pemikiran yang tidak baik, apalagi radikalisme. Tak hanya konten media sosial, AI ini juga bisa memilah aplikasi yang baik dan kurang baik. Selain itu, tak hanya mencegah, tapi AI ini diharapkan bisa juga memberikan saran pembelajaran minat dan bakat dari internet. Jadi sebagai anak-anak atau remaja pun AI ini akan belajar sesuai dengankeywordsyang dicari oleh penggunanya dan setelah itu pun memilahnya sesuai minat dan bakat mereka sehingga bisa lebih mengembangkan potensi dirinya.
Selain itu pun, untuk kalangan remaja, AI ini tak hanya selalu menampilkan konten media sosial terus-menerus, tapi juga harus diberi edukasi lebih misalnya lewat berita yang tentunya terpercaya, ataupun pembelajaran di sekolah atau kuliah, atau bisa juga mengenai rasa empati terhadap masyarakat sekitar agar moralnya semakin meningkat. Kemudian untuk kalangan dewasa, AI ini diharapkan bisa memilah-milah bahan bacaan yang ada di media sosial, berita, ataupun internet. Hal ini untuk mencegah maraknya berita atau info hoax yang membuat resah masyarakat. Karena menurut saya, rata-rata info hoax tersebar via media sosial (WhatsApp dan Facebook) dan platform berita yang kurang bertanggung jawab. Berita atau info hoax tersebut bermacam-macam dan yang berbahaya bagi kita yaitu mengenai perpolitikan, kesehatan, dan agama. Hal-hal tersebut bisa membuat orang dewasa tidak mencerminkan “dewasa” yang sesungguhnya dan yang juga berbahaya bila sewaktu-waktu sifat-sifat buruk yang didapatkan orang dewasa tersebut bisa diturunkan kepada anaknya. Maka dari itu, diharapkan AI ini bisa menyingkirkan semua info dan konten tersebut via database yang mereka miliki.
Mekanisme database ini tentunya sudah seharusnya ada di platform online dan terkoneksi satu sama lain antar rumah, atau istilahnya semua data yang digunakan untuk memilah-milah konten, berita, dan info ini sudah berbasiscloud computingyang bisa menerima dan mengambil data secara tak terbatas, kapan saja, dan dimana saja. Tentunya hal ini juga harus disupport oleh ketersediaan internet di setiap rumah. Dalam hal ini pemerintah harus bisa mengupayakan kedepannya seluruh warga di Indonesia memiliki internet di setiap rumahnya sehingga sistem AI ini pun bisa digunakan dengan efektif.
Tentunya, AI ini juga bila sudah sukses dan berhasil diuji coba dirumah-rumah, maka bukan hal yang tak mungkin apabila AI ini dipakai di instansi-instansi pemerintah yang akan membantu dalam pekerjaan pemerintah membangun Indonesia dan tentunya dengan transparan, tidak curang, serta bisa mengontrol sistem yang diterapkan dengan baik sehingga mental kejujuran, integritas, dan moral orang-orang dikalangan pemerintah pun bisa berkembang juga. Bila semua hal itu sudah berkembang, maka kita pun sebagai masyarakat tentunya juga memiliki sifat dan karakter yang baik, dan bisa menjunjung tinggi nilai-nilai yang telah diturunkan secara turun-temurun sesuai Pancasila.
Pada akhirnya, saya berharap dari ide yang mungkin cukup sederhana ini bisa dikembangkan suatu saat nanti agar semua hal diatas bisa terwujud dan membuat Indonesia semakin maju, karena yang membuat Indonesia maju yaitu dimulai dari diri kita sendiri. Maka dari itu kita juga perlu karakter, moral, dan mental yang baik agar bisa meneruskan perjuangan dan memajukan bangsa ini. Saya sebagai mahasiswa yang berlatar belakang teknologi, tentunya akan patut berpartisipasi memajukan bangsa ini melalui teknologi juga. Saya harap karya ide ini kelak akan bisa diterapkan dan bisa berguna buat masyarakat Indonesia.",,"Character Development, Artificial Intelligence, Ideas, Indonesia, Inventions"
https://medium.com/s/story/klasifikasi-gambar-menggunakan-r-mxnet-fcb19994e9da,Klasifikasi Gambar Menggunakan R-MxNet,Hafizhan Aliady Afif,2018-01-24T04:19:01.593Z,"Well Asalamualaikum People, berhubung postingan sebelumnnya ngebahas tentang gimana sih caranya mesin mengenali gambar. nah di story kaliini mari membahas gimana caranya klasifikasi gambar Pake R
Sebelum kita mulai mari install package “Mxnet” dan “ebimage” caranya pake perintah berikut di R ya
Nah, 2 package di atas merupakan package yang digunakan untuk model deeplearning nya dan untuk merubah gambar menjadi angka. Setelah selesai mari panggil beberapa library, jika belum tersedia silahkan install terelebih dahulu
Setelah semua library bisa di muat dengan baik, mari membuat fungsi dimana fungsi ini merubah gambar dari 1 folder menjadi sebuah data matriks.
Dari fungsi di atas jadi intinya gini, pertama itu package ebimage membaca gambar dari folder yang di definisikan > kemudian resize gambar ke ukuran yang di inginkan> kemudian gambar tersebut dirubah ke grayscale dimana awalnya gambar tersebut memiliki 3 cannel warna yaitu RGB disini yang di gunakan hanya grayscale saja, jadi hanya 1 channel > lalu data tersebut di ambil matriksnya kemudian disimpan menjadi 1 dataset dengan kategori yang sama.
Nah sekarang masuk ke prosesnya, disini menggunakan data mnist yang berupa kumpulan angka tulis tangan, datanya selahkan download di kaggle dengan keyword “Mnist Jpg Dataset” atau bisa download dis.id/fFX. setelah itu extract dan simpan di 1 folder. jangan lupa untuk ubah directory di syntax di bawah sesuai dengan punya kalian
Perintah di atas ini buat ngeliat gambarnya udah masuk belum sih, ternyata gambarnya udah bisa terbaca oleh R
tunggu proses nya sampai selesai, disini emang makan ram yang lumayan besar dari pc kita jadi sabar aja nunggunya, ini masih belum berat sih wkwk
Setelah berhasil semuanya lalu semua data disatukan menjadi 1 dataframe menggunakan perintah di atas.
Dan Perintah di atas untuk menyimpan data keseluruhan dengan format rds(rdataset) dan yang baris ke2 digunakan untukload datadari file yang kita sudah simpan tadi. silahkan gunakan jika di perlukan.
Disini kita membuat partisi dari datanya menggunakan library caret disini memisahkan data train dan data test dimana 90% untuk data train dan 10% untuk data test.
Perintah ini digunakan untuk memisahkan mana data gambar dan mana labelnya, lalu 2 baris terakhir merupakan konfigurasi untuk matriks input kedalam model dimana urutannya adalah (tinggi gambar,lebar gambar,channel gamar, jumlah gambar). jadi nanti hasil dimensi matriksnya adalah (32,32,1,37801) bagian belakang adalah jumlah gambarnya.
Perintah di atas juga sama untuk data test dan yg sebelumnya untuk data train.
Perintah di atas merupakan perintah untuk membentuk model Convolutional NN. Perintah di atas merupakan layer pertama
Perintah di atas merupakan layer ke-2
Perintah di atas merupakan (fully conected) layer ke-3
Perintah di atas merupakan fully conected layer ke-4 yang merupakan node/ titik akhir dimana Neural Netnya akan melakukan klasifikasi, jangan lupa untuk set num_hidden dengan kategori yang kalian miliki. Disini kita menggunakan data mnist yang memiliki 10 kategori.
Perintah di atas merupakan perintah untuk menjalankan model untuk melakukan epochs dalam metode pembelajaran mesin. jangan lupa untuk set jumlah epochs di num.round, jumlah ini bebas tergantung kalian. disini saya set 8 biar cepet aja hehe.
Nah gambar di atas ini si mxnet lagi proses belajar, dia pake lumayan ram yang gede. Jadi tunggu aja sampe selesai hehe
Nah dari 10x train udah bagus tuh nyampe 99% di train ke 6. berarti struktur cnn yang di pake udah mantep. selanjutnya masuk ke akurasi dari data test
disini kita sudah dapat modelnya, kemudian dicoba gimana sih dia klasifikasiin data testnya.
DAN! dapet nih mayan dapet 98.8% untuk klasifikiasinya. Jadi cukup sekian dulu untuuk story yang sekarang
mantap kan. nah untuk syntaxnya ini bisa di pake untuk data lain dengan mengganti di bagian input data. untuk selanjutnya bisa di tingkatkan dimensi gambarnya yang tadinya 32x32 pixel bisa di naikin jadi 100x100 gitu wkwk biar nanti 1 gambar punya 10.000 data. kan mantep.
Cerita ini masih bersambung sih selanjutnya mari bercerita tentang cara masukin gambar yang kita buat bisa di klasifikasiin menggunakan model yang udah kita buat.
Problem Solver | ML / AI Engineer",https://miro.medium.com/v2/resize:fit:480/0*A1AyQugs75An7V9L.jpg,"Mxnet, R, Machine Learning"
https://medium.com/s/story/perusahaan-startup-gosu-ai-fda3a1807657,"Perusahaan Startup Gosu.ai, Pencipta Aplikasi Pelatihan Esports Berhasil Mengumpulkan 1,9 Juta Dolar",Ira Atika Zahra Soewandhi,2018-04-09T11:49:27.148Z,"Sebuah publikasi informasi berbahasa Indonesia yang aktif untuk mengedukasi masyarakat Indonesia dengan pengetahuan dan juga temuan dari data ataupun analisis seputar fenomena atau peristiwa yang terjadi di industri esports.
Pendiri Game Insight, Alisa Chumachenko, telah meluncurkan sebuah perusahaanesportsbaru, yang baru saja mengumpulkan pendanaan sebesar 1,9 juta Dolar.Gosu.aimenggunakan teknologi kecerdasan buatan untuk memberi masukan dan pelatihan secara langsung kepada pemainnya, mulai dari merekomendasikan pilihanitem buildhingga menyarankan strategi bermain untuk situasi tertentu.
Gosu.ai memberi saran dan masukan untuk paracompetitive gamers.
Saat ini, program pelatihan tersebut dapat digunakan untukpermainan Dota 2. Sedangkan untuk judulCounter Strike: Global Offensivedan PLAYERUNKNOWN’S BATTLEGROUNDS, masih berada di tahap pengembangan. Pada situsnya, gosu.ai juga memberitahukan rencananya untuk memberi dukungannya padagameHearthstone dan World of Tanks pada tahun 2019.
Babak penggalangan investasi dipimpin oleh Runa Capital, bersamaan dengan partisipasi Ventech dan Sistema VC.
Perusahaan tersebut akan mengumumkan kemitraannya dengan perusahaan lain, dan tidak hanya berfokus dalam membantu melatih atletesports, tetapi juga pemain amatir yang ingin meningkatkan kemampuannya.
Babak penggalangan investasi dipimpin olehRuna Capital; bersamaan dengan partisipasi dari Ventech dan Sistema VC, yang menginvestasikan 420 ribu Dolar pada Oktober lalu.
Dmitry Chikhachev,managing partnerRuna Capital mengatakan: “Kami terus mendukung para pendiri yang mengembangkan aplikasi berbasismachine learninguntuk berbagai jenis industri. Gosu.ai menggabungkan semua hal yang dicari olehVenture Capitalistberpengalaman: industricybersportyang berkembang pesat denganbig datayang terstandardisasi dan dapat diakses, tim kuat yang dipimpin oleh Alisa, pengusaha yang berpengalaman dalame-gaming, dan produk yang disukai pengguna.”
Gosu.ai kini masuk ke dalam daftar perusahaan-perusahaan yang menggabungkan bidang ilmu kecerdasan buatan denganesports. Bulan lalu, sebuahplatformmonetisasi audiensesports,FanAIsukses mengumpulkan dana sebesar 2,5 juta Dolar. Pada daftar tersebut, juga terdapat perusahaan teknologi AI milik Elon Musk,OpenAI, yang dimana perusahaan tersebut mengajar sebuah program untuk dapat memainkangame esportsseperti Dota 2 dan Starcraft II dengan kemampuan tingkat tinggi.Startupasal Jerman,Shikenso,berfokus untuk mengintegrasikan kecerdasan buatan pada fungsi filter Twitch, menambahkan opsi tambahan bagi para penggemar untuk menontonstreaming.
Akan semakin banyak perusahaan yang memanfaatkan kecerdasaran buatan untuk memajukan dan memonetisasiesports.
Dengan teknologi yang kian berkembang, kita mungkin akan melihat lebih banyak perusahaan yang mampu mengutilisasi kecerdasan buatan untuk memajukan dan memonetisasiesports.
Tulisan ini pertama kali diterbitkan diventure.ninja.
Sebuah publikasi informasi berbahasa Indonesia yang aktif untuk mengedukasi masyarakat Indonesia dengan pengetahuan dan juga temuan dari data ataupun analisis seputar fenomena atau peristiwa yang terjadi di industri esports.",https://miro.medium.com/v2/resize:fit:1080/0*GpNU4Z-dEsqCeFhg.png,"Fanai, Artificial Intelligence, Indonesia, OpenAI, Esports Indonesia"
https://medium.com/s/story/kebun-kurma-thailand-telp-wa-0822-4069-7469-kebun-kavling-kurma-thailand-fe28164b6995,"Kebun Kurma Thailand, Telp / Wa 0822–4069–7469, Kebun Kavling Kurma Thailand",basirul akbar,2018-08-14T23:10:56.805Z,"Hari ini banyak sekali hal yang bisa Anda temui di berbagai macam berita, baik di dunia maya maupun media cetak hingga elektronik lainnya. Salah satu bahan pemberitaan yang tidak habis — habisnya ini adalah tentang investasi.
Coba saja Anda lihat di televisi, pasti familiar sekali dengan acara yang mempromosikan investasi property yang dikelola oleh developer properti terkenal. Di berita lainnya seperti media cetak atau berita di internet, berbagai macam tawarantempat investasi terbaikpun bermunculan menawarkan kelebihannya masing — masing.
Memang benar adanya jika investasi menjadi sebuah kepentingan saat ini mengingat persaingan ekonomi yang begitu berat, kemajuan teknologi setiap hari yang juga berdampak pada segala macam kebutuahn hidup yang terus meningkat sementara tidak akan selamanya Anda bekerja, pun tidak akan selamanya kita memimpin sebuah perusahaan atau membuat sebuah usaha.
Kavling Taman Kurmaini adalah sebutan bagi lahan atau kavling tanah yang diperjual belikan. Kavling tersebut nantinya boleh di bangun menjadi villa, resort, atau bahkan rumah tinggal.
Ketika Anda membeli tanah kavling diKavling Taman Kurma, Anda akan diberikan Lima bibit pohon kurma berikut dengan perawatannya hingga berbuah. Karena itulah kawasan ini disebut sebagaiKavling Taman Kurmayang salah satu tujuannya adalah untuk mengembangkan lahan Kurma di Indonesia dan Kalimantan Khususnya.
Anda pun tidak akan rugi jika beli kavling diKavling Taman Kurmakarena harga tanah selalu naik dan ini akan jadi investasi yang terbaik untuk Anda.
Tempat Investasi Terbaik adalah Investasi Properti Syariah
Sebagai seorang muslim, segala macam kegiatan khususnya dalam kegiatan perekonomian harus dilakukan sesuai dengan ketentuan hukum islam. Hal yang sama juga berlaku saat Anda akan mencari sebuah investasi masa depan.
Carilah investasi berbasis syariah yang bebas riba dan semua tata cara pelaksanaan dilakukan sesuai dengan syariat Islam.Kavling Taman Kurmasebagai salah satu contoh investasi syariah paling baik karena sudah pasti bebas riba, DP dan Angsuran juga tidak akan dibuat ribet, dan juga sudah memilikilegalitas.
Dengan memilih investasi berbasis syariah, maka hasil yang akan Anda dapat pun sudah pasti terjamin halal. Dengan demikian, kita juga akan hidup jauh lebih tenang di masa depan bersama anak cucu Anda.
Selagi masih muda dan memiliki penghasilan cukup dari bekerja, mulailai untuk sisihkan sedikit demi sedikit untuk kita putarkan ditempat investasi terbaikseperti Membeli Kavling Taman Kurma.
Dengan mulai berpikir untuk melakukan investasi, artinya Anda sudah peduli dengan kehidupan di masa depan terutama juga untuk kelangsungan Hidup anak, istri, hingga cucu dan buyut Anda di masa depan.
Achmad Solihin Prajamas
Untuk Berdiskusi Lebih Lanjut, Berikut Link WA Saya
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
https://goo.gl/vgdjmi
Bisa Juga Langsung Datang Ke PonTren IT Madinatul Iman Balikpapan
Jalan Prajabakti VII Blok II D No 15 RT. 07 Belakang Kantor DISHUB Sebarang Taman 3 Generasi, Rumah Dua Lantai Pagernya Warna Hijau Depan Posyandu RT. 07",https://miro.medium.com/v2/resize:fit:720/1*6ILyNimXaKefsnpMZ0gXHQ.jpeg,"Mulheres Que Escrevem, Mobile App Development, Maintenance, Motivation, Machine Learning"
https://medium.com/s/story/visualisasi-data-tingkat-pendidikan-anak-di-jawa-tengah-dengan-python-fe6e775648a2,Visualisasi Data Tingkat Pendidikan Anak di Jawa Tengah Tahun 2012 dengan Python,Farhan,2018-04-23T15:36:11.993Z,"Visualisasi data adalah bagian penting dariData Scienceterutama saat bersentuhan denganDescriptive Analytics.Dengan visualisasi data kita bisa membaca dan menganalisis data untuk diambilvalueatauinsightdari data dengan mudah.
Gambar di atas emang ga nyambung, buat hiasan aja sich hehe.
Pada kesempatan kali ini saya akan mempraktikan bagaimana merapikan dan memvisualisasikan sebuah data denganPythonuntuk dianalisa bagaimanatingkat pendidikan anak miskin di Jawa Tengahyang saya ambil dari dataTingkat Pendidikan Anak Miskin di Indonesia. Jadi, dari sebuah dataset Tingkat Pendidikan Anak Miskin di Indonesia tidak semua saya ambil untuk divisualisasikan, namun hanya tingkat pendidikan anak miskin di Jawa Tengah saja yang akan saya ambil.
Dalam data tersebut terdapat variabel sebagai berikut:1. kode_provinsi: Kode Provinsi2. nama_provinsi: Nama Provinsi3. tingkat_pendidikan: Tingkat Pendidikan (SD/MI/SMP/MT/SMA/MA/Perguruan Tinggi)4. jenis_kelamin: Jenis Kelamin5. jumlah_individu: Jumlah dari Anak dengan Tingkat Pendidikan dan Jenis Kelamin tersebut
Yaudah langsung saja hhe..
Perlu diketahui, saya menggunakanJupyterNotebookyang bisa menampungtextdancode,ditambahcommand%matplotlib inlineyang bisa membuat plot diagram ditampilkan dalam satu halamancodedantext,atau dengan kata lain plot bisa muncul tanpa jendela tambahan.
Sebelum di-importtentu sebelumnya saya sudah download data yang berformat .csv tersebut darihttps://data.go.id/dataset/tingkat-pendidikan-anak-miskindan me-renamenama file menjadidata.csv. Kemudian saya masukan ke variabledatauntuk ditampilkan sebagaidataframeseperticodedi atas.
Untuk melihat preview data bisa kita lihat dengancodedi atas, yang menampilkandataframesebanyak 5 baris pertama data seperti gambar di bawah:
3. Hapus Kolom yang Tidak Dibutuhkan
Hapus kolom yang tidak diperlukan seperti kolomkode_provinsikarena memang kolom tersebut tidak diperlukan hehe. Makadataframehasilnya akan menjadi seperti gambar berikut:
4. Mencari nomor index provinsi Jawa Tengah
Seperti yang saya jabarkan di atas jika saya akan memvisualisasikan hanya data Jawa Tengah saja, maka sebelum mengambil data Jawa Tengah kita harus mengetahui terlebih dahulu di mana nomorindexprovinsi Jawa Tengah dengancodedi bawah ini:
Daricodedi atas akan menghasilkanoutputlist berisi diindexnomor berapa sajakah Jawa Tengah berada, yaitu nomor 168 sampai 181.
4. Membuatdataframebaru
Setelah mengetahui di mana nomorindexlokasi data Jawa Tengah selanjutnya kita bisa membuatdataframebaru yang hanya berisi data untuk provinsi Jawa Tengah saja dengancodeberikut:
Dataframebaru yang akan saya visualisasikan saya berinama jawatengah seperti yang kita lihat padacodedi atas. Lalu saya juga menghapus kolom “nama_provinsi” karena ya memang sudah tidak diperlukan saja hehe.
5. Visualisasikan Data
Nah, setelahdataframerapi selanjutnya kita bisa visualisasikan, pada data ini akan saya visualisasikan dalam bentukbar chartyang berisi 2 bar dalam tiap kategori. Yaitubaruntuk jenis kelamin pada tiap tingkatan pendidikan yang saya masukan sebagai kategori dan, tentu saja jumlah_individu sebagaivalue-nya.
Hasil visualisasinya adalah sebagai berikut:
Nah, seperti yang kita lihat, kategori pada tingkat_pendidikan tampat tumpuk menumpuk sehingga tidak jelas, maka dari itu harus saya rapikan terlebih dahulu, dengan cara me-replacenama kategori menjadi lebih singkat dengancodedi bawah ini.
Selanjutnya kita buat plot dan tampilkan kembali plotnya.
Hasilnya jadi lebih rapi dengan singkatan yang sudah saya buat seperti gambar di bawah ini.
Setelah rapi begini barulah kita bisa menganalisa dan mengambil kesimpulan dari data seperti misalnya: kebanyakan anak miskin di Jawa Tengah mengenyam pendidikan sampai SD, dan ternyata antara laki-laki dan perempuan masih lebih banyak laki-laki pada tiap jenjang pendidikan yang ditempuh.
Nah, sekian dulu praktek visualisasi datanya ya, disambung lagi dengan praktek lainnya nanti hehe bye..
Catatan pemahaman saya yang sedang mempelajari Big Data, dan Machine Learning. Portfolio lainnya:github.com/theinternetbae/",https://miro.medium.com/v2/resize:fit:1024/1*g5B1pJRUUB54CxXXHGYwFA.jpeg,"Big Data, Data Visualization, Python, Data Science, Machine Learning"
https://blog.javan.co.id/oracle-digital-prime-ajak-pelaku-bisnis-terapkan-komputasi-awan-1b254f1e4267,Oracle Digital Prime Ajak Pelaku Bisnis Terapkan Komputasi Awan,Katon Fergiawan B,2018-07-13T03:39:31.928Z,"Business Process Optimization Partner. Contact us to help you optimize your business using technology.
Katonfergie - OK… Alhamdulillah beberapa waktu yang lalu saya dipercayakan oleh perusahaan untuk menghadiri acara yang diselenggarakan oleh oracle digital, yang berjudul “Oracle Executive Workshop — Bridging to The Age of Artificial Intelligence”. Beberapa speaker yang hadir pun sangat komunikatif dalam penyampaikan ke audiensi seperti Fitrah Muhammad (Director of sales oracle asia), Deepanshu Mital (Oracle solution specialist), Peter Pang (Director Oracle APAC), Freddy Darmawan (Oracle Director) dan beberapa speaker lainnya. Pertemuan tersebut secara langsung memperkenalkan produk baru dari oracle yaitu oracle digital prime yang sekaligus mengajak pelaku bisnis untuk migrate ke komputasi awan atau cloud.
Pada pembukaan acara tersebut di mulai dari Bpk Fitrah Muhammad tentang apa itu oracle digital prime dan isu bahwa oracle itu teknologi yang mahal itu salah. Berikut beberapa poin yang saya ambil dari paparan beliau.
Isu oracle itu mahal itu tidak benar, solusinya ada teknologi baru yaitu cloud, atribut cloud itu sendiri adalah value. Artinya subscribe service cloud bukan membeli tapi bisa dicustom perbulan atau mungkin hanya 2 bulan tapi bukan perpaket. Subscribe to use dari capital expenditure (Capex) ke operational expenditure or Opex. (You pay as you use) dan lebih flexibel.
All of them is Integrated Offering ☝
All of them=engineering to work together ☝
Point-point diatas adalah keuntungan dari migrasi darion-premisekecloud.Dan sebagai informasi tambahan diprediksi di tahun 2.020 hampir 80% pelaku industri akan migrasi semua ke sistemcloudini. Dan saat ini Badan Intelijen Pusat Amerika (CIA) sudah menggunakan teknologi tersebut. Untuk pelaku industri yang ingin bermigrasi ke cloud dan sebelumnya sudah pernah memakai oraclelicencetidak usah khawatir karena dengan bermigrasi ke cloud akan mudah dilakukan akan tetapi dengan tambahan penghitungan komersial tentunya.Customer Benefitsyang akan didapatkan setelah berpindah kecloudbisa menekancost,dan meningkatkan inovasi, serta transformasi bisnis akan meningkat dan juga performa dari sistem yang dibangun semakin lebih baik.
Oracle Ravello Cloud Service adalah layanan cloud pertama di industri yang memungkinkan perusahaan mengambil beban kerja VMWare dan Kernel-based Virtual Machine (KVM) dan menjalankannya di cloud publik tanpa perubahan apa pun. Dengan Oracle Ravello Cloud Service, pelanggan bisa menjalankan VMWare secara asli di cloud publik tanpa membutuhkan konversi VM, rekonfigurasi aplikasi ataupun perubahan jaringan. Oracle Container Cloud Service menyediakan cara yang kompatibel dengan Docker untuk menerapkan stack aplikasi dengan satu tombol saja. Kemampuan integrasi registrasi, orkestrasi aplikasi tingkat perusahaan, dan penjadwalan aplikasi serta kemampuan berskala menjadikan Oracle Container Cloud Sevice sebagai pilihan yang unik untuk pengembang cloud. Oracle juga memperkenalkan Oracle FastConnect — edisi standar dan partner — untuk membantu pelanggan menghubungkan pusat datanya ke Cloud dengan terpercaya dan aman. Dengan adanya Oracle FastConnect, Oracle kini menawarkan beberapa konektivitas terluas dan banyak pilihan di industri, termasuk IPsec, VPN untuk setup yang mudah, MPLS untuk terhubung ke jaringan perusahaan yang ada dan Fast Connect untuk melakukan peering dengan cepat.
Analogi Oracle Ravello Cloud Service
“Ok sekarang bagaimana analoginya kk? Karena sejak awal ngebahas teknis mulu… “
Tenang bro.. Ane jelasin ya
Jadi gini bro, kita kasih gambaran secara umum mengenai apa itu ravello secara umum. Gambar diatas adalah gambar sebuah pulau dari suatu titik segi empat ke titik bulat atau sebaliknya. Gambar biru adalah daratan/pulau dan background putih itu diibaratkan sebuah lautan. Jika kita menggunakan jalur darat atau jalur merah maka kita menggunakan kendaraan darat seperti mobil atau motor dll (VM Ware). Dan akan menempuh jarak yang panjang dan lama serta tidak efisien kan?…
Coba kita menggunakan jalur kuning untuk melewati pulau itu.. apakah bisa? tentu bisa… Tetapi kita harus menggunakan media… Apa medianya?
Tentu kapal laut (ravello) solusinya. Jadi kapal laut sebagai media kita untuk menyebrangi lautan ke tujuan yang kita capai. Efeknya waktu yang kita tempuh semakin cepat dan biaya yang dikeluarkan semakin sedikit. Ok kita bedah satu persatu proses kita melewati jalur laut (cloud) ini atau kita sebut jalur cloud.
Proses ini dimana mobil atau kendaraan kita akan tiba dipelabuhan dan kendaraan kita akan dimasukan ke kapal secara otomatis. Dalam hal ini bahasa teknisnya adalah nested (kendaraan dalam kendaraan) dan kita hanya membayar kendaraan yang di bawa oleh kapal laut saja tidak lebih.
2. Tempat Parkir atau Storage (No Modification)
Proses ini dimana kendaraan kita akan diparkirkan didalam kapal dan secara otomatis akan mendapatkanspaceatau ruang untuk parkir kan? Dan jika kita sudah sampai tujuan kendaraan yang dibawa oleh kapal laut menjamin tidak akan merubah bentuk, rusak ataupun mengalami perubahan fisik.
Nah semua proses diatas dinamakan ravello cloud
Source Vm yang mendukung untuk ravello antara lain: VM Ware, KVM dan Xen. Untuk Support OS yang didukung adalah Windows dan Linux. Untuk Database bisa menggunakan Oracle DB dan Opensource App. Jika sistem sudah menggunakan on-premise maka akan mudah untuk melakukan migrate ke oracle cloud dengan menggunakan ravello. Salah satu kelebihan saat migrasi adalah tidak perlu melakukan setting IP kembali karena semua sudah terkontainer dengan baik.
Sesuai dengan namanya, Big Data didefinisikan sebagai sebuah problem domain di mana teknologi tradisional seperti relational database tidak mampu lagi untuk melayani dan tidak lepas dari teknologi komputasi awan. DefinisiBigdi sini adalahvolume, velositas dan variasi datanya. Peningkatan volum, velositas dan variasi data banyak diakibatkan oleh adopsi internet. Setiap individu memproduksi konten atau paling tidak meninggalkan sidik jari digital yang berpotensial untuk digunakan untuk hal-hal baru; dari audiens targetting, rekomendasi ataupun penggunaan yang lebih tak terduga seperti Google Translate yang menggunakan machine learning di atas Big Data yang Google punya untuk translasi bahasa.
Hadoop (in memory discover indexes)Apache Hadoop menawarkan fitur MapReduce yang memungkinkan kita melakukan prinsip-prinsip yang disebut di atas. Hadoop banyak dipakai oleh perusahaan web dan Startup yang kita kenal sekarang seperti Yahoo, Facebook, Foursquare dsb.
Begitu pula di sisi enterprise, vendor-vendor solusi enterprise merangkul Hadoop untuk mengatasi masalah Big Data di dalam enterprise. Microsoft (Windows Azure Hadoop), Oracle (Big Data Appliance yang mencakup solusi Hadoop dari Cloudera),ORAAH, SAP (Hana), EMC (GreenPlum Hadoop) adalah beberapa contoh solusi di space ini.
FYI: Untuk pertanyaan lebih lanjut mengenai oracle cloud computing atau ingin menggunakan produknya bisa menghubungijavan
Business Process Optimization Partner. Contact us to help you optimize your business using technology.
Interest of IT things",https://miro.medium.com/v2/resize:fit:1200/1*NtqhHceOqPfZnrZ8FFE0Vw.jpeg,"Hadoop, Artificial Intelligence, Oracle, Vmware, Oracle Cloud"
https://medium.com/qasir/technical-debt-dalam-software-development-eb824f40839e,“Technical Debt”dalam Software Development,Rahmad Hidayat,2020-06-23T07:07:33.769Z,"Aplikasi untuk membantu usahawan berkembang lebih baik dimulai dari pencatatan transaksi
Dalamdevelopmentsebuah software, sebagaiengineersering kali kita berhadapan dengan penentuan berapa lamatimelineyang dibutuhkan untuk men-developsebuah fitur yang ideal dari sudut pandangengineervstimelineyang diberikan olehproduct team.
Timelineyang dipersingkat biasanya menghasilkanTechnical Debtkarenaengineertidak mempunyai waktu yang cukup untuk berpikir secara jangka panjang maupun memikirkanbest overall solutiondari sebuahcode.
Sangat lumrah jika beberapa prosestechnicaldipangkas oleh engineeruntuk membantuproduct teammengejar momentum, terutama diStartup. Tapi tentu ada n̶y̶a̶w̶a̶ harga yang harus dibayar di kemudian hari.
Inilah yang disebut utang teknikal aliasTechnical Debtatau kita singkat“tech debt”.
Utang jika tidak dibayar, akan menghasilkan bunga. Semakin besar bunganya, semakin berat pula untuk melunasi utang. Jika kita tidak bisa membayar utang, semakin lama hidup kita akan semakin tidak nyaman karena bakal dikejar-kejardebt collector.
Ini juga terjadi terjadi dalamsoftware development,tech debtyang besar akan mempersulitscalabilitydanmaintenabilitydari sebuahsoftware. Mungkin awalnya bisa saja kita “akali” implementasinya agar tetap bisadeliver. Tapi semakin lama,tech debtini akan mengurangi produktivitas karena kita harus mencari cara kotor hingga akhirnya tidak bisa lagi menambahkan fitur sama sekali. Bahkan yang lebih buruk, kita juga akan kesulitanme-maintain softwarekita, karena ketika kita baca code nya, isinya tambal sulam sana-sini. Ruwet lah, kalo kata Pak Jokowi.
Lalu dengan apa kita membayarnya? Tentu saja dengan “refactoring”.
Oke, cukupintro-nya. Setelah ini saya ingin berbagi pengalaman tentang apa yang kami lakukan untuk mengatasitech debtdi Qasir.
Hal pertama yang kami lakukan adalah menyamakan persepsi. Tidak semuastakeholderpaham tentangtech debt. Biasanya yang sering disalahpahami adalahbugs, masalahUI /UX, dan fitur-fitur yang tertunda dianggap termasuktechnical debt.Padahalsebenarnyatidak.
Saya suka penjelasantech debtdariAgile Allianceini:
“The Technical Debt concept is an effective way to communicate about the need for refactoring and improvement tasks related to the source code and its architecture.
Code issues may be related to architecture, structure, duplication, test coverage, comments and documentation, potential bugs, complexity, code smells, coding practices and style. All these types of issues incur technical debt because they have a negative impact on productivity.”
SingkatnyaTechnical Debtterjadi ketika kita membangunsoftwaresecara cepat dan tidak mendesign untuk jangka panjang.
Hal-hal yang sepertibugs,UI/UX, fitur yang dikurangi, memang termasuk utang, tetapi tidak dikategorikan sebagaitech debt. Untuk mempermudah pemahaman, mari kita kategorikan jenis-jenis utang ini dengan istilah lain.
Selain dari masalahcode,tech debtjuga mencakup masalah dari pihak ke-3. Bisa seperticodeyangdeprecated,Libraryyang sudah tidak di-maintain, ataupun kemajuan teknologi diplatformyang kita pakai sehingga membuatcodekita tidakrelevanlagi dan membuatsoftwareyang kita buat cepat atau lambat sulit untukscale-updan di-maintainatau malah menurunkan produktivitas kita.
Setelah semuastakeholdersepemahaman, kita lanjut ke perencanaan dan eksekusinya.
Tidak ada solusi yang singkat untuk menanganitech debtini. Hal pertama yang kami lakukan adalah dokumentasitech debt. Mendokumentasikantech debtadalah hal yang wajib untukengineer. Kemudian kita urutkan berdasarkanimpactdaneffort-nya. Setelah itu kita ajukan dan presentasikan keproduct teamuntuk meminta wakturefactoringdi dalamproduct roadmap.
Semudah itu? tidak juga. Kita harus bisa menerjemahkan manfaat teknis darirefactoringyang akan kita lakukan ke dalam kepentingan bisnis dan produk. (dibaca: meyakinkan tim produk). Hal ini sangat penting, karena ketika antar-stakeholdersama-sama mengerti konteks,urgency, dan mendapatkan nilai lebih dari sebuahrefactoring,akan lebih mudah untukproduct teammemprioritaskan antara fitur /refactoring.
Strategi pertamayang kita lakukan adalah menyisipkan waktu 30% di tiapsprintuntuktech debt. Satusprintumumnya mengalokasikan waktu 2 minggu yang dibagi menjadi 7 haridevelopmentdan 3 hariQA testing. Artinya kita bisa memakai 2 hari kerja itutech debtdi setiap sprint.
Tetapi untuk menyelesaikantech debtyang kompleks, 30% tiap sprint itu sangat-sangat kurang dan sulit untuk memecah pekerjaannya untuk tiap sprint.
Strategi keduayang kita lakukan sekarang adalahtech debtpunya wadah sendiri yang dinamakansprint cooldowndan dieksekusi tiap kuartal, dengan 1sprintyang berdurasi 2 minggu (bisa lebih, bisa kurang, tergantung dengan pekerjaanrefactoringapa yang kita angkat dan seberapaimpact-nya).
Selain dari sprintcooldownyang kita punya, diengineeringteamjuga menerapkan satu peraturan sederhana yang sangat berguna untuktech debtyang sifat nya kecil-kecil:The Boy Scout Rule.
Prinsip sederhana yang di adopsi dari Boy Scout Rule adalah:
“Leave the campground cleaner than you found it.”
Seorang pramuka, ketika selesai berkemah membiasakan diri untuk membersihkan tempat dia berkemah lebih bersih dari saat dia datang.
Setiap kaliengineermengubah sebuah code,engineerproaktif membersihkancode-smellyang ditemukan diclasstersebut diluar dari fitur yang sedang dikerjakan.
Sederhananya seperti masalahtypo,naming, kompleksitas sebuahmethod,style, duplikasi sebuah helperdan hal kecil lainnya bisa diatasi secara berkala dan tidak membutuhkan waktu yang lama menjadi kebiasaan yang sangat membantu mengurangitech debtdan meningkatkantechnical ownership.
Tidak ada yang salah dari strategifaster time-to-market. Tetapistakeholderdiluarengineering teamharus punyaawarenessbahwa cepat atau lambattech debtakan mengganggu kita.
Tech debttidak akan pernah selesai selamasoftwareterus dikembangkan. yang kita perlukan adalah strategi untuk terus menjagatech debttidak menjadimonsteruntuk kita dan kita terus melakukan iterasi untuk hal ini.
By the way,ini tulisan pertama saya di Qasir. Silakan tekan tombolclapssebanyak-banyaknya jika tulisan ini bermanfaat, dan mari berdiskusi di kolom komentar tentang hal terkait yang mungkin tidak ter-coverartikel ini.
Aplikasi untuk membantu usahawan berkembang lebih baik dimulai dari pencatatan transaksi
I talk about software engineering especially mobile apps and my whole experience during my career. Currently my role is Sr Principal Engineer at SaaS Company.",https://miro.medium.com/v2/resize:fit:1200/1*OPpKFpgdTCPisQRV92U9Yg.jpeg,"Software Development, Tech, Startup, Technical Debt, Tech Debt"
https://medium.com/tunaiku-tech/apache-airflow-is-fun-for-data-engineer-b9b0b45a2550,Apache Airflow Is Fun For Data Engineer!,Salim Masagus,2020-01-05T06:08:22.409Z,"Stories behind Tunaiku Products, Engineering and Data Team
Apache Airflow implementation in Tunaiku
Pada tulisan pertama saya di medium ini, saya akan bercerita tentang pengalaman saya menggunakan AirflowsebagaiData Engineerdi Tunaiku. Sekilas tentangAirflow,Data EngineerTunaiku menggunakanAirflowterkhusus dikarenakan kebutuhanschedulling task,danmonitoring workflowyang telah dirancang untuk kebutuhan data di Tunaiku, dengan menggunakanAirflowsemua prosesautomasidanmonitoringakan terhandle!!! . untuk lebih jelasnya anda dapat melihat dokumentasinyadisini, dan untuk memahamietentialtentangData Engineer, anda dapat membacablog creator Airflowmaxime beauchemindisini
To a modern Data Engineer, traditional ETL tools are largelyobsoletebecause logic cannot be expressed using code. As a result, the abstractions needed cannot be expressed intuitively in those tools. Now knowing that the Data Engineer’s role consist largely of defining ETL, and knowing that a completely new set of tools and methodology is needed, one can argue that this forces the discipline to rebuild itself from the ground up. New stack, new tools, a new set of constraints, and in many cases, a new generation of individuals. — Maxime Beauchemin
Pada case kali ini saya menggunakan case membuat automate report yang datanya berasal dariBigQuery.Sekilas tentang BigQuery, BigQuery merupakan sebuahanalytic data-warehouseyang termasuk ke dalam produk Google Cloud Platform (GCP), untuk lebih lengkapnya anda dapat melihat dokumentasinyadisini
berikut konfigurasi saya ketika membuatAirflow:
Pada proses pengambilan data dari beberapa table sesuai dengan kebutuhan report, berikut contoh script pengambilan data di BigQuery :
pada script diatas, saya menuliskan query yang mana menuliskan semua kolom yang ingin saya ambil, dibanding menggunakan ‘*’, kenapa saya memanggil semua kolom ? , anda dapat melihatdisiniuntuk penjelasannya. pada kondisi wheredate(_PARTITIONTIME)ini merupakan sebuah partition pada table di BigQuery agar menghemat cost dan mempercepat proses query.
setelah penulisan query, hasilnya akan saya simpan kedalam dataFrame, setelah itu akan saya loads ke ndjson (NewDelimitedJson) untuk menyesuaikan spesifikasi yang diperlukan BigQuery dalam Load Data to BigQuery using File.
Berikut contoh script mengirimkan data ke BigQuery :
Pada proses ini, saya load data ke sebuah table yang sudah di buat partition menggunakan partition_field, yang mana partition nya menggunakan sebuah kolom, dan padadestinasion_dataset_ref,saya mendefine table diikuti oleh lambang ‘$’ dan tanggal tanpa ‘-’, itu merupakan cara define table menggunakan partition.
Dikarenakan timData EngineerTunaiku telah sepakat menggunakan script dari pada operatorAirflowuntuk memproses data, maka dari itu, saya menggunakan bash operatorAirflowuntuk menjalankan script saya, bash operator sama seperti bash pada terminal. berikut contohnya :
Terdapat ds, itu merupakan macros variable untuk mendapat kan tanggal dengan formatYYYY-MM-DD, untuk lebih lengkapnya dapat melihatdisini
Hasilnya, ketika seorangData Engineermelihatdagsnya memperlihatkan warna hijau hijau, harusnya sudah dapat tersenyum karena proses yang dibuatnya telah berhasil! , dan akan automatis sesual dengan settingschedulenya.
“No man is so wise that he can afford to wholly ignore the advice of others.” — James Lendall Basford (1845–1915), Sparks from the Philosopher’s Stone, 1882
Dengan masuknya kita ke notimportant section!berakhir sudah sharing knowledge saya kali ini, saya harap ini adalah awal dari lanjutan tulisan saya di medium, dan ini jauh dari kata sempurna, akan tetapi ingatlahMark Zuckerberg tidak membuat Facebook dalam percobaan pertamahehehe :D
Saya harap artikel yang saya tulis ini dapat membantu semua orang, terkhusus bidangData Engineer. Jika Anda memiliki pertanyaan atau diskusi lebih lanjut, Anda dapat menuliskan komentar, atau menghubungi saya melalui email. Terima kasih!
Some rights reserved
Stories behind Tunaiku Products, Engineering and Data Team
A man with fast learner, enthusiastic, high achiever, collaborative, breaker the limit, and growth mindset.",https://miro.medium.com/v2/resize:fit:1200/1*FJsMPN5kPMI7JuqhsaP7rA.png,"Apache Airflow, Data, Software Engineer, Google Cloud Platform, Data Engineer"
https://medium.com/@devitawidya/analisis-data-marketplace-sales-235755601e0d,Analisis Data Marketplace Sales,Devita Widyasari,2020-12-23T15:08:25.466Z,"Saya mendapatkan data penjualan barang dalam sebuah marketplace. Data yang digunakan adalah dataorder listyang berisi record order yang dilakukan setiap harinya, dataorder detailsyang berisi detail pembelian barang, dan datasales targetyang berisi target jumlah penjualan setiap bulannya untuk setiap kategori. Seluruh dataset dapat diunduh dengan klik nama datanya.
Beberapa hal yang ingin diketahui dari data-data tersebut, yaitu:
Selanjutnya, import library yang dibutuhkan. Library pandas saya gunakan untuk mengeksplor dataset. sqldf saya gunakan untuk gather data sebelum visualisasi. untuk visualisasi sendiri saya gunakan library plotly.
Import ketiga dataset.
Order ID adalah ID untuk setiap pembelian barang. Order Date adalah tanggal pembelian barang. CustomerName adalah nama pembeli. State adalah negara bagian, dimana pembelian tersebut dilakukan. City adalah kota dimana pembelian tersebut dilakukan.
Order ID adalah ID untuk setiap pembelian barang. Amount adalah total harga dari barang-barang yang dibeli / berhasil terjual. Profit adalah keuntungan yang didapat perusahaan dari setiap pembelian. Quantity adalah jumlah barang yang dibeli. Category adalah kategori barang. Sub-Category adalah sub-kategori barang.
Month of Order Date adalah bulan dan tahun target pembelian. Category adalah kategori barang. Target adalah angka target penjualan.
Rename (ubah nama kolom) dengan menghilangkan spasi pada nama kolom agar mudah untuk penulisan kodenya.
gunakan function .info() untuk mengetahui informasi mengenai kolom dan tipe data, dan deteksi missing value setiap kolomnya dengan function .isnull(). sum().
Dalam dataset Order List terdapat 60 baris kosong di setiap kolomnya. jika ingin menghapusnya, bisa menggunakan function .dropna(), namun tidak di-drop pun tidak berpengaruh karena selanjutnya dilakukan inner merge dengan dataset orderdetails. Lalu ubah tipe data dan format tanggal OrderDate menjadi datetime mm-YYYY untuk memudahkan agregasi selanjutnya, lalu rename menjadi ‘Month’ karena telah menjadi data bulan pembelian. Selain itu, hapus kolom CustomerName karena tidak akan digunakan untuk analisis selanjutnya.
Sebelum menggabungkan table orders dan salestarget, terlebih dahulu mengubah tipe data kolom Month dari object menjadi datetime dengan format yang sama yaitu mm-YYYY. Karena sebelumnya pada data salestarget, kolom Month berformat bbb-YYYY.
Join-kan kedua table dengan key Month dan Category.",https://miro.medium.com/v2/resize:fit:739/1*VpafAHqRMwWvDc-GrNXpEg.png,"Pandas, Plotly, Data Wrangling, Data Analysis"
https://medium.com/@adtiyadwiramadani98/membuat-resful-api-menggunakan-codeigniter4-f4f6951b2c4f,Membuat Restful Api Menggunakancodeigniter4,ADITIYA DWI RAMADANI,2021-01-04T02:05:44.645Z,"Hello teman-teman jumpa lagi. bersama saya Aditiya Dwi Ramadani. Di tutorial Kali ini kita akan membuat Restful Api menggunakan framework codeigniter4.
codeigniter sendiri Merupakan Framework Yang Menggunakan bahasa Pemograman Php . Dengan Konsep Mvc. Yaitu Model Views Dan Controller . Codeigniter Sendiri Mempuyai Ukuran Yang Kecil Teman Teman dan Fiturnya Juga Lumayan Banyak. Ok Jadi Itu Pengertian Singkat Tentang Codeigniter.
Untuk Menginstall Codeigniter Harus Install Php dulu di Laptop Atau Computer Kalian. Atau Kalian Bisa Download Xampp DiSiniTeman Teman.Kemudian Kalian Install Composer Dulu Di Laptop Atau Computer Kalian. Kalau Belum Megerti Tentang Composer Teman Teman Bisa Klik Link DiSini. Atau Teman Teman Bisa Langsung Download Composer DiSini.
Kalau Suda Semua Buka Terminal Kalian. Untuk Di windows Bisa Buka Cmd.Kemudian Ketik Di Bawah Ini.
Ok Kalian Suda Menginstall Codeigniter Computer Kalian Masing-Masing. Buka text Editor Kalian Masing-Masing. Kemudian copy file env menjadi .env.
Buka File .env.Kemudian Cari CI_ENVIRONMENT. Mejadi Seperti Di Bawah.
Sedikit Penjelasan. Kita Menggubah ENVIRONMENT Teman Teman Menjadi development. Kalau Kalian Udah Deploy Appnya. Kalian Edit ENVIRONMENT Menjadi Semula Ya Teman Teman.
Ok Kalian Buka folder Controllers nya Kemudian Tambahkan File Base.php.Buka Filenya Tabahkan Di Bawah Ini Teman Teman.
oK Kita Suda Membuat Controllernya. Kemudian Buka Lagi Filenya . Teman Teman.Di Dalam class Base Kalian Tabahkan Di Bawah Ini.
Kemudian. Buka Terminal Teman Teman. Kemudian Ketik php spark server.Dan Coba Buka Browser Teman Teman. Ketik Di Urlnya. localhost:8080. Maka Akan Seperti Gambar Di Bawa Ini.
Kemudian Buka Xampp Teman Teman. Aktifkan Apache Sama Mysqlnya.
Buka Terminal Lagi Teman. Kemudian Ketik. php spark migrate:create add_blog. kemudian buka File Database/Migration/2020–12–25–012344_add_blog.php. File Nya Pasti Beda Teman Teman.Sesuai Waktu Kalain Membuat Migrasi databasenya Teman Teman. Di Dalam Methods up. Tambahkan Di Bawah Ini Teman Teman .
Kemudian Edit File .env nya Cari DATABASE di file nya Edit. Seperti Di Bawah Ini
Kemudian Buka Browser Kalian Lagi. Kemudian Ketik .localhost/phpmyadmin. Kemudian Kalian Click New. Lihat Contoh Gambar Di Bawah Ini.
Kalau sudah Kalian Buka Terminal Kalian.Ketik php spark migrate.
Kemudian Kalian Buka Folder App/Models. Kemudian Buat File Yang Bernama Blog.php. Kemudian Buka Filenya. Dan Tambahkan Di Bawah ini.
Kemudian Kalian Ke Controllernya Lagi . Untuk Create Dan Read . Kemudian Kalian Edit Filenya. Seperti Di Bawah Ini
Kemudian Kalian Buka FIle App/Config/Routes.php. Dan Kalian Edit Filenya Seperti Di Bawah Ini
Kemudian Untuk Details, Delete Dan Edit.Buka File Controllenya.Kemudian Kalian Edit Seperti Di Bawah Ini.
Kemudian Buka File App/Confing/Routes.php. Dan Kemudian Edit Seperti Di Bawah Ini.
Finally. Kalian Suda Membuat Rest Api Menggunakan Ci.
Ok Trimakasih Suda Membaca.",https://miro.medium.com/v2/resize:fit:1000/1*8lLCWmv1aYJZbbUSXaz7mA.jpeg,"Rest Api, PHP, MySQL, Codeigniter"
https://medium.com/@ridzki-hidayat/utak-atik-konseling-psikologi-di-halodoc-ux-case-study-4d7cedece123,Utak-atik Konseling Psikologi di Halodoc (UX Case Study),Rdzkday,2021-02-16T21:20:19.392Z,"Hallo! Setelah melewati draft UI_Final_Revisi_akhir_VVI.figma errr, ga ding canda, sekarang kansemua udah auto save. Akhirnya prototype kami, saya (UX Designer);gempitaff(UX Researcher); danAndhika S Pratama(UX Writer), selesai dan siap untuk di test kepada pengguna setia layanan Halodoc. Sembari menunggu hasil Usability Testing (UT), simak dulu yuk udah sejauh apa sih perubahan yang kami buat.
Sebelumnya, kami menggunakan Design Thinking dalam proses pembuatannya, udah paham kan ya? Bagus. Lanjut! Alasan kenapa kami memilih Halodoc sendiri simpel. Kami merasa aplikasi ini bisa di tingkatkan lebih baik lagi dari beberapa hal minor yang kita temukan serta kita validasi ke user. Thanks buatgempitaffyang telah memandu proses researchnya.
Oke jadi problemnya adalah …
“Bagaimana membuat user merasa nyaman saat berkonsultasi dengan psikolog?”
Sebuah pertanyaan yang bersifat general sebenarnya. Kami tidak berafiliasi dengan Halodoc, atau mungkin belum? Jadi kami belum memiliki data aktual dan/atau permasalahan terkait app Halodoc dari sudut pandang internal. Kami hanya bisa menerka sambil memvalidasi terkaan kami dengan banyak bertanya kepada user.
Oke selamat datang di malam-malam dan akhir minggu penuh diskusi dalam rangka menerapkan apa yang kami pelajari sembari berupaya menambah nilai jual alias portfolio. Namun pada postingan kali ini saya tidak akan banyak menjabarkan keseluruhan prosesnya karena kadang melakukan lebih mudah dibanding menjelaskan, betul? yaa.
Sedikit gambaran, kami melalui proses research, affinity diagram, meramu design mandate, ideate, dan jargon-jargon lainnya yang ada di design sprint ala aplikasi Duco serta beberapa langkah lain terkait UX Writing yang akan dijelaskan olehAndhika S Pratamadangempitaffdalam tahapan Researchnya.
Sedikit memberikan pandangan, dari serangkaian proses empati yang kami lakukan, beberapa problematika yang kami temukan terutama adalah lama waktu konsultasi, metode konsultasi, jadwal konsultasi psikolog, dan metode pembayaran.
Oke baik, saya akan langsung menjelaskan data/input yang kami dapat dan implementasinya dalam desain yang kami buat. Jika penasaran dengan proses lengkapnya, bisa langsung hubungi saya atau kasih komentar dibawah yaa, hehe.
Tidak banyak perubahan di bagian ini, hanya peletakan posisi yang kami ubah dengan berpedoman padathumb rule.Banner hanyalah banner, user hanya perlu mengambil instisarinya saja sehingga bagian dengan aksesibilitas tinggi bagi ibu jari dapat digunakan untuk menu utama.
Kami mengeliminasi shortcut yang ada di navbar dari 5 menjadi 4. Alasannya adalah bila navigasi bisa dibuat lebih mudah dengan fungsi yang lebih maksimal, kenapa tidak? Dengan pengurangan ini kami juga berharap fungsi shortcut pesan yang berisikan iklan atau informasi tambahan akan lebih banyak diakses oleh pengguna. Kami cukup terinspirasi dari tulisaneconomist. Untuk alasan penyederhanaan lainnya mari kita bedah halaman profil.
Ini alasan utama navbar dapat dirampingkan, keberfungsian halaman profil dapat ditingkatkan dengan langsung memperlihatkan riwayat penggunaan Halodoc sehingga shortcut riwayat transaksi (yang isinya bukan seperti transaksi😅) di navbar tidak lagi dibutuhkan.
Saldo halodoc serta paket konsultasi yang sudah dibeli oleh user juga diperlihatkan. Sebelumnya saya berupaya mencari dan tidak menemukan cara untuk mengetahui saldo Halodoc saya. Jadi saya coba menaruhnya disini.
Isi dari menu Pesan dan Lainnya (pengaturan) tidak mengalami perubahan karena bukan menjadi fokus pengerjaan kali ini. Namun ada beberapa hal yang menjadi catatan dibagian pengaturan, seperti pengaturan aplikasi diganti saja jadi pengaturan bahasa yang dapat langsung diubah dengan sekali klik. Kemudian untuk paket sehat dan metode pembayaran sudah bisa diakses di card paling atas di laman Profil.
Mari masuk ke menu utama, kesehatan mental. Sebelumnya terima kasih untuk design rant yang telah menyorot halaman ini di postinganIG, kami jadi semakin terpacu untuk melakukan pembenahan disini.
Halaman ini kami jadikan sebagai gerbang utama sekaligus cara mempromosikan kesehatan mental yang baik. Kami menemukan bila user sebelum memutuskan berkonsultasi membutuhkan pertimbangan sekaligus kekhawatiran sendiri. Alih-alih memperbesar kekhawatiran itu, kami berupaya menenangkan user.
Ide dari halaman ini adalah untuk memberi sambutan yang menenangkan kepada pengguna yang sedang terpikir untuk bercerita dengan psikolog atau psikiater. Dengan rancangan desain baru ini kami juga berupaya menjadikan halodoc sebagai aplikasi harian dengan fitur mood tracker dan artikel harian.
Pertimbangan user dalam memilih psikolog yang menjadi temuan kami diantaranya adalah gender, pengalaman praktik, wajah psikolog dan berapa banyak pengguna yang telah menggunakan layanan psikolog tersebut. Namun sayangnya disini kami hanya menggunakan unsplash untuk mengisi foto psikolognya, jadi apresiasi untuk Halodoc yang telah memiliki standar dalam menampilkan foto partnernya!
Salah satu temuan besar kami adalah user ingin menggunakan metode lain dalam berkonsultasi, seperti call atau video call. Selidik punya selidik, di Halodoc sebenarnya kita memang sudah bisa menggunakan fitur video call loh! adakah yang pernah mencobanya? Coba tulis di komentar yaa!
Selain itu akses untuk melihat jadwal konsultasi yang dimiliki psikolog pun kami tampilkan langsung. Salah satunya karena temuan kami mengatakan user cenderung ingin bercerita ke satu psikolog yang sama saja ketimbang berganti-ganti psikolog. Sehingga jadwal psikolog perlu dapat diketahui lebih mudah untuk membantu user berkonsultasi dengan psikolog favoritnya.
Selanjutnya adalah pembayaran, bagian yang cukup tricky karena ada beberapa penyesuaian terkait dengan fitur baru yang kami ajukan untuk diimplementasikan.
Bagaimana jika user berencana bercerita banyak hal kepada psikolog? Tentu ia membutuhkan waktu lebih banyak. Kami berusaha mewadahi hal tersebut dengan kesempatan untuk memilih durasi konsultasi sesuai kebutuhan user. Sementara frasa Estimasi Biaya dipilih karena user nantinya bisa memperpanjang durasi konsultasinya yang akan dijelaskan kemudian.
Selanjutnya akan masuk sesi konsultasi dengan di dahului ruang tunggu.
Ruang chat ada dengan pertimbangan kemungkinan user perlu melampirkan foto dalam sesi konsultasi atau mengetik suatu hal semisal koneksi tidak stabil atau hendak menginformasikan teks tertentu.
Sementara apabila koneksi bermasalah hingga menyebabkan sambungan terputus maka akan berlanjut ke halaman berikut.
Bagaimana bila user masih di tengah sesi konsultasinya sementara waktu sudah hampir habis? Di sini kami memberikan kesempatan user untuk memperpanjang waktu sesi konsultasinya dengan metode pembayaran sesuai dengan pilihan yang diambil di halaman pembayaran sebelumnya.
Sistem rating psikolog kami ubah menjadi hanya dua, yakni terbantu (jempol keatas) atau tidak (jempol kebawah) hal ini untuk memudahkan informasi seberapa banyak user yang terbantu oleh psikolog tersebut di tampilan card psikolog di halaman pencarian. Sementara rating untuk layanan psikolog berbentuk lima bintang untuk lebih menggali seberapa terbantunya user oleh layanan yang disediakan oleh Halodoc sebagai bahan evaluasi internal aplikasi.
Rancangan ini tentu masih jauh dari kata ideal, salah satunya terutama karena belum di ujicobakan langsung kepada user halodoc itu sendiri. Namun sejauh ini kami mendapat banyak pelajaran berharga seperti bagaimana tim yang solid dan memiliki satu visi bersama dapat menciptakan inovasi dalam memecahkan masalah yang ada. Saya sendiri ditengah prosesnya setelah berguru dan bertanding di innovation battlefield jadi lebih memahami hierarki antar konten, menerapkan panduan dari material.io dan mendapat banyak masukan mengenai desain saya dari grup designjam danAssyifa Narulitasaya jadi merasa selangkah lebih dekat dalam memahami UI Design meski di pengerjaan kali ini masih banyak yang terlewatkan.
Jika kalian ingin melihat prototypenya, silahkan mampir ke laman berikut ya!
Sekian dan terima kasih sudah membaca hingga selesai. Untuk artikel lain bisa dilihat di lamanmedium, jika ingin menyapa bisa mampir keIGataulinkedinsaya. Sampai bertemu di artikel berikutnya!
UX Designer enthusiast",https://miro.medium.com/v2/resize:fit:1000/1*YYMMCYoonDzGI6t3wBgIBg.png,"UX Design, Ux Case Study, UI Design, Medical Apps, Counseling Psychology"
https://medium.com/nusanet/flutter-ui-instagram-desktop-c0ed262b9278,Flutter UI: Instagram Desktop,Yudi Setiawan,2021-05-06T02:00:08.889Z,"Stories and insights from the developers in Nusanet
Menjelang akhir tahun 2020 ini saya ingin membahas mengenai Flutter Desktop. Saya lebih tertarik ke desktop karena dulunya saya pernah develop aplikasi desktop pakai J2SE. Dan ini membuat saya penasaran apakah Flutter Desktop mampu memberikan saya kesan pertama yang luar biasa atau biasa-biasa saja. Untuk menjawab pertanyaan tersebut mari kita bahas ditulisan ini ya. Jadi, pada tanggal 20 Desember 2020 tepat tulisan ini saya publikasikan saya lihat Flutter telah mampu untuk melakukan pengembangan untuk aplikasi desktop MacOS, Linux, dan Windows hanya saja belum ready to use di production ya. Info tersebut bisa kamu baca di link berikut ya.
Di tulisan ini nantinya saya akan berusaha menduplikat design UI Instagram Desktop dari Dribble.
Dan mau tahu hasilnya sepertinya apa? Berikut adalah perbandingan antara design UI dan output di Flutter Desktop.
Saya rasa hasilnya benar-benar cukup memuaskan ya. 😃
Sebelum memulai pembuatan projeknya saya ingin memberitahukan diawal bahwa pada tulisan ini yang saya bahas adalah sebatas menduplikasi design UI-nya. Jadi, UI-nya itu belum ada interaksi ataupun event yang bisa diklik. Dan untuk masalah responsive pun tidak akan saya bahas pada tulisan ini. Oleh karena itu, mohon dimaklumi jika ternyata ketika teman-teman menjalankan aplikasinya dan hasilnya tidak sesuai dengan digambar pada tulisan ini mengingat mungkin resolusi
Adapun beberapa persiapan yang perlu kita persiapkan adalah sebagai berikut:
Silakan buat projek di IDE favoritmu dan buat nama projeknyainstagram_desktop.
Selanjutnya kita perlu setup filepubspec.yaml-nya menjadi seperti berikut.
Untuk aset gambar dan font-nya silakan kamu unduh disini. Lalu, kamu ekstrak file tersebut dan masukkan ke dalam projek seperti gambar berikut.
Selanjutnya, jangan lupa jalankan perintah berikut.
Jadi, pada tulisan ini kita hanya menggunakan 1 file saja yaitu filemain.dart. Untuk pertama-tama silakan ubah kode di dalam filemain.dartmenjadi seperti berikut.
Dan berikut adalah outputnya.
Sekarang kita akan buat layout dasarnya. Silakan buka filemain.dartdan ubah kode didalamnya menjadi seperti berikut.
Selanjutnya kita ubah kode di dalamWidgetNavigationDrawermenjadi seperti berikut.
Langkah berikutnya kita akan membuat widget yang menampilkan logo Instagram. Untuk membuatnya kita perlu buat satu fungsi yang bernama_buildWidgetInstagramLogodan isi dengan kode berikut.
Selanjutnya, kita ubah kode todo berikut.
Menjadi seperti berikut.
Berikut adalah perubahan lengkapnya.
Dan outputnya akan menjadi seperti berikut.
Langkah selanjutnya adalah kita akan membuat fungsi yang menampilkan widget foto profil si pengguna. Silakan buat fungsi dengan nama_buildWidgetPhotoProfiledan isi dengan kode berikut.
Dan ubah kode todo berikut.
Menjadi seperti berikut.
Outputnya akan menjadi seperti berikut.
Kemudian kita akan membuat satu widget yang akan menampilkan teks ‘Hello Ditta’. Untuk membuatnya, silakan ubah kode todo berikut.
Menjadi seperti berikut.
Kemudian, kita akan menampilkan teks username-nya si pengguna. Pada kali ini kita contohkan nama username-nya helloditta. Untuk membuatnya silakan kita ubah kode todo berikut.
Menjadi seperti berikut.
Selanjutnya kita akan menampilkan info jumlah post, followers, dan following si pengguna. Silakan buat fungsi dengan nama_buildWidgetInfoPostsFollowersFollowinglalu, isi dengan kode berikut.
Selanjutnya, kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan menampilkan list menu Feed, Explore, Notifications, Direct, IG TV, Stats, dan Settings. Untuk membuatnya kita perlu buat fungsi dengan nama_buildWidgetItemMenudan isikan dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Sampai langkah ini akan saya tunjukkan kode lengkapnya sekarang seperti berikut.
Dan berikut ialah outputnya.
Langkah berikutnya ialah kita akan menampilkan garis horizontal. Untuk membuatnya silakan ubah kode todo berikut.
Menjadi seperti berikut.
Dan widget terakhir yang akan kita buat didalamWidgetNavigationDraweradalah kita akan menampilkan menu Logout. Untuk membuatnya silakan ubah kode todo berikut.
Menjadi seperti berikut.
Sampai di sini akan saya tunjukkan kode lengkapnya agar teman-teman bisa mengikutinya dengan baik.
Sekarang kita akan menuju ke dalamWidgetContent. Silakan kita ubah kode didalam widget tersebut menjadi seperti berikut.
Sekarang kita akan menampilkan widget TextField pencarian. Cara membuatnya adalah kita buat satu fungsi dengan nama_buildWidgetTextFieldSearchdan isikan dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah selanjutnya adalah kita akan membuat fungsi widget yang akan menampilkan icon notification dibagian header. Untuk membuatnya silakan buat fungsi baru dengan nama_buildWidgetIconHeaderNotificationdan isi dengan kode berikut.
Lalu, ubah kode todo berikut.
Menjadi seperti berikut.
Langkah selanjutnya adalah kita akan menampilkan icon direct dibagian header-nya. Caranya adalah kita buat fungsi baru dengan nama_buildWidgetIconHeaderDirectdan isi dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Sekarang kita akan menampilkan satu button dengan teks Add Photo didalamnya. Untuk membuatnya silakan buat fungsi baru dengan nama_buildWidgetButtonAddPhotodan isi dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Selanjutnya adalah kita akan menampilkan teks Stories beserta dengan tombol watch all. Untuk membuatnya kita perlu membuat fungsi baru dengan nama_buildWidgetTitleStoriesdan isi dengan kode berikut.
Lalu kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan menampilkan list story pengguna lain secara horizontal. Untuk membuatnya kita perlu membuat class baru dengan namaWidgetStoriesdan isi dengan kode berikut.
Lalu, kita ubah kembali kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan membuat fungsi yang bernama_buildWidgetTitleFeedLatestPopulardimana, output dari fungsi tersebut adalah akan menampilkan teks Feed dan filter Latest dengan Popular.
Selanjutnya kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah terakhir adalah kita akan menampilkan list posts dari pengguna lain menggunakan StaggeredView. Untuk membuatnya silakan buat class baru dengan namaWidgetFeeddan isikan dengan kode berikut.
Jika ada error jangan lupa diimport class-nya ya. Berikut adalah kode yang perlu kita import.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Dan berikut adalah output terakhirnya.
Akhirnya selesai juga ya. Jadi, setelah saya lihat hasilnya ternyata Flutter Desktop mampu memberikan kesan pertama yang luar biasa bagi saya. Saya ingat benar bahwa dulu di Java Desktop untuk membuat tampilan seperti UI diatas benar-benar susah loh. Namun, sekarang semakin berkembangnya bahasa pemrograman dan framework maka, untuk membuat UI yang keren pun juga semakin lebih mudah. Untuk source code lengkapnya bisa kamu lihat di Github berikut ya.
Stories and insights from the developers in Nusanet
Product Engineer @nusawork.com| #AndroidDev | #FlutterDev |",https://miro.medium.com/v2/resize:fit:1200/1*9H2NjK6vPIovG19sx76Jow.png,"Flutter, Flutter Widget, UI Design, Flutter Ui, Flutter Desktop"
https://medium.com/@technicalvkv/copyright-free-images-%E0%A4%95%E0%A5%88%E0%A4%B8%E0%A5%87-%E0%A4%A1%E0%A4%BE%E0%A4%89%E0%A4%A8%E0%A4%B2%E0%A5%8B%E0%A4%A1-%E0%A4%95%E0%A4%B0%E0%A5%87-2020-4e2a050cb76d,Copyright Free Images कैसे डाउनलोड करे 2021,Kunvip,2021-04-07T08:51:32.280Z,"halo doston aapka bahut bahut swagat hai doston is post mein main aapko yah bataunga ki aap apne blog ya website ke liye copyright free images Kahan se download kar sakte hain aapko UN website on ke bare mein main is post mein main batayega doston post ko Pura padhen jisse ki aapko is baat ka naraj ho jaega ki aapko free copyright images Kahan se download karni hai to ja shuru karte Hain aaj ki post
doston is website se aap copyrights free images download kar sakte hain ya bahut hi acchi website hai yahan se aap high resolution wali stock images download kar sakte hain aapko jisse images ki jarurat hai aap is per search karke usi mein jisko download kar sakte hain copy acchi aur popular website hai
Is website mein roj lakho copyright free images upload kiye jaate Hain is website main aap images ke sath sath free copyright videos ko bhi download kar sakte hain is website main aap custom size choose karke bhi images aasani se download kar sakte hain
iske alava or dusri website ke bare me jane jinse aap free copy right image download kar payenge
top 10 websites for free copyright images
I am blogger my site ishttps://www.technicalvkv.com",https://miro.medium.com/v2/resize:fit:1200/1*wk2WIXdxiSYrVW1jvZkTLA.png,"Blogging, Technology, Blog, Hindi Blog, Images"
https://medium.com/@adzzie/laravel-8-membuat-restful-api-crud-sederhana-101c8c869d40,Laravel 8 : Membuat RESTful API CRUD Sederhana,Aji Gojali,2021-02-18T14:49:09.502Z,"Assalamu’alaikum Warahmatullahi Wabarakaatuh.
Kali ini kita akan membuat API menggunakan laravel dengan versi yang terbaru (pada saat tulisan ini dibuat), yaitu versi 8. yups ga usah kelamaan intro langsung aja kita kerjakan.
Sebelumnya kalian harus menginstalphp,Databasedancomposerlalu setting environment terlebih dahulu.Saya sendiri menggunakan php versi 7.3 dan menggunakan database mysql. Bagi yang belum menginstal composer kalian bisa liat dilink ini.
Membuat Project— Jika sudah menginstal yang di atas, kita buka command prompt atau cmd atau terminal atau sejenis teman-temannya. Lalu tuliskan script ini.
jika sudah sampai seperti gambar di bawah ini berarti penginstalan atau membuat project laravel sudah selesai.
lalu kita pindah kedirektoriapi_crudyang kita buat tadi. dan hasilnya seperti gambar di bawah ini.
Setting Database— Setelah itu kita setting database yang ada di file.env-nya. sesuaikan settingan database kalian. contohnya seperti di bawah ini.
Membuat Database— Sebelum mulai coding, mari kita buat databasenya terlebih dahulu. Kita balik lagi ke terminal atau cmd. jika kalian menggunakan mysql seperti yang saya guanakan kalian tinggal mengikuti pertintah berikut atau menyesuaikan dengan database kalian.
setelah masuk dalam client database-nya. kita buat databasenya.
Membuat Migrasi— Sekarang saatnya coding. pertama kita buat migration databasenya terlebih dahulu. saya di sini akan membuat table companies.
Setelah tergenerate filenya, kita ubah atau kita tambahkan field atau column pada methodup-nya sesuai kebutuhan, jika kalian lupa atau belum tau type apa yang akan dibuat pada fieldnya, kalian bisa liat dilink ini.
contohnya saya buat seperti ini.
Jika sudah selesai menambahkan fieldnya, mari kita eksekusi dengan perintah ini.
Jika kalian mendapatkanerrorketika migrasi database. Kalian bisa lihat yang ada dilink ini.
Kalau hasilnya success akan seperti ini.
Membuat Model— Sekarang kita buat modelnya. kalian ketikan seperti script dibawah ini di cli atau terminal kalian.
Setelah tergenerate modelnya kita tambahkan code seperti di bawah ini.
Membuat Controller— Untuk membuat controllernya kalian bisa ikuti sctrip di bawah ini.
Untuk penjelasannya di bawah ini.
Di dalam controller sudah terbuat method-method-nya sepertiindex,store,show,update,destroy. sekarang kita tinggal isi script sesuai kebutuhan kita.
Method index— Method ini digunakan untuk menampilkan data yang berbentuk list. contohnya seperti dibawah ini.
Method store— Method ini digunakan untuk menyimpan data. dan ini contoh codenya.
Method show— Method ini digunakan untuk menampilkan data dalam bentuk satu data. contoh codenya.
Method update— Method ini digunakan untuk mengupdate data. bisa kalian ikuti codenya di bawah ini.
Method destroy— Method ini digunakan untuk menghapus salah satu data. dan ini contoh codenya.
Setelah selesai membuat controller kita akan membuat routes.
Membuat Route— Route digunakan untuk mendaftarkan atau mendefinisikan suatu alamat yang biasa kita sebut URI (Uniform Resource Identifier) ke dalam file route. Di dalam direktori routes terdapat beberapa file route, salah satunya file api.php yang akan kita gunakan untuk mendefinisikan uri apinya. kalian tambahkan code berikut kedalam file api route tersebut.
Kalian bisa lihat list route yang sudah terdaftar di dalam aplikasi kita dengan menggunakan script dibawah ini pada cli atau terminal.
Gambar di atas adalah hasil dari route yang kita definisikan, karena kita mendefinisikan routeapiResource, maka route akan otomastis membuat route seperti pada gambar di atas. selain menggunakan routeapiResourcekalian juga bisa mendefinisikan route secara manual. contohnya seperti di bawah ini.
Code route tersebut akan menghasilkan route yang sama seperti pada gambar route list di atas.
Lihat hasil— Untuk melihat hasil yang kita buat, kita membutuhkan tools seperti postman, rested atau yang lainnya. di sini kita akan mencoba memakai postman. dan di bawah ini adalah hasilnya.
Terima kasih, salam Programmer.
Wasalamu’alaikum Warahmatullahi Wabarakaatuh.",https://miro.medium.com/v2/resize:fit:1200/1*5uBsg8qj0UkdojEOpOntNQ.png,"Restful, Postman, Laravel, API, Crud"
https://medium.com/@teddykoerniadi/basic-crud-api-on-laravel-8-3092b7a79c29,Basic CRUD API di Laravel 8,Teddy,2020-12-25T03:35:21.456Z,"Dalam dunia software development mungkin sudah tidak asing lagi di telinga kalian sebuah istilah yang satu ini yaitu API atau Application Programming Interface. Kali ini kita akan coba membuat API Create, Read, Update, dan Delete di salah satu framework php yaitu Laravel. Yang perlu kita siapkan adalah:
Oke kita langsung coding
Code di atas adalah contoh dasar routing api pada 1 modul, disini kita coba buat module product. ada 5 route:
disebelahnya di dalam kurung siku adalah controller dan function yang akan di jalankan ketika routenya terpanggil.
3. Buat modul (model, controller, migration), disini kita buat memakai perintah di terminal ya, coba jalankan di terminal dan pastikan posisinya di folder project kalian.
perintah di atas akan mengenerate model, factory, migration, seeder, dan controller. Karena diakhir perintahnya kita tambahkan -a yang artinya all jadi ter-generate-lah semuanya tadi dalam satu perintah. Ini akan membantu kita dalam naming modul nya karena seperti di dalam migrationnya sudah tergenerate nama table ‘products’, model sudah menggunakan table ‘products’, dan controller sudah menggunakan model ‘Product’. EMEIZING-kan!!!
4. Silahkan edit product migration sesuai dengan kebutuhan kita. karenakita disini coba-coba. Jadi kita akan tambahkan saja kolom name, price, dan stock. lalu jalankan di terminal “php artisan migrate”
5. Edit app/models/Product.php, tambahkan variable fillable yang diisi dengan semua nama kolom yang kita ingin bisa masukan datanya lewat aplikasi kita, contoh code lihat di bawah.
6. Edit app/Http/Controllers/ProductController.php. Ketika kita buka ProductController pasti sudah ada template function yang sudah dibuat. kita hanya akan edit function index, show, store, update, dan delete. Karena dasarnya function itu yang sering dipakai untuk api. Lihat code di bawah.
untuk function lainnya dibiarkan saja atau juga boleh di hapus karena kita tidak pakai.
Oke sekarang sudah selesai kita coba di postman hit ke 5 endpointnya. Jangan lupa jalankan dulu servernya ya, jalankan di terminal “php artisan serve”, jika server berjalan dengan lancar. READY FOR TESTING
Ketika semua test case telah berhasil, maka percobaan api kita berhasil.Sekia Basic CRUD API on Laravel 8, terima kasih semoga bermanfaat.
Full-Stack developer. Currently is javascript instructor.",https://miro.medium.com/v2/da:true/resize:fit:887/0*En1OqgjqVhLwOGTU,"Laravel, API, Laravel Framework, Crud, Eloquent"
https://medium.com/@azharogi/cara-handle-notifikasi-midtrans-pada-api-coreapi-ep3-50861ad4e7a6,Cara Handle Notifikasi Midtrans pada API —CoreAPI #ep3,Azhar Ogi,2020-12-10T13:27:48.855Z,"Hallo brosis, sesuai yang di janjikan nih kita lanjut ke episode 3 untuk Handle notification. Sebenarnya ini simple saja ya brosis.
Masih Bingung ? Perkenalan dulu, baca Disini
Jika brosis lebih tertarik dengan metode video, brosis bisa langsung akses video dibawah ini yah.
Btw karna disini kita masih menggunakan localhost, maka kita menggunakan simulasi menggunakan webhook.
Tahap 1 : Request Checkout
Untuk brosis yang belum paham Checkout menggunakan Core API Midtrans brosis bisa baca dulu diEp1&Ep2
Tahap 2 : Handle Notification
Kali ini kita menyiapkan table terlebih dahulu yah guys. Siapkan dulu saja schemanya sesuai dengan database yang kalian gunakan, disini kita menggunakan MySQL. Buat 1 migration pada lumen kalian.
Masukan syntax ini ke dalam file migration kalian pada function up()
Sehingga menghasilkan struktur table seperti dibawah ini
Jika sudah jangan lupa buat 1 class model untuk table orders ya
Kita buat 1 class controller baru dengan nama NotificationController dan buat 1 function di dalamnya yaitu post() , brosis bisa lihat syntaxnya dibawah ini :
Penting :Notifikasi yang dikirim oleh midtrans memiliki format JSON, sehingga brosis harus melakukan decode pada json yang diterima oleh API ini.
Jangan lupa untuk routingnya
Lanjut, brosis modifikasi class PaymentController agar menyimpan data checkoutnya ke dalam sebuah table orders
(tambah) function buyProduct
(modifikasi) function bankTransfer
(modifikasi) function chargeCreditCard
Jika sudah maka jangan lupa di serve dulu
Kita buka webhook :https://webhook.site/
Kita copy unique url
Oke sekarang kita buka akun midtrans > settings > configuration, dan paste unique url ke kolom Payment Notification URL.
Kita uji coba, lakukan checkout terlebih dahulu menggunakan bank transfer
Brosis bisa cek data pada table orders
Brosis bisa cek notification history / log ke :https://dashboard.sandbox.midtrans.com/settings/vtweb_configuration/history
Jika statusnya complete, kalian bisa cek di webhook kalian
Brosis bisa copy va_number pada json di column raw content
Untuk simulasi pembayarannya kalian bisa buka :https://simulator.sandbox.midtrans.com/bca/va/index
Jika berhasil maka brosis akan mendapatkan payment confirmation seperti dibawah ini.
Dan jangan lupa klik pay.
Brosis buka kembali webhook kalian dan wala… ada notifikasi success
Sekarang, brosis copy json pada raw content.
Kita sekarang test json request menggunakan Postman,
Jangan lupa send, jika status 200 OK maka kalian bisa cek data kalia pada table orders. Jika status berubah sukses maka kalian sudah berhasil sampai sini
Oke, sekian dulu dan semoga bermanfaat. Jika brosis merasa bingung atau mengalami error, bisa chat aja langsung ke socmed ya.
Jika kalian suka silahkan di clap ya :D
PaymentGateway — CoreAPI Series
Social Media & Channel
Youtube :CLASSIC IMPLEMENTS
facebook: zharlet30 ,linkedin:https://www.linkedin.com/in/azhar-ogi-2bb408112/,gitlab: @zharlet ,line: zharlet .
Engineer with 5+ years of experience in web development, backend, APIs, and cloud infrastructure. Tech enthusiast with a strong Soft Skills & Hard Skills",https://miro.medium.com/v2/resize:fit:1002/1*LVOK9XskY5tNW-dFhDr1cg.png,"Payment Gateway, Rest Api, Laravel, Lumen, Backend"
https://medium.com/@gusjul/react-native-firebase-authentication-b28aa301e3af,[React Native] Firebase Authentication,Gus Jul,2020-12-03T08:14:46.592Z,"Firebase merupakkan salah satu realtime database yang berbasis cloud yang dikembangkan oleh google, kali ini kita akan menghubungkan react native dengan firebase
Langsung kita mulai saja untuk mengauthentifikasi firebase dengan react native, kali in kita akan menggunakan package react-native firebase, yang merupakan package yang sudah dikostum dan disesuaikan untuk native app
Buat sebuah projek baru dengan perintah berikut :
Pergi ke lamanfirebasekemudian klik konsole yang berada di bagian pojok kanan atas kemudian akan tiba di halaman konsole firebase
Buat projek pada konsole firebase dengan klik ikon “add projek” sehingga nantinya akan diberikan form untuk melanjutkan projek, kemudian isi nama projek, setelah itu untuk google analytic bisa di turn off atau on, selanjutnya mengkonfigurasi google analytic (untuk yang turn on google analytic)
Tambahkan packcage agar lebih mudah menggunakan firebase untuk dokumentasi lengkap dapat dilihatdisini, atau dapat diinstal pada projek kita menggunakan perintah berikut :
Setelah projek selesai dibuat, tambahkan aplikasi ke firebase, dengan memilih antara aplikasi IOS, Android, ataupun Web. Khusus saat ini kita akan menambahkan aplikasi android.
Setelah itu isikan nama aplikasi anda, nama ini terdapat pada bundle projek yang sudah dibuat. Dapat dicari pada folder projek yang sudah dibuat/android/app/src/main/AndroidManifest.xml
kemudian untuk app nickname anda bisa membuatnya secara bebas, dan untuk SHA-1 bersifat optional jika punya dapat ditambahkan, namun jika tidak bisa dikosongkan kemudian klik “register app”
Download file yang diberikan kemudian pindahkan tersebut ke folder projek anda tepatnya pada /android/app/google-services.json.
Tambahkan Firebase SDK sesuai dengan petunjuk yang tersedia. SDK yang ditambahkan berada pada /android/build.gradle, kemudian untuk langkah dibawahnya tidak perlu dan lanjut ke langkah berikutnya
Seperti yang terdapat pada dokumentasi react-native-firebase tambahkan line berikut pada /android/app/build.gradle
Install auth pada react-native-firebase menggunakan perintah sebagai berikut :
Selamat anda sudah menghubungakan projek react-native dengan firebase
Pertama kita buat stack screen dimana akan menjadi navigasi antar halaman kita, selain itu penting diketahui bahwa kita harus membuat komponen seperti gambar projek diatas. Berikut merupakan file App.js yang kita jadikan sebagai stackscreen
Ikuti langkah-langkah dibawah ini
Setelah itu kita lanjutkan untuk develop signup.js dimana kita akan membuat tampilan sebagai berikut, dengan menginputkan program seperti dibawah ini
Untuk signup sendiri kita memerlukan fungsi untuk dapat terdaftar sebagai pengguna pada firebase dengan menambahkan function berikut :
Hal tersebut akan dapat membuat kita mendaftar dan terautentifikasi pada aplikasi yang dibuat
Setelah itu kita akan mendevelop login pada login.js, setelah membuat akun kita arahkan kembali ke login screen setelah itu user harus login menggunakan akun yang sudah didaftarkan tadi berikut ini perintah login.js
Selanjutnya kita harus menambahkan fungsi login agar dapat masuk pada aplikasi yang kita gunakan, dengan perintah berikut :
Dengan itu kita dapat login pada aplikasi kita menggunakan akun yang sudah dibuat tadi pada halaman signup
Dan yang terakhir adalah log out, logout disini adalah fungsi untuk user keluar dari sistem. fungsi logout lebih simpel daripada fungsi lainnya. Berikut merupakan perintah pada homescreen.js
Tambahkan function untuk logout, agar user dapat keluar dengan perintah berikut :
Horee, sekarang anda sudah membuat semua komponen dan dapat dites untuk firebase.
Sekarang saatnya kita melakukan testing apakah sudah semua fungsi berjalan baik atau tidak. Silahkan buka firebase konsole pada projek yang telah dibuat tadi
Yey kita sudah berhasil menghubungkan firebase dengan react-native dan juga membuat firebase authentication menggunakan email pada halaman signup, login dan juga homescreen untuk logoutnya.
Untuk keseluruhan code dapat dilihat seperti di bawah ini :
tapi jangan di copy paste aja ya, pastikan ikuti langkah dari awal, karena kalau eror lama untuk deteksinya :D
Untuk yang gemar UX, ada sedikit hal yang dapat ditambahkan yaitu pada Textinput baik pada saat memasukkan email di login dan juga signup. Dimana keyboard akan berisi tanda “@” dan juga tidak otomatis huruf besar pada awal pengetikkan. dengan menambahkan code berikut
Mungkin sekian untuk toturial menghubungkan react native dengan firebase menggunakan package react-native-firebase. sampai jumpa di toturial selanjutnya",https://miro.medium.com/v2/resize:fit:1200/1*LPJL9qcf4SVNFbZg1vi2WQ.png,"React Native Firebase, Mobile App Development, Firebase, React, React Native"
https://medium.com/@utpalblog/youtube-se-paise-kaise-kamaye-uski-puri-jankari-97e6b2198d40,Youtube Se Paise Kaise Kamaye Uski Puri Jankari,Utpal konwar,2020-12-26T09:31:16.025Z,"Hi friends aaj me aapko batauga kiyoutube se paise kaise kamaye. Dekha jaye toh india me bahut sare log hai jo sirf youtube ki kamai par apne ghar chala raha hai. Youtube ek aisa platform hai jaha par hum apne video dalke apne ghar me hi paise kama sakte hai, jab se jio network aaya hai tab se youtube me itne sare youtuber aagaye hai jo jyadatar tech video hi apne YouTube channel me upload karte hai.
Agar aap bhi youtube se paise kamana chahte hai to aap is post ko jarur padhe, kiyoki is post me aapko youtube ke bare me puri jankari denge. Bahut sare log hai jo youtube ko full time career ki hisab se le rahe hai. Agar aap apne mehnat yotube me kuch mahine lagate hai toh aap bhi ek successful youtuber bon sakte hai. Me bhi youtube me video dalta hu lekin me apne blog ke upar jyada focus karta hu.
Youtbe se paise kamane keliye aapke channel par 1000 subscriber aur 4000 watch time hone chahiye. Jiske channel par 1000 subscriber aur 4000 watch time ho jate hai, unke channel ko monetize karke paise kama sakte hai. Aapke video ko monetize karne keliye aapke paas ek google adsense account hona chahiye. Agar aapke pass google adsense account nahi hai toh aap bahut he easy apply kar sakte hai.
Aapke videos ko monetize karne ke baad aapke videos par ads show hoga. uske baad aapke videos par jitney views milenge aapko utnahi paise income kar sakenge. Aur aapke adsense account mei 100$ hone ke baad woh paise aapke bank account me withdraw kar sakte hai.
Agr app bhi youtube se paise kamana chahte hai to pahle aapko youtube par ek channel create karna hoga. uske baad apne channel keliye video shoot karke publish kare. Apne youtube channel keliye aap koi bhi categories ke video bana sakte hai example- funy, tech, sport, blogging, internet etc.. matlab aapko jis categories ke knowledge hai aap usi categories ke video banaye. Pahle aapke channel ke videos par views aane par kuch time log sakte hai kiyoki aapke youtube channel par subscriber nahi honge. Lekin jab aapke channel par subscriber badhte jayenge aapke views bhi jyada honge.
Ek baat aapko mei clear kar dena chahta hu ki aap jo bhi content aapke channel par publish karnege woh video unique hona chahiye. Bahut sare log hai jo youtube se video download karke usko thora sa edit karke firse apne channel par publish karte hai. Aisa videos youtube policy ke bahor hai aise karne par aapke channel aaj nahi to kaal band ho sakta hai. Isiliye aap sirf aapne original content he publish kare.
1.Computer Knowladge :Agr aapke pass computer hai to aapke videos ko editing karne me easy hoga. waise dekha jaye toh india me hajaro log apne mobile par hi apne videos ko edit karte hai.
2.Basic internet knowladge :Dekha jaye to bahut kom logo ke pass internet ka knowledge hote hai. maine bahut sare logo ko dekha hai ki woh sirf facebook, whatsapp, youtube he dekhte hai. Usse jyada unlogo ke pass internet ka jyada knowledge nahi hota. Agar aap youtube par kaam start karna chahte hai to aapko internet ka knowledge hona jaruri hai.
3.Video Editing :Ek successful youtuber bane keliye aapko thora kuch video editing ka knowledge hona chahiye. Taki aap jo video apne youtube channel keliye bana rahe hai woh video dekhne me moja aaye. Internet par aapko bahut sare free aur paid video editing software milega aap free software ko thora din keliye use kar sakte hai.
4.Quality Video :Aap apne youtube channel keliye jo bhi video banante hai us video ka quality achhi honi chahiye tabhi aapke viewer aapke videos ko dekhenge. Agr aapke video quality acchi nahi hai to aapke viewers aapke videos ko ignore kar denge. Aur duwara aapke channel par visit nahi karenge.
5.Your Voice :Agar aap apne video me aapke voice use karte hai to aapke voice clear hona chahiye, taki logoko aap kiya bol rahe hai woh easy samajah me aaye. Apne youtube me dekha hoga ki bahut sare youtuber ke voice clear nahi hota aise video me views bhi jyada nahi aate.
To dosto youtube se paise kaise kamaye aapko puri jankari maine diya hai. I hope ki mera ye post aapko achha lga aur bhi jyada janakri keliye aap hamare comment box par comment kijiye.
My name is Utpal and I am a blogger. I love writing articles in this media platform, if you liked my article then follow me. Thank you",,Make Money Online
https://medium.com/@ryansombreros/contoh-kalimat-menulis-e-mail-dengan-bahasa-inggris-dalam-berbagai-situasi-4d78b6eb51b1,Contoh kalimat menulis E-Mail dengan bahasa Inggris dalam berbagai situasi,Rian,2020-12-17T03:36:46.222Z,"Buat kalian yang bekerja memakai Email dengan menggunakan bahasa Inggris namun masih merasa bahasa Inggrisnya belum bagus atau belum PD. Berikut berbagai contoh kalimat dalam bahasa Inggris dengan berbagai situasi:
Dalam situasi Mengekspresikan terima kasih:
Dalam situasi menunggu tanggapan:
Dalam situasi membutuhkan bantuan:
Saran waktu dan tempat:
1.Could we meet on Monday in the afternoon at 13:00 pm?
Balasan tidak bisa ikut:
Balasan mengubah jadwal meeting/rapat:
Balasan bisa ikut meeting/rapat:
Balasan menolak meeting/rapat:
Memberitahu darimana anda mendapat kontak:
We met last Thursday on your stand at the Jakarta Trade Fair. I am emailing you off your website, which I found through Goggle.
Alasan anda mengirim email:
We are a manufacturer/supplier/provider of…… We are interested in… We are a Japanese Company exporting to the EU and we need…
Permintaan yang akan diminta:
We would be grateful for some information about… Please send us information about your product range and prices.
Permintaan yang lebih spesifik:
In particular, please send full details of your prices, discounts, terms of payment, and delivery times. Could you also say whether there is any minimum order?
I look forward to an early reply and I’m sure that there is a market for your products here in Jakarta.
Ucapan terima kasih terlebih dahulu
Thank you for your email of 4 December inquiring about…..
Memberikan informasi yang dibutuhkan
We can quote a price of… CIF/FOB Instanbul. We can deliver by…(date)/within…. (period of time). The goods will be shipped 3 days from receipt of a firm order. We can offer a discount of… om orders over…
We require payment by bank transfer/letter of credit. Our normal procedure is to… Our normal terms for first-time customers are… We can supply the items you require directly from stock.
May I suggest that I call you at your convenience to discuss the matter further? If you need any further information, please do not hesitate to contact me. My direct line is….
I am writing to complain about the quality of a product I bought from your website/company.
Our order date 16 December clearly stated that we wanted 1,000 items, however, the goods were faulty/damaged/in poor condition.
Permintaan tanggung jawab:
Please replace the faulty goods as soon as possible. We must insist on an immediate replacement/full refund. Unless I receive the goods by the end of this week, I will have no choice but to cancel my order.
I hope that you will deal with this matter promptly as it is causing me considerable incovenience.
Sebenarnya masih banyak lagi situasi urusan kerjaan menggunakan email dalam bahasa Inggris. Saya akan tambahkan apabila ada waktu luang kembali. Terima kasih
Just Do It - Impossible Is Nothing",,"Email, Kalimat, Bahasa Inggris, Contoh, Pekerjaan"
https://medium.com/@bobbyzarkasih/understanding-universe-in-your-free-time-6c82bdebdb48,Understanding Universe in your free time,Bobby Zarkasih,2020-12-27T11:29:29.000Z,"My thoughts on “Astrophysics for People In a Hurry” by Neil Degrasse Tyson
Okay, where to start?
Gw akan mulai dengan bilang, waktu kecil cita-cita gw adalah menjadi seorang Astronom. Ya sedari kecil gw udah seneng banget baca buku-buku tentang antariksa, gw punya puluhan koleksi buku dan ensiklopedia yang membahas mulai dari bumi, bulan, bintang, dan tata surya. Alesannya saat itu simple,it’s cool
Salah satu konsep favorit gw adalah soal betapa jauhnya bintang-bintang yg kita lihat di langit. saking jauhnya sampai cahaya dari mereka butuh waktu jutaan tahun untuk sampai ke bumi(dalam 1 detik, cahaya bisa bergerak sejauh 300.000 km. itung dah tuh kalo jutaan tahun). yang berarti saat kita melihat bintang di langit, kita sedang melihat masa lalu, kita sedang melihat sisa-sisa cahaya yang dikirimkan oleh bintang itu jutaan tahun yang lalu, dan bintang yang kita sedang kita lihat mungkin saat ini sudah tidak ada lagi. isn’t that cool?
Hal-hal tentang betapa kerennya alam semesta ini yang bikin gw jatuh cinta dengan astronomi. But in highschool i give up on my dream to be an astronomer, because in high school i hate physics so much haha. Setelah itu gw memutuskan untuk kuliah DKV dan ujung-ujungnya sekarang meniti karir di Film. agak random emang wkwk.
Bertahun-tahun tidak tersentuh dengan dunia astronomi lagi, di tahun 2019 gw lihat ada buku “Astrofisika untuk Orang Sibuk” di gramedia. Dari judulnya yang terdengar norak (karena ini terjemahan) gw kira ini buku abal-abal tulisan penulis yang tidak jelas asal-usul dan kredibilitasnya, sejenis buku-buku cara cepat menjadi kaya yang banyak banget di obral di gramedia. Tapi gw lihat penulisnya Neil deGrasse Tyson,well, i’ve heard this guy, this guy is cool.Setelah gw googling ternyata buku ini adalah terjemahan dari buku “Astrophysics for People in a Hurry” yang kalau kata review membahas alam semesta secara singkat dan mudah dipahami. Dan akhirnya gw pun beli bukunya… di 2020, karena waktu itu ga bawa uang dan besoknya lupa.
Impresi pertama saat membaca buku ini, gw dibawa kembali ke masa kecil gw. Persis kaya Anton Ego di film Ratatouille waktu makan… well.. ratatouille. gw dibawa kembali mengingat kenapa gw dulu bisa suka banget tentang alam semesta. Dan gw langsung men-chargekembalicuriositygw tentang dunia astronomi.
Oke ini sangat amat personal, tapi secara objektif gw akan bilang
Buku ini terdiri dari 12 bab, yang ngebahas mulai dari Big Bang,konsep-konsep rumit kayakDark Matter &Dark Energy,planet dan galaksi kita, bahkan kita diajakrevisittabel periodik, untuk kenal unsur-unsur yang ada di alam semesta, dan ditutup dengan perenungan akancosmic perspective
Di halaman pertama buku jelas tertulis
For all those who are too busy to read fat books — Yet nonetheless seek a conduit to the cosmos.
ya buku ini sangat tipis, versi bahasa indonesianya cuman 146 halaman. tapi apakah buku ini cukup untuk memahami semua tentang alam semesta kita?
Jawabannya, gak juga.
Gini, buku ini bukan buku “Cara cepat memahami Astrofisika, 2 minggu dijamin langsung paham”.Astrofisika adalah bidang yang sangat rumit, butuh puluhan tahun mungkin untuk bisa memahami bagaimana alam semesta bekerja. Buku ini gak akan menjelaskan itu semua, dia gak akan menjelaskan secara detail rumus-rumus, argumen-argumen yang digunakan oleh para astrofisikawan. Tapi buku ini akan membantu me-simplifiedkonsep-konsep tersebut agar lebih mudah kita cerna. Buat gw pribadi, buku ini setidaknya akan membuat kita paham, apasih yang bikin para astrofisikawan ini tertarik dengan alam semesta. Kita dibawa masuk ke bagian kulit dari ilmu Astrofisika, pertanyaannya kemudian apakah cukup? kalau kamu ingin jadi seorang astrofisikawan, jelas enggak. tapi kalau kamu mau setidaknya memahami bagaimana alam semesta bekerja dengan cara yang mudah dicerna, maka buku ini cukup.
Gw tetep merekomendasikan kalian untuk baca buku “Origins” yang ditulis juga sama NDT sebagai lanjutannya. Disana bakal ada penjelasan yang lebih panjang soal alam semesta, tapi tetep cukup mudah untuk dipahami.
Menurut gw Neil DeGrasse Tyson mungkin bukanAstrophysicistpaling jenius di dunia saat ini, yang mungkin akan menemukan teori-teori baru soal alam semesta. Tapi dia adalahAstrophysicistyang cukup jenius untuk bisa menjelaskan konsep yang sangat rumit, menjadi sesuatu yang bisa dicerna oleh orang awam seperti kita, yang mungkin ga lebih cerdas dari anak SD.Dan ini yang sangat dibutuhkan oleh ilmuwan di masa sekarang,sciencebukan lah benda yang eksklusif dan hanya akan dibicarakan oleh orang-orang cerdas.Scienceadalah cara bagaimana kita memahami apa yang terjadi di sekitar kita, dan ini harus juga bisa diakses oleh orang-orang yang tidak cukup cerdas.
Sekarang gini, kenapa orang-orang masih banyak yang ga patuh dengan protokol COVID-19, masih banyak yang gamau di vaksin, dll sebagainya. Mungkin sebagian ada yg memang egois atau tidak ada pilihan lain. Tapi sebagian lainnya mungkin bahkan gak paham apa yang para ilmuwan dan pemerintah udah jelasin. Ilmuwan kita yang pinter banyak, banyak banget, tapi ilmuwan yang bisa menjelaskan itu ke orang awam? gak banyak.
Prof. Herawati Sudoyo pun mengamini hal ini dalam interviewnya di Endgame
Sekali lagi, menurut gwScientistga bisa jadi kayak broker yang seneng banget pake bahasa dan istilah ribet buat jelasin gimana main saham cuman supaya mereka kelihatan eksklusif dan mereka kelihatan paling paham soal saham. Karena gak ada ruginya sama sekali jikasciencebisa diakses oleh semua orang.
Oke, gw udah mulai ngalor ngidul nih. balik ke bukunya Neil DeGrasse Tyson
Buku ini ditutup dengan bab yg membahas soalcosmic perspective, sebagai manusia kita punya ego yang sangat besar, kita selalu merasa kita adalah spesies paling spesial di alam semesta.cosmic perspective will lowering your ego by explaining how insignificant we are in this vast universe. Gimana caranya? gw ga akan masukin semua yang ada di buku kesini, tapi ini yang paling menarik buat gw.
Kita punya hidup yang sangat amat penting untuk kita di bumi,
Bumi yang kita tinggali ini sangat amat luas,
Planet kita tercinta ini mengitari sebuah bintang yg kita sebut matahari, yang merupakan bintang yang biasa-biasa aja,
Matahari kita ada di sebuah galaksi bernama bimasakti yang didalamnya ada ratusan milyar bintang,
Dan galaksi kita ini adalah bagian dari 50 sampai 100 milyar galaksi lain di alam semesta,
Alam semesta yang sangat amat luas ini sudah ada sejak milyaran tahun lalu dan diprediksi akan tetap ada milyaran tahun kedepan,
Umat manusia hidup hanya sekitar 200ribu tahun yang lalu
We are really really really tiny and insignificant, the universe doesn’t give a damn about us.
Gw mau menutup tulisan gw dengan salah satu “puisi” favorit gw, judulnya “Pale Blue Dot”. Tahun 1990, wahana Voyager 1 akan meninggalkan tata surya. NASA memerintahkan untuk memutar kamera Voyager 1 dan mengambil gambar bumi ditengah luasnya alam semesta.
Look again at that dot. That’s here. That’s home. That’s us. On it, everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every “superstar,” every “supreme leader,” every saint and sinner in the history of our species lived there — on a mote of dust suspended in a sunbeam.
The Earth is a very small stage in a vast cosmic arena. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds.
Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.
The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.
It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we’ve ever known.",https://miro.medium.com/v2/resize:fit:1200/1*4tdN7kdiZWGAqWjyWM5Glw.jpeg,"Book Review, Neil DeGrasse Tyson, Indonesia, Astrophysics"
https://medium.com/@nasriadzlani/rest-api-dengan-fastapi-dan-elasticsearch-93f035b5f05,Rest API dengan FastAPI dan Elasticsearch,Nasri Adzlani,2020-12-21T07:19:27.714Z,"FastAPI adalah sebuah framework python yang digunakan untuk membangun API(). FastAPI merupakan salah satu framework python yang memiliki performa bagus dalam membangun API. Bahkan lebih dari itu, dalam documentasinya yang bisa kita lihat disini, FastAPI memiliki banyak kelebihan lain.
Elasticsearch adalah salah satudatabaseyang masuk ke dunia NoSQL dengan fokus disearch engine database. Elasticsearch ditenagai oleh Apache Lucene yang juga merupakansearch engine databaseyang memilikiquery low level. Elasticsearch memilikiqueryyang lebih mudah untuk digunakan karena berbasis RESTful.
bahan yang dibutuhkan antara lain
🔹 pip install fastapi uvicorn
🔹 pip install fastapi
🔹 pip install elasticsearch
Jika kedua library di atas sudah terinstall dengan benar, saatnya kita mencoba membuat API. Kita buat sebuah file dengan nama main.py,
import yang dibutuhkan
Deklarasi yang dilakukan
Macam fungsi di FastAPI
tukang ketik kode. //https://nasriadz.tech/",https://miro.medium.com/v2/resize:fit:1023/1*du7p50wS_fIsaC_lR18qsg.png,"Elasticsearch, Fastapi, Rest Api, Rest, Python"
https://medium.com/@mnaufala/menggabungkan-beberapa-database-dengan-menggunakan-psql-logical-replication-b99db0ee3d3,Menggabungkan Beberapa Database dengan Menggunakan PSQL Logical Replication,Muhammad Naufal Abiyyu,2021-04-26T16:10:55.447Z,"Saat kita migrasi dari arsitekturmonolithkemicro services, kita akan mengubah cara kita menyimpan data. Saatmonolithkita hanya memiliki satudatabase, maka dimicro serviceskita akan punya banyakdatabasekarena setiapserviceakan memilikidatabasesendiri-sendiri. Dari sini muncul masalah baru yaitu saat kita ingin analisis data dan memerlukan operasijoinketablelain, namuntabletersebut berada didatabaselain.
Dari permasalahan diatas, berbagai solusi muncul untuk mengatasi permasalahan tersebut. Salah satu solusinya dengan cara memanfaatkan fiturlogical replicationdari postgresql untuk menggabukandatabase. Fitur tersebut memungkinkan kita untuk mereplika data pada sebuahdatabasesatu kedatabaselain.
Ok, langsung saja kita simulasikan fiturlogical replicationuntuk menggabungkandatabase. Untuk mempermudah simulasi kita akan menggunakan 3postgres(2db_masterdan1db_replicasebagai aggregator) yang kitainstalldidalamdocker container.
2. Langkah selanjutnya yaitu jalankanfiledocker-compose berikut:
3. Setelah db berhasil hidup, buattabledidb_memberdandb_merchantlalu masukkan beberapa datasample, misalkan kita buattable users(db_member) dantable merchant(db_merchant) denganschemaseperti berikut:
4. Langkah berikutnya yaitu membuat publication didb_memberdandb_merchant.Publicationadalah mekanismepostgresagar dapat mereplikasitable. Untuk membuatpublication, jalankanquerydibawah ini:
5. Terakhir yaitu kita buattable usersdanmerchantsdidb_replica.Setelahtableberhasil dibuat lalu buatsubscriptionsuntuk menghubungkanpublicationyang telah kita buat ke db_replica. Berikutqueryyang dijalankan:
Jika berhasil maka akan mendapatkanoutputsebagai berikut:
6. Lakukan testing dengan memasukkan data daridb_memberdandb_merchant. Nantinya secara otomatis data yang kita masukkan akan muncul juga didb_replica.",https://miro.medium.com/v2/resize:fit:451/1*Y5m694V1i1omr1l_B4RSWA.jpeg,"Database, Data Engineering, Microservices, Data Warehouse, Postgres"
https://medium.com/@sijjin/fix-wifi-ubuntu-20-04-rtl8188eu-4058ee7fb096,Fix wifi Ubuntu 20.04 ‘rtl8188eu’,ジブリル,2021-08-16T10:15:25.399Z,"Assalamualaikum warahmatullahi wabarakatuSalam sejahtera bagi kita semuaShamlomOm SwastyastuNamo BuddhayaSalam Kebajikan
Hello, lama kali ngga buat story di medium. Jadi intinya wifi pada ubuntu 20.04 yang awalnya normal, namun seteleah didist-upgradejadi tidak bisa konek dan selaludisconnect. Sebelum di upgrade lancar-lancar saja, saya akan membagikan “cara saya” mengatasi hal tsb.
Output file, tidak ada error:
Output file, tidak ada error:
Dan hasilnya seperti gambar dibawah
Sekian dari saya, semoga bermanfaat. Salam rahayu.
Eat - Sleep - Profit - Repeat",https://miro.medium.com/v2/resize:fit:1200/1*3P2tGLvCZwNlzuMHVsriuA.png,"Wifi, Linux, Ubuntu 20 04, Linux Tutorial, Ubuntu"
https://medium.com/@imnitishverma/bitcoin-mining-guide-bitcoin-mining-kya-hai-6a9006b6d429,Bitcoin Mining Guide : Bitcoin Mining Kya Hai,Nitish Verma,2020-04-22T11:20:05.022Z,"Aaj Bitcoin ka chalan bahut hi teji se world me hi nai india me bhi popular ho raha hai. Bitcoin ke bare me aaj bhi logon ke paas utni jaankari nai hai India me. Lekin ye ek Bahut popular Virtulal Currency hai. Main ish Post me Bitcoin Mining Guide dene jaa raha hun. Ki Bitcoin Mining Kya Hai
Bitcoin Minning ko janne se pehle aapko ye janna jaruri hai aakhirBitcoin Kya Hai?Aap Ynaha Click karke padh sakte hain. To aaiye Jante hain Bitcoin Mining ke bare me.
Bitcoin Mining Ko Profitable dhang se karna Thoda kathin hai. Lekin ish post ko padh kar aap ek acche bitcoin Minner ban sakte hain. Sabse Pehle ki aap Bitcoin Mining Suru karein samjh lena jaruri hai ki bitcoin Mining ka matlab kya hai. Bitcoin Mining legal hai aur SHA256 double round hash verification process ke sampann ki jati hai. Taaki Bitcoin Transactions ko proof kiya jaa sake aur bitcoin network ke Sarvjanik ledger ko aavshayak security pradaan ki jaa sake. Jis Raftaar se aap Bitcoin Mine karte hain usey Hash/PerSecond (Hash Per Second ) me mapa jaata hai.
Bitcoin Network Bitcoin Miners dwara aavshyak Computational Power ka yogdaan karne ke Pryaas ke badle Bitcoins ko release karta hai. Yeh Naye Jaari Kiye gaye bitcoins aur bitcoins ki mining karte samay satyapit kiye gaye transactions me samil transaction fee dono ke rup me aata hai. Aap Jitni adhik Computational Power ka Yogdaan karte hain aapko utna hi adhik Profit hota hai.
[caption id=”attachment_940"" align=”aligncenter” width=”300""]
Bitcoin Mining Guide : Bitcoin Mining Kya Hai[/caption]
Kuch Mamlon me aapko Bitcoins Mining wala Hardware Kharidne Ki jarurat pad sakti hai. Aaj Ke Samay me aap adhiktar HardwareAmazonpar Kharid Sakte hain. Ya aap Chahein toBitcoin Chartsme bhi iski talaash kar sakte hain.
Bitcoin Mining Start Karne Ke liye, Aapko Bitcoin Mining Harware lena hoga. Starting ke dino me aapke Computer ke CPU ya High Speed wale video Processor Card Se Mining Karna Possible tha. Aaj aisa karna sambhav nahi hai. Custom Bitcoin ASIC CHIP purane Systems ke Power se 100 guna adhik tak aur teji ke saath kaam kar sakte hain.
Issey low Speed ki kisi bhi any Chij se Bitcoin ki mining karne me ussey adhik Electricity Consume hoti hai. To Electricity Bill ki wajah se aapki profit bahut hi kam reh jayegi. Yeh Jaruri hai ki Bitcoin Mining ke liye khaastaur par banaye gaye Best Bitcoin Mining Hardware se hi Mine Kiya jaye. Avalon Jaisi Kai Companies Bitcoin Mining ke liye khas taur par bane Best systems ko provide karwati hain.
Aap Ke liye Ek aur Option hai Bitcoin Mining ke liye Cloud Contracts Ko Purchase Karna. Ye Process to bahut easy hai lekin isme Jokhim badh jaata hai, Kyunki aap Real way me Physical Hardware ko control nai kar rahe hote hain.
Yanha Hum Bitcoin Mining ke Best Cloud Mining Services ki list bata rahe hain. Lekin Hum inke Services lene ke liye aapko aagrah nai karte hain. Kyunki Bitcoin ke cloud mining me pehle bhi kai Ghotale ho chuke hain.
Genesis Mining: Genesis Mining Sabse Badi aur Bitcoin aur Scrypt Cloud mining Provider hai. Genesis Mining 3 Cloud Mining Plans offers karta hai jiske price bahut hi reasonable hain. Saath hi ZCash Mining Contracts bhi available hain.
Hashing 24 Mining: Hasing 24 varsh 2012 se Bitcoin Mining kar rahi hai. Inka Center Georgia aur Iceland me hai. Ye BitFury dwara Manufacture Modern ASIC Chips ka use karte hain Jinki wajah se maximum Performance aur efficiency milti hai.
Hashflare Mining: Hashflare SHA-256 Mining Contracts ko offer karta hai, Aur BTC me automatic Payouts hote rehne ke dauran hi adhik Profitable SHA-256 Coins ko mine kia jaa sakta hai. Coustomers ko kam se kam 10 GH/S purchase karna Chahiye.
Minex Mining: Minex Blockchain Project ka ek naya rup hai. Jisey ek Kifayti Simulation Game Version me present kiya gaya hai. Users Cloudpacks purchase kar sakte hain Jinhe fir Cloud mining farms, lotteries, casinos, real-world markets aadi ke purv Chayanit sets se ek index banane me use kiya jaata hai.
Hahnet Mining: Hashnet ka sanchalan Bitmain Dwara liya jaata hai, jo bitcoins minors ki Antmier Chain ka nirmata hai. Hashnet Me Filhal 600 se adhik Antminer S7 kiraye par uplabdh hain. Aap Hashnet Ki website par sabse latest price aur availability dekh sakte hain. Ish post ke likhe jaane ke samay tak Antiminer S7 ki hashrate ko 1200 Dollar me rent par liya jaa sakta hai.
Minergate Mining: Minergate Bitcoin ke liye Pool, Merged Mining aur Cloud Mining Offer karta hai.
Bitcoin Cloud Mining: Currently Bitcoin Cloud Mining ke sabhi Contract Sell Ho chuke hain.
MineOnCloud: Present time me MineOnCloud ne cloud me rent par dene ke liye Mining Equipment ke lagbhag 35 TH/S ublabdh karaye hain. Rent par uplabdh kuch miners me AntiMiner S4 aur S5 availble hain.
NiceHash: NiceHash ki viseshta yeh hai ki ye Mining Contracts Buyers aur Sellers ka milan karne ke liye ek orderbook ka upyog karta hai. Iske Latest Price ke liye site par visit karein.
Eobot: Yanha aap 10 Dollar jitne low amount se bhi cloud mining suru kar sakte hain. Eobot ka daawa hai ki grahak 14 Mahino me laabh kamana suru kar sakte hain.
Aapko Apna Bitcoin Mining Software Mil jaane ke baad, Aapko Bitcoin mining ke liye ek visesh program Download karna hoga. Bitcoin Mining ke liye prayog kiye jane ke liye kai programs available hain, Lekin Do Sabse Lokpriye Programs hain CGminer aur BFGminer, jo comand Line Program hain.
Yadi aap GUI dwara Pradaan ki jane wali upyog ki saralta pana chahte hain, to aap Easyminer ko aajma sakte hain jo ek click se suru hone wala Window/Linux/Android Program hai.
fully decentralized pool ke liye , hum highly recommend karte hainp2pool. ko.
Yanha jo pool diye gaye hain un pools me aajkal Bitcoin core aur uske baad ke versions me ye currently fully validating blocks hain.(0.10.2 or later recommended due to DoS vulnerabilities):
Aaj Ke Post Ko hum yanhi End karte hain. Next Post me Bitcoin Wallet setup karne ke baare me batayenge. Kynki Bitcoin Wallet me hi aapke coins save hotey hain. To Thoda wait kijiye next post ke liye. Waise aage abhi bahut kuch batana hai aapko Bitcoin aur Bitcoins Mining ke baare me.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",https://miro.medium.com/v2/resize:fit:300/0*pIiNtF62Gu37zOC2.png,"Bitcoincash, Bitcoin Wallet, Hindi, Bitcoin, Bitcoin Mining"
https://medium.com/@imnitishverma/koinex-indian-cryptocurrency-exchange-3dd17411c81f,Koinex Indian Cryptocurrency Exchange,Nitish Verma,2020-04-22T11:28:38.109Z,"India me cryptocurrency ka chalan bahut teji ke saath badh raha hai. Isi kram me Koinex Indian Cryptocurrency Exchange aa gai hai. Kisi Bhi Cryptocurrency ki selling ko takkar uske nikat ke competitors dete hain. Haal me bahut se exchanges ne pop up kiya hai aur bhi sayad cryptocurrency exchanges indian market me aane ko taiyaar hongi. Koinex India me New Platforms me ek hai.
India ke logon ke Koinex ek naya uphaar hai Jo crypto currency ka use karta hai. Kaafi lambe samay aur kadi mehnat ke baad safaltapurvak “Koinex Multi currency Exchange” Banaya gaya jo 25 August 2017 ko bharat me launch kiya gaya tha.
Aaiye Milte hain Koinex Exchange ke Founders se.
Koinex exchange ki sthapna Rahul Raj (IIT Khargpur), Rakesh Yadav (IIT Delhi) aur Aaditya Nayak (BITS Pilani) ne ki hai. Sabse Pehle 2016 me inhone Blockchain Technology ki suruaat ki, uske baad sansthpakon ne 2017 me Cryptocurrency me trading suru ki. Jaisa ki aap sabhi jante hain desh me abhi bhi in chijo ke naa ke brabar vikalp hain, we sbhi Global Exchange (Coinbase, Poloniex, Liqui etc) par business business karne ke liye majbur they. Global Exchange aur Present Indian Exchange (Zebpay, Unocoin, EthexIndia etc) dwara diye jane wale quality ke mamle aur kafi research ke baad, Unhone Indian Currency aur kai cryptocurrency ko support karne waali ek indian Exchange banane ka Nirnay liya. Jo Koinex ke Rup me aapke saamne hai.— Koinex Indian Cryptocurency Exchange
Sabhi teeno sansthapko ne Technology, Product aur business se pahuchne waale kaushal ka shandaar aur asadhaarn mixture pesh kiya hai. Koinex Team ke saath we daawa karte hain ki Koinex ek centralized, Live Open Order Book,Pear to pear exchange hai, Jo Maujuda khiladiyon ke viprit nahi hai.
Koinex ko ek highly secure, scalable, trading friendly aur bahut adhik digital assets exchange pradaan karne ke liye banaya gaya hai. Grade A Security architecture ke saath milakar New aur latest technology, Koinex ke uddesya India ko International Manko ko par laaya jaana hai. Yeh Super Fast, Super Safe, Super Interactive hai, aur crypto traders ke liye visesh rup se taiyaar kia gaya hai. Koinex High frequency ke liye atyadhik competitor traded Business Fee anusuchi pradaan karta hai. Crypto Financial Instruments aur anya cryptocurrency products ko pahle se avdharna aur 1 year ke roadmap me shaamil kiya gaya hai.
CoinFunda ko diye interview me batate huye Company ke Co Founder Rahul Raj ne kaha ki Koinex suruaat me 4 Coins ka samarthan karega: Bitcoin, Ethereum, Litecoin aur Ripple (other altcoin ko baad me joda jayega). Present me, Koinex fiat-to-crypto ka samarthan karta hai, INR/BTC aur INR/ETH etc. (Crypto-to-crypto Koinex launching ke baad 2–4 month ke bhitar live kiya jayega). Halanki wey ish process ka experience karne ki planning kar rahe hain jissey wo users ko striking launch offer de sakein.
Koinex me bhugtaan aap india me Jo banking system hai uske hisaab se kar sakte hain Jaise NEFT, RTGS, IMPS Debit card aadi. Waise khabar ye bhi hai Koinex jaldi hi Bitcoin Cash me kaam shuru karne waala hai. Halanki india me abhi bitcoin exchange ke niyam effectively laagu nahi hain.
Koinex attacks, chori aur privacy leak se users ko safety provide karega. Yeh ek quickly accessible wallets me se ek hai, Jo order, Booking ki jaanch karne aur sthaan ke aadeshon ki jaanch me madad karega, aur ye bahut teji se hoga.
Payment Mode me Card ya bank transaction aur cryptocurrency shaamil honge. Dusri aur transaction fee prakaar ke business ke liye kam hoga, Web se alag Android aur IOS platform par ek app ke rup me availble hoga, Jo number of users ko badhayega.
Yeh user ke wallet me Cryptocurrency aur INR ka prabandhan karega. Users dwaara aavshyak kisi bhi help ke lie On Demand help available hogi.
Koinex Pros and Cons:
Koinex accha hai aur other cryptocurrency ke mukable iske kai advantages hain.
Yeh Fst aur adhik secure hai.
Apne KYC ko pura karne me kam samaya lagata hai aur teji se business karne me saksham banata hai.
Yeh Bitcoin,BHC aur Ethereum sahit 4–5 altcoins ka samrthan karta hai.
Iska Interface bahut hi aasan hai Jisey aasani se use kiya jaa sakta hai.
Other Wallets ki tulna me tulna me iski fee thodi adhik hai.
Heavy traffic hone ki wajah se kabhi kabhi server down ho jaata hai jiske liye wey aksar trading band kar dete hain.
Yadi aap dektop versio use kar rahe hain to har baar account me sign karne ke liye aapke registered e mail par OTP verification aata hai.
Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai
Buy Bitcoin — Unocoin Ki Help Se India Me Bitcoin Buy Karein [Full Guide]
Bitcoin Mining Guide : Bitcoin Mining Kya Hai
Halanki abhi suruaat ki wajah se ye thodi bahut samsya hai aur nisandeh ye ek secure aur accha wallet hai. Jish trah se ish par users grow ho rahe hain nischit hi ye ek Cryptocurrency ki duniya me accha mukaam hasil kar lega. Meri maane to Koinex jaldi hi indian market me acchi pakad bana lega.
- Bitcoin Gold, Golem, IOTA, Omisego and few more standard coins coming soon
Join now and get 50 Rs Free on first trade with this referral code:
Koinex Indian Cryptocurency Exchange ke baare me apne vichaar Hume comments me jarur dein. Hum aage aur bhiBitcoin, Bitcoin Minning aur Cryptocurrencyke baare me information dete rahenge.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",https://miro.medium.com/v2/resize:fit:500/0*NkirQKbfW4fuKris.png,"Cryptocurrency, Koinex, Bitcoin Wallet, Bitcoin, Bitcoin Mining"
https://medium.com/@imnitishverma/ethereum-vs-bitcoin-ethereum-aur-bitcoin-me-kya-difference-hai-b85854bad326,Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai,Nitish Verma,2020-04-22T11:26:36.165Z,"Vitalik Buterin dwara Varsh 2014 ki suruaat me North America me Bitcoin Conference me Ethereum ki ghoshna ke baad Ethereum par bahut adhik logo ka dhyaan gaya. Iski Badhti Popularity ka parinaam ye hai ki Bitcoin Ke saath lagataar iski tulna ho rahi hai. Aaj Ke Post me Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai? Iske Baare batayenge.
2. Ethereum Me Blocks ka samay Bitcoins ki tulna me kam hai. Bitcoins Blocks ko clear karne me 10 minute ka samay lete hain wanhi Ethereum 14 se 15 Seconds me ye kaam karta hai. Yeh Fast transaction ki anumati deta hai. Ethereum aisa Ghost Protocol ka use karke karta hai.
3. Ethereum Ki tulna me Bitcoin ka Economic Model bhi Thoda Different hai. Bitcoin Block Rewards har 4 saal me aate hain, Jabki Ethereum Pratyek Varsh ke antraal par saman raashi jama karta hai.
4. Ethereum me unki Computational Complexity, Bandwidth Use, aur Storage ki Jaruraton ke aadhar par Transaction ka ek alag tarika hai. Bitcoin Transaction ek dusre ke saath saman rup se pratispardha (Compete) karte hain. Isey Gas in Ethereum kaha jaata hai aur Bitcoin me Prati Block limit hota hai, Yeh Block Size ke dwara limit hota hai.
5. Ethereum Ka apna Complete Turing Internal Code hai. Ek Complete Turing Code ka matlab hai enough computing power and enough time.. Janha Kuch Bhi calculated kiya jaa sakta hai. Jabki Bitcoin ke saath aapko ye Flexibility nahi Milti hai.
6. Ethereum Me Bitcoin ko jaari kiye jane ke dauraan Crowd Funding se dhan Muhayaa karaya gaya tha aur suruaat me adhikansh coins Khud hi mine kiye jaa chuke hain. Ethereum ke saath 50% Coins ki mining 5 saal me hogi.
7. Ethereum Centralized Pool Mining ko apne Ghost Protocol ke madhyam se Rewarded stale blocks ko discourage karta hai. Jiska saaf matlab hai aapko Block Prasar ke mamle me pool me hone ka koi fayda nahi hai.
8. Ethereum ASIC ke pryog ke khilaf Mitigate karne wale ETHASH namak ek memory hard hashing Algorithm ka upyog karta hai aur logon dwaara apne GPU ke upyog se decentralized mining ko protsahit karta hai.
Bitcoin Wallets Ke Types
Indian Users Ke Liye Top Bitcoin Wallets
Bitcoin Ki aur adhik Jankari ke Liye Yanha Click Karein.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",,"Bitcoin Wallet, Bitcoin, Bitcoincash, Bitcoin Mining"
https://medium.com/@ryanrezafadillah/tripadvisor-data-review-bdc908c0f9a4,Scrapping Data Using R,Ryan Reza Fadillah,2020-07-17T13:48:24.275Z,"Hi Data Enthusiast !!!
Materi kali ini kita akan menganalisis data review Marina Bay Sands Hotel, Singapore. Jadi kita ingin mengetahui bagaimana sih secara garis besaranya review pengjung terhadap hotel tersebut.
Review hotel bisa kalian dapatkan di websiteTripAdvisordan kalian juga perlu mengunduh SelectorGadget untuk melihat posisi review yang akan diambil.
Pada materi ini package yang kalian butuhkan sebagai berikut,
Lalu copy-kan link halaman yang memuat review hotel yang akan kita scraping dan pindahkan link tersebut kedalam R
nampak hasilnya seperti gambar dibawah,
Selanjutnya sorot salah satu komentar yang ada dengan menggunakan SelectorGadget,
Lalu masukkan script letak review ke dalam R,
nampak hasilnya seperti gambar di bawah,
Selanjutnya, kita akan membersihkan huruf yang tidak diperlukan misal “\n” dan lainnya dan simpan data yang telah dibersihkan dalam csv
Selanjutnya, buka file csv yang telah tersimpan dan lihat jika ada baris yang tidak memuat data maka bisa kita hapuskan. Contohnya dibawah ini pada baris pertama tidak memuat data yang berarti jadi bisa kalian hapus
Selanjutnya, panggil kembali data csv yang telah dibersihkan kedalam R,
Lalu buatlah Corpus dan str untuk dokumen di atas,
Kemudian, cobalah dari kelima dokumen yang ada,
maka akan nampak seperti gambar di bawah,
Setelah itu, buatlah matriks kata- kata dalam dokumen dengan menggunakan script di bawah ini,
Bisa dilihat jika dari 5 dokumen ada 262 kata yang berbeda, maka kita gunakan
maka akan menghasilkan,
Pada gambar di atas bisa kita ketahui jika, pada dokumen ke-1 kata “citi” muncul 2 kali, “hotel” 8 kali, dst. Kemudian kita dapat menampilkan semua kata- kata yang ada,
maka akan menghasilkan,
Selanjutnya run script dibawah ini,
maka akan menghasilkan,
(dokumenDTM,3) artinya kata — kata yang mempunyai frekuensi 3 kali muncul atau lebih, sedangkan (dokumenDTM,4) rtinya kata — kata yang mempunyai frekuensi 4 kali muncul atau lebih.
maka akan menghasilkan,
Selanjutkan, buatlah matriks nya,
Setelah muncul matriksnya, kita akan melihat 6 data terbesar,
maka akan menghasilkan,
Setalh itu, kita akan membuat wordcloud dari kata- kata di atas menggunakan script di bawah ini,
maka akan menghasilkan,
Dari hasil wordcloud diatas, terlihat masih ada kata “the”, “was”, dll yang artinya kurang bagus. Jika kita ingin mengetahui asosiasi kata- kata yang sering muncul, misal kata “good”, “great”, “nice” maka gunakanlah perintah berikut,
dan akan menghasilkan,",https://miro.medium.com/v2/resize:fit:452/1*w5oyrWsl7O9UfXfvDc5ncw.png,"R, Data Science, Hotel Reviews, Sentiment Analysis, Tripadvisor"
https://medium.com/@yogiisari/why-use-selenium-grid-5c2ab3f13a5a,Why use Selenium Grid?,Yogi is ariyanto,2021-06-04T02:55:38.936Z,"Selenium Grid adalah alat yang digunakan untuk menjalankan tes paralel di berbagai mesin dan browser yang berbeda secara bersamaan sehingga proses eksekusi lebih cepat.
Dengan Selenium Grid, satu server bertindak sebagai hub yang merutekan perintah dalam format JSON ke satu atau lebih node yang terdaftar.
Memungkinkan kita untuk menjalankan tes secara paralel pada beberapa mesin, dan untuk mengelola berbagai versi browser dan konfigurasi browser secara terpusat.
Mengapa Selenium Grid?
How to use Selenium Grid version in Mac
*sesuaikan path location selenium server standanlone , hub, node & chrome driver
maka selenium grid sudah jalan di localhttp://localhost:4444/grid/console
Untuk setup automation testing untuk parallel execution menggunakan beda browser pada selenium Remotewebdriver dengan selenium grid. untuk menjalankan paralel test yang di perlu di perhatikan ada di:
Pada codingan di atas, menggunakan classDesiredCapabilitiesuntuk mengatur properti pada Selenium WebDriver. Properti pada Capability digunakan untuk konfigurasi browser seperti BrowserName BrowserVersion, lokasi script yang akan di Execute dan lainnya.
Catatan:script hanya akan berjalan di versi browser yang diinstal pada laptop. Misalkan, jika kamu menjalankan automation di Chrome versi 72 dan Chrome di laptop kamu versi 74, maka scipt automation kamu akan fail karna browser tidak sesuai. itu terjadi juga jika settingan platform tidak sesuai dengan laptop kamu.
Full source code diGitHub:https://github.com/yogiis/autobot-selenium-grid-ui
Test Engineer atFlip.id",https://miro.medium.com/v2/resize:fit:800/1*bwQQ6gsi256xirno2x0u4g.png,"Web Automation Testing, Selenium Webdriver, Selenium Test Automation, Selenium, Selenium Grid"
https://medium.com/jeasee%E9%9A%A8%E7%AD%86/python-%E5%8F%8A-firmata-c8d104c1cf00,Python 及 Firmata,Jease,2020-02-17T07:25:16.332Z,"在上一章節中我們使用了PySerial函示庫來進行簡單的操作，但是他缺乏了Firmata的通訊能力，而PyFirmata他也是Python的一個函示庫，看他的名稱就知道他是支援Firmata的，Python也有提供其他支援Firmata的函示庫，但我們這邊聚焦在PyFirmata上。
安裝PyFirmata的方法跟安裝PySerial一樣，但是由於PyFirmata有最新版本了，我們這邊直接安裝PyFirmata2而非是PyFirmata
2. 要使用Firmata協定時，我們需要再次的上傳StandarFirmata草稿碼到Arduino板子上
3. 開啟Python，並輸入下面的指令
在這邊我們將pyfirmata2中的Arduino套件給引用近來。
4. 接下來我們要去設定腳位，跟之前PySerial類似的設定方法，在這邊我們會先去設定我們的阜號，接下來再去設定我們設號的port跟Arduino連接起來
而由於我們使用的版本是pyfirmata2，而非pyfirmata在這次更新成2之後，提供了自動辨識port腳位，所以我們在port的位置可以更換成以下的命令
而在我們設定完成之後，python與開發板間的通訊便已經連接好了，Arduino上會有兩個Led燈開始閃爍。在之前的測試中，我們使用現成的Firmata軟體來控制pin13Led燈的閃爍，而現在我們嘗試使用Python來做控制。
而當我們把這些指令編輯成一個py文件，再去執行的話，我們也就不需要再去動用到Arduino草稿碼了。
在PyFirmata2這次的更新之後，可支援的開發版有UNO、MEGA、DUE、NANO這四塊板子，且在於切換開發版的時，不需要重構程式碼即可使用。
但當我們都不是使用以上這些開發版的時候，我們可以參考UNO開發版的範例來進行修改成目前所使用的開發板來實作
根據上面UNO的示範，我們可以去依照我們開發版的腳位去做相關的設定，所以我們需要客製化Dictionary物件，去像digital、analog、pwm、use_ports、disabled等腳位去做相關的更動。而在我們設定好我們的開發板參數之後，要用以下指令來呼叫開發板，在乎教之後我們便將開發板給同步了
在這一小節我們將重頭講起如何使用PyFirmata，而作為操作Arduino開發板與Firmata通訊協定的一部分，我們需要將Arduino開發板作為一個變數(參數)，PyFirmata能讓使用者將開發粄作為一個變數去使用，而在使用前我們需要去將套件給引入，並將她的COM給設定好
在我們指派完變數board後，我們便可以開始做一些處理，例如讀取某個腳位或使用此變數傳遞訊號到腳位。而在我們要去指定某一個腳位的角色時，我們會用get_pin()方法。在下列程式碼中，d代表數位腳位，7為腳位編號，i則代表腳位類型為輸入。若此時我們要的訊號是邏輯訊號，則d的位置要改成a。若是要輸出訊號則要將i的位置改成o，若訊號是PWM訊號，i的位置需要改成p
而此時我們便可以對被指定角色的腳位去做一些動作，像是讀取或是輸入:
當然你不想指定腳位的角色也是可以去命令他去輸入跟讀取的:
Arduino的類比腳位以線性比例將0–5V間的輸入電壓轉換成0–1023，然而
上面的那兩個函式是在處理數位訊號會用到的，接下來說明邏輯訊號。而邏輯訊號使用上較數位訊號更佳的複雜，首先我們需要先去設定我們採樣的頻率，簡單來說就是指一秒鐘我們要收集多少數據，接下來我們有兩種去讀取數值的方式，第一種是用read的方式，這種方式比較單純，直接寫read()就可以了，第二種方法是用調用的，也就是常跟class或是def一起使用，而他們在使用後都需要加上enable_reporting，這樣資料才會回傳回來
以下來做個示範吧，下面的範例是使用調用的方式去做取值
在處理PWM與SERVO訊號前，我們先講一個函式可以幫助我們輕鬆的去設定目前pin腳使用哪一種方法，有輸出、輸入、伺服馬達、PWM訊號可以選擇，但是在使用PWM跟SERVO前我們需要先把他們import進來
首先補充一下PWM訊號，在電子學領域中，PWM是一種訊號調變技術，大量用於提供零件電力控制，與數位腳位配合後，PWM便以方波及控制訊號寬度取得類比訊號。Arduino開發板的數位信號只有兩種狀態5V跟0V，但是可以藉由控制HIGH與LOW之間的開關規則來產生方波脈衝。藉由改變這些脈衝的寬度，可以產生任何介於0V-5V的電壓值，從下圖來看的話，有一個方波為25%的工作週期寬度，也就意味著我們有0.25∗5V=1.25V0.25∗5V=1.25V的電壓
Arduino語言使用analogWrite()功能之援PWM，能版0到5V的電壓值線性對應到0到255，以50%工作週期(2.5V)來說轉換過去的值為127，同樣的20%為64
在Arduino中，PWM為0到255之間的訊號，代表工作週期0到100%的脈衝寬度，pyfirmata函示庫中簡化了PWM的使用方法，我們只需要將數值變成0-1.0之間的浮點數就可以了，如果想要一個50%的工作週期，只需要輸入0.5就可以了，在輸入後pyfirmata會自動轉換傳送適當的數值到Arduino開發板:
在伺服馬達的部分，我們一開始依樣需要先去將腳位指定模式成SERVO，接下來才能去做相關的設定
接下來我們要去設定角度，設定腳度也很簡單，直接寫入0–180內的數值就好
除了上面介紹到的函式還有一些函式也是可能用到的
2. 跟time模組提供的sleep()很像，但他是以秒來提供無間段的暫停
4. 程式執行後切斷Firmata與Arduino的連接
在這一小節中，我們從Firmata的安裝講到他的使用方法，最後用一個小範例做一個結束，而這個範例是Firmata2在Github上的範例，他還有提供其他的範例，可以自己去查看",https://miro.medium.com/v2/da:true/resize:fit:795/0*-2p9Ymp7Nm4v4wZO.gif,"Arduino, IoT"
https://medium.com/@akbarfarraz/yang-maha-kecil-c22c7b4143cd,Yang Maha Kecil,Farraz Akbar,2021-04-26T12:29:28.856Z,"Dua tahun sebelum saya lahir, Carl Sagan pernah menulis sebuah renungan indah yang terilhami oleh sebuah foto kabur yang diambil dari jarak 6.4 milyar kilometer dari tempat kita tinggal. Satu Titik Biru Pucat judulnya. Sesuai namanya, ia memaknai sebuah foto yang menampakkan mungilnya planet bumi di tengah semesta. Bumi hampir tidak terlihat, sekalipun terlihat ia hanya satu titik biru pucat yang samar. Berikut cuplikan renungan Carl Sagan yang berjudulA Pale Blue Dot.
Look again at that dot. That’s here. That’s home. That’s us. On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every ""superstar,"" every ""supreme leader,"" every saint and sinner in the history of our species lived there--on a mote of dust suspended in a sunbeam.
The Earth is a very small stage in a vast cosmic arena. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds.
Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.
The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.
It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we’ve ever known.
Sagan menggarisbawahi keserakahan manusia yang berakibat pertumpahan darah yang mubazir, yang hanya memperebutkansebagiandari satu titik biru pucat yang umurnya barusekedip matadibandingkan semesta. Andai saja kita semua menyadari kesendirian komunitas kita di tengah lautan kosmos yang maha luas ini, mungkin kita akan saling jaga, alih-alih saling jagal.
Mengakui posisi kita sebagai Yang Maha Kecil haruslah didampingi dengan pengakuan adanya Yang Maha Besar. Dalam Islam, pengakuan ini disebuttakbir. Makna kalimat takbir adalah pengingat bahwa tiada hal yang lebih besar dari-Nya. Namun, takbir juga bisa dimaknai untuk menyadarkan kita betapa kecilnya manusia di hadapan-Nya. Maka dari itu saya seringkali heran dengan kelompok orang yang meneriakkan kalimat takbir untuk mengusung egonya, untuk menindas kaum minoritas. Bukankah yang dibesarkan dalam kalimat takbir adalah Tuhannya, mengapa jadi egonya yang tambah besar, jangan-jangan ego adalah Tuhan-Nya(?).
Seorang muslim mengucap takbir minimal 85 kali dalam sehari ketika mendirikan shalat. Kebanyakan dari kita (termasuk saya) 85 takbir tidaklah cukup untuk mengerdilkan diri kita di hadapan semesta dan pencipta-Nya. Jika demikian, tambahlah dengan ibadah sunnah, maka bertambah pula takbir harian kita. Bila perlu bertakbirlah dalam hati setiap saat, tidak mesti ketika shalat. Apapun caranya, takbir kita tidak boleh salah sasaran, jangan sampai ego kita yang dibesarkan takbir, alih-alih Tuhan kita.
Wallahualam bishawab
Writes about science, energy, data center, film, photography, and personal reflections.",https://miro.medium.com/v2/resize:fit:1035/1*C-2qIeBVO6R9DSd7N_yQ-w.jpeg,"Islam, Cosmos"
https://medium.com/qasir/technical-debt-dalam-software-development-eb824f40839e,“Technical Debt”dalam Software Development,Rahmad Hidayat,2020-06-23T07:07:33.769Z,"Aplikasi untuk membantu usahawan berkembang lebih baik dimulai dari pencatatan transaksi
Dalamdevelopmentsebuah software, sebagaiengineersering kali kita berhadapan dengan penentuan berapa lamatimelineyang dibutuhkan untuk men-developsebuah fitur yang ideal dari sudut pandangengineervstimelineyang diberikan olehproduct team.
Timelineyang dipersingkat biasanya menghasilkanTechnical Debtkarenaengineertidak mempunyai waktu yang cukup untuk berpikir secara jangka panjang maupun memikirkanbest overall solutiondari sebuahcode.
Sangat lumrah jika beberapa prosestechnicaldipangkas oleh engineeruntuk membantuproduct teammengejar momentum, terutama diStartup. Tapi tentu ada n̶y̶a̶w̶a̶ harga yang harus dibayar di kemudian hari.
Inilah yang disebut utang teknikal aliasTechnical Debtatau kita singkat“tech debt”.
Utang jika tidak dibayar, akan menghasilkan bunga. Semakin besar bunganya, semakin berat pula untuk melunasi utang. Jika kita tidak bisa membayar utang, semakin lama hidup kita akan semakin tidak nyaman karena bakal dikejar-kejardebt collector.
Ini juga terjadi terjadi dalamsoftware development,tech debtyang besar akan mempersulitscalabilitydanmaintenabilitydari sebuahsoftware. Mungkin awalnya bisa saja kita “akali” implementasinya agar tetap bisadeliver. Tapi semakin lama,tech debtini akan mengurangi produktivitas karena kita harus mencari cara kotor hingga akhirnya tidak bisa lagi menambahkan fitur sama sekali. Bahkan yang lebih buruk, kita juga akan kesulitanme-maintain softwarekita, karena ketika kita baca code nya, isinya tambal sulam sana-sini. Ruwet lah, kalo kata Pak Jokowi.
Lalu dengan apa kita membayarnya? Tentu saja dengan “refactoring”.
Oke, cukupintro-nya. Setelah ini saya ingin berbagi pengalaman tentang apa yang kami lakukan untuk mengatasitech debtdi Qasir.
Hal pertama yang kami lakukan adalah menyamakan persepsi. Tidak semuastakeholderpaham tentangtech debt. Biasanya yang sering disalahpahami adalahbugs, masalahUI /UX, dan fitur-fitur yang tertunda dianggap termasuktechnical debt.Padahalsebenarnyatidak.
Saya suka penjelasantech debtdariAgile Allianceini:
“The Technical Debt concept is an effective way to communicate about the need for refactoring and improvement tasks related to the source code and its architecture.
Code issues may be related to architecture, structure, duplication, test coverage, comments and documentation, potential bugs, complexity, code smells, coding practices and style. All these types of issues incur technical debt because they have a negative impact on productivity.”
SingkatnyaTechnical Debtterjadi ketika kita membangunsoftwaresecara cepat dan tidak mendesign untuk jangka panjang.
Hal-hal yang sepertibugs,UI/UX, fitur yang dikurangi, memang termasuk utang, tetapi tidak dikategorikan sebagaitech debt. Untuk mempermudah pemahaman, mari kita kategorikan jenis-jenis utang ini dengan istilah lain.
Selain dari masalahcode,tech debtjuga mencakup masalah dari pihak ke-3. Bisa seperticodeyangdeprecated,Libraryyang sudah tidak di-maintain, ataupun kemajuan teknologi diplatformyang kita pakai sehingga membuatcodekita tidakrelevanlagi dan membuatsoftwareyang kita buat cepat atau lambat sulit untukscale-updan di-maintainatau malah menurunkan produktivitas kita.
Setelah semuastakeholdersepemahaman, kita lanjut ke perencanaan dan eksekusinya.
Tidak ada solusi yang singkat untuk menanganitech debtini. Hal pertama yang kami lakukan adalah dokumentasitech debt. Mendokumentasikantech debtadalah hal yang wajib untukengineer. Kemudian kita urutkan berdasarkanimpactdaneffort-nya. Setelah itu kita ajukan dan presentasikan keproduct teamuntuk meminta wakturefactoringdi dalamproduct roadmap.
Semudah itu? tidak juga. Kita harus bisa menerjemahkan manfaat teknis darirefactoringyang akan kita lakukan ke dalam kepentingan bisnis dan produk. (dibaca: meyakinkan tim produk). Hal ini sangat penting, karena ketika antar-stakeholdersama-sama mengerti konteks,urgency, dan mendapatkan nilai lebih dari sebuahrefactoring,akan lebih mudah untukproduct teammemprioritaskan antara fitur /refactoring.
Strategi pertamayang kita lakukan adalah menyisipkan waktu 30% di tiapsprintuntuktech debt. Satusprintumumnya mengalokasikan waktu 2 minggu yang dibagi menjadi 7 haridevelopmentdan 3 hariQA testing. Artinya kita bisa memakai 2 hari kerja itutech debtdi setiap sprint.
Tetapi untuk menyelesaikantech debtyang kompleks, 30% tiap sprint itu sangat-sangat kurang dan sulit untuk memecah pekerjaannya untuk tiap sprint.
Strategi keduayang kita lakukan sekarang adalahtech debtpunya wadah sendiri yang dinamakansprint cooldowndan dieksekusi tiap kuartal, dengan 1sprintyang berdurasi 2 minggu (bisa lebih, bisa kurang, tergantung dengan pekerjaanrefactoringapa yang kita angkat dan seberapaimpact-nya).
Selain dari sprintcooldownyang kita punya, diengineeringteamjuga menerapkan satu peraturan sederhana yang sangat berguna untuktech debtyang sifat nya kecil-kecil:The Boy Scout Rule.
Prinsip sederhana yang di adopsi dari Boy Scout Rule adalah:
“Leave the campground cleaner than you found it.”
Seorang pramuka, ketika selesai berkemah membiasakan diri untuk membersihkan tempat dia berkemah lebih bersih dari saat dia datang.
Setiap kaliengineermengubah sebuah code,engineerproaktif membersihkancode-smellyang ditemukan diclasstersebut diluar dari fitur yang sedang dikerjakan.
Sederhananya seperti masalahtypo,naming, kompleksitas sebuahmethod,style, duplikasi sebuah helperdan hal kecil lainnya bisa diatasi secara berkala dan tidak membutuhkan waktu yang lama menjadi kebiasaan yang sangat membantu mengurangitech debtdan meningkatkantechnical ownership.
Tidak ada yang salah dari strategifaster time-to-market. Tetapistakeholderdiluarengineering teamharus punyaawarenessbahwa cepat atau lambattech debtakan mengganggu kita.
Tech debttidak akan pernah selesai selamasoftwareterus dikembangkan. yang kita perlukan adalah strategi untuk terus menjagatech debttidak menjadimonsteruntuk kita dan kita terus melakukan iterasi untuk hal ini.
By the way,ini tulisan pertama saya di Qasir. Silakan tekan tombolclapssebanyak-banyaknya jika tulisan ini bermanfaat, dan mari berdiskusi di kolom komentar tentang hal terkait yang mungkin tidak ter-coverartikel ini.
Aplikasi untuk membantu usahawan berkembang lebih baik dimulai dari pencatatan transaksi
I talk about software engineering especially mobile apps and my whole experience during my career. Currently my role is Sr Principal Engineer at SaaS Company.",https://miro.medium.com/v2/resize:fit:1200/1*OPpKFpgdTCPisQRV92U9Yg.jpeg,"Software Development, Tech, Startup, Technical Debt, Tech Debt"
https://medium.com/tunaiku-tech/apache-airflow-is-fun-for-data-engineer-b9b0b45a2550,Apache Airflow Is Fun For Data Engineer!,Salim Masagus,2020-01-05T06:08:22.409Z,"Stories behind Tunaiku Products, Engineering and Data Team
Apache Airflow implementation in Tunaiku
Pada tulisan pertama saya di medium ini, saya akan bercerita tentang pengalaman saya menggunakan AirflowsebagaiData Engineerdi Tunaiku. Sekilas tentangAirflow,Data EngineerTunaiku menggunakanAirflowterkhusus dikarenakan kebutuhanschedulling task,danmonitoring workflowyang telah dirancang untuk kebutuhan data di Tunaiku, dengan menggunakanAirflowsemua prosesautomasidanmonitoringakan terhandle!!! . untuk lebih jelasnya anda dapat melihat dokumentasinyadisini, dan untuk memahamietentialtentangData Engineer, anda dapat membacablog creator Airflowmaxime beauchemindisini
To a modern Data Engineer, traditional ETL tools are largelyobsoletebecause logic cannot be expressed using code. As a result, the abstractions needed cannot be expressed intuitively in those tools. Now knowing that the Data Engineer’s role consist largely of defining ETL, and knowing that a completely new set of tools and methodology is needed, one can argue that this forces the discipline to rebuild itself from the ground up. New stack, new tools, a new set of constraints, and in many cases, a new generation of individuals. — Maxime Beauchemin
Pada case kali ini saya menggunakan case membuat automate report yang datanya berasal dariBigQuery.Sekilas tentang BigQuery, BigQuery merupakan sebuahanalytic data-warehouseyang termasuk ke dalam produk Google Cloud Platform (GCP), untuk lebih lengkapnya anda dapat melihat dokumentasinyadisini
berikut konfigurasi saya ketika membuatAirflow:
Pada proses pengambilan data dari beberapa table sesuai dengan kebutuhan report, berikut contoh script pengambilan data di BigQuery :
pada script diatas, saya menuliskan query yang mana menuliskan semua kolom yang ingin saya ambil, dibanding menggunakan ‘*’, kenapa saya memanggil semua kolom ? , anda dapat melihatdisiniuntuk penjelasannya. pada kondisi wheredate(_PARTITIONTIME)ini merupakan sebuah partition pada table di BigQuery agar menghemat cost dan mempercepat proses query.
setelah penulisan query, hasilnya akan saya simpan kedalam dataFrame, setelah itu akan saya loads ke ndjson (NewDelimitedJson) untuk menyesuaikan spesifikasi yang diperlukan BigQuery dalam Load Data to BigQuery using File.
Berikut contoh script mengirimkan data ke BigQuery :
Pada proses ini, saya load data ke sebuah table yang sudah di buat partition menggunakan partition_field, yang mana partition nya menggunakan sebuah kolom, dan padadestinasion_dataset_ref,saya mendefine table diikuti oleh lambang ‘$’ dan tanggal tanpa ‘-’, itu merupakan cara define table menggunakan partition.
Dikarenakan timData EngineerTunaiku telah sepakat menggunakan script dari pada operatorAirflowuntuk memproses data, maka dari itu, saya menggunakan bash operatorAirflowuntuk menjalankan script saya, bash operator sama seperti bash pada terminal. berikut contohnya :
Terdapat ds, itu merupakan macros variable untuk mendapat kan tanggal dengan formatYYYY-MM-DD, untuk lebih lengkapnya dapat melihatdisini
Hasilnya, ketika seorangData Engineermelihatdagsnya memperlihatkan warna hijau hijau, harusnya sudah dapat tersenyum karena proses yang dibuatnya telah berhasil! , dan akan automatis sesual dengan settingschedulenya.
“No man is so wise that he can afford to wholly ignore the advice of others.” — James Lendall Basford (1845–1915), Sparks from the Philosopher’s Stone, 1882
Dengan masuknya kita ke notimportant section!berakhir sudah sharing knowledge saya kali ini, saya harap ini adalah awal dari lanjutan tulisan saya di medium, dan ini jauh dari kata sempurna, akan tetapi ingatlahMark Zuckerberg tidak membuat Facebook dalam percobaan pertamahehehe :D
Saya harap artikel yang saya tulis ini dapat membantu semua orang, terkhusus bidangData Engineer. Jika Anda memiliki pertanyaan atau diskusi lebih lanjut, Anda dapat menuliskan komentar, atau menghubungi saya melalui email. Terima kasih!
Some rights reserved
Stories behind Tunaiku Products, Engineering and Data Team
A man with fast learner, enthusiastic, high achiever, collaborative, breaker the limit, and growth mindset.",https://miro.medium.com/v2/resize:fit:1200/1*FJsMPN5kPMI7JuqhsaP7rA.png,"Apache Airflow, Data, Software Engineer, Google Cloud Platform, Data Engineer"
https://medium.com/@devitawidya/analisis-data-marketplace-sales-235755601e0d,Analisis Data Marketplace Sales,Devita Widyasari,2020-12-23T15:08:25.466Z,"Saya mendapatkan data penjualan barang dalam sebuah marketplace. Data yang digunakan adalah dataorder listyang berisi record order yang dilakukan setiap harinya, dataorder detailsyang berisi detail pembelian barang, dan datasales targetyang berisi target jumlah penjualan setiap bulannya untuk setiap kategori. Seluruh dataset dapat diunduh dengan klik nama datanya.
Beberapa hal yang ingin diketahui dari data-data tersebut, yaitu:
Selanjutnya, import library yang dibutuhkan. Library pandas saya gunakan untuk mengeksplor dataset. sqldf saya gunakan untuk gather data sebelum visualisasi. untuk visualisasi sendiri saya gunakan library plotly.
Import ketiga dataset.
Order ID adalah ID untuk setiap pembelian barang. Order Date adalah tanggal pembelian barang. CustomerName adalah nama pembeli. State adalah negara bagian, dimana pembelian tersebut dilakukan. City adalah kota dimana pembelian tersebut dilakukan.
Order ID adalah ID untuk setiap pembelian barang. Amount adalah total harga dari barang-barang yang dibeli / berhasil terjual. Profit adalah keuntungan yang didapat perusahaan dari setiap pembelian. Quantity adalah jumlah barang yang dibeli. Category adalah kategori barang. Sub-Category adalah sub-kategori barang.
Month of Order Date adalah bulan dan tahun target pembelian. Category adalah kategori barang. Target adalah angka target penjualan.
Rename (ubah nama kolom) dengan menghilangkan spasi pada nama kolom agar mudah untuk penulisan kodenya.
gunakan function .info() untuk mengetahui informasi mengenai kolom dan tipe data, dan deteksi missing value setiap kolomnya dengan function .isnull(). sum().
Dalam dataset Order List terdapat 60 baris kosong di setiap kolomnya. jika ingin menghapusnya, bisa menggunakan function .dropna(), namun tidak di-drop pun tidak berpengaruh karena selanjutnya dilakukan inner merge dengan dataset orderdetails. Lalu ubah tipe data dan format tanggal OrderDate menjadi datetime mm-YYYY untuk memudahkan agregasi selanjutnya, lalu rename menjadi ‘Month’ karena telah menjadi data bulan pembelian. Selain itu, hapus kolom CustomerName karena tidak akan digunakan untuk analisis selanjutnya.
Sebelum menggabungkan table orders dan salestarget, terlebih dahulu mengubah tipe data kolom Month dari object menjadi datetime dengan format yang sama yaitu mm-YYYY. Karena sebelumnya pada data salestarget, kolom Month berformat bbb-YYYY.
Join-kan kedua table dengan key Month dan Category.",https://miro.medium.com/v2/resize:fit:739/1*VpafAHqRMwWvDc-GrNXpEg.png,"Pandas, Plotly, Data Wrangling, Data Analysis"
https://medium.com/@adtiyadwiramadani98/membuat-resful-api-menggunakan-codeigniter4-f4f6951b2c4f,Membuat Restful Api Menggunakancodeigniter4,ADITIYA DWI RAMADANI,2021-01-04T02:05:44.645Z,"Hello teman-teman jumpa lagi. bersama saya Aditiya Dwi Ramadani. Di tutorial Kali ini kita akan membuat Restful Api menggunakan framework codeigniter4.
codeigniter sendiri Merupakan Framework Yang Menggunakan bahasa Pemograman Php . Dengan Konsep Mvc. Yaitu Model Views Dan Controller . Codeigniter Sendiri Mempuyai Ukuran Yang Kecil Teman Teman dan Fiturnya Juga Lumayan Banyak. Ok Jadi Itu Pengertian Singkat Tentang Codeigniter.
Untuk Menginstall Codeigniter Harus Install Php dulu di Laptop Atau Computer Kalian. Atau Kalian Bisa Download Xampp DiSiniTeman Teman.Kemudian Kalian Install Composer Dulu Di Laptop Atau Computer Kalian. Kalau Belum Megerti Tentang Composer Teman Teman Bisa Klik Link DiSini. Atau Teman Teman Bisa Langsung Download Composer DiSini.
Kalau Suda Semua Buka Terminal Kalian. Untuk Di windows Bisa Buka Cmd.Kemudian Ketik Di Bawah Ini.
Ok Kalian Suda Menginstall Codeigniter Computer Kalian Masing-Masing. Buka text Editor Kalian Masing-Masing. Kemudian copy file env menjadi .env.
Buka File .env.Kemudian Cari CI_ENVIRONMENT. Mejadi Seperti Di Bawah.
Sedikit Penjelasan. Kita Menggubah ENVIRONMENT Teman Teman Menjadi development. Kalau Kalian Udah Deploy Appnya. Kalian Edit ENVIRONMENT Menjadi Semula Ya Teman Teman.
Ok Kalian Buka folder Controllers nya Kemudian Tambahkan File Base.php.Buka Filenya Tabahkan Di Bawah Ini Teman Teman.
oK Kita Suda Membuat Controllernya. Kemudian Buka Lagi Filenya . Teman Teman.Di Dalam class Base Kalian Tabahkan Di Bawah Ini.
Kemudian. Buka Terminal Teman Teman. Kemudian Ketik php spark server.Dan Coba Buka Browser Teman Teman. Ketik Di Urlnya. localhost:8080. Maka Akan Seperti Gambar Di Bawa Ini.
Kemudian Buka Xampp Teman Teman. Aktifkan Apache Sama Mysqlnya.
Buka Terminal Lagi Teman. Kemudian Ketik. php spark migrate:create add_blog. kemudian buka File Database/Migration/2020–12–25–012344_add_blog.php. File Nya Pasti Beda Teman Teman.Sesuai Waktu Kalain Membuat Migrasi databasenya Teman Teman. Di Dalam Methods up. Tambahkan Di Bawah Ini Teman Teman .
Kemudian Edit File .env nya Cari DATABASE di file nya Edit. Seperti Di Bawah Ini
Kemudian Buka Browser Kalian Lagi. Kemudian Ketik .localhost/phpmyadmin. Kemudian Kalian Click New. Lihat Contoh Gambar Di Bawah Ini.
Kalau sudah Kalian Buka Terminal Kalian.Ketik php spark migrate.
Kemudian Kalian Buka Folder App/Models. Kemudian Buat File Yang Bernama Blog.php. Kemudian Buka Filenya. Dan Tambahkan Di Bawah ini.
Kemudian Kalian Ke Controllernya Lagi . Untuk Create Dan Read . Kemudian Kalian Edit Filenya. Seperti Di Bawah Ini
Kemudian Kalian Buka FIle App/Config/Routes.php. Dan Kalian Edit Filenya Seperti Di Bawah Ini
Kemudian Untuk Details, Delete Dan Edit.Buka File Controllenya.Kemudian Kalian Edit Seperti Di Bawah Ini.
Kemudian Buka File App/Confing/Routes.php. Dan Kemudian Edit Seperti Di Bawah Ini.
Finally. Kalian Suda Membuat Rest Api Menggunakan Ci.
Ok Trimakasih Suda Membaca.",https://miro.medium.com/v2/resize:fit:1000/1*8lLCWmv1aYJZbbUSXaz7mA.jpeg,"Rest Api, PHP, MySQL, Codeigniter"
https://medium.com/morning-haiku/christmas-morning-20201225-9743d4116255,Christmas Morning 20201225,Jenny Lawton,2020-12-25T13:02:46.251Z,"Haiku and Sijo from my daily awakening mind
Quiet calm through rain
Wind blowing in the morning
Grateful for the day
Haiku and Sijo from my daily awakening mind
entrepreneur, mentor, advisor, mother, wife, dog parent and lover, tennis player : changing the world one woman and entrepreneur at a time",https://miro.medium.com/v2/resize:fit:1200/1*2vKQX_QU8sZbND69VuzB2g@2x.jpeg,
https://medium.com/@ridzki-hidayat/utak-atik-konseling-psikologi-di-halodoc-ux-case-study-4d7cedece123,Utak-atik Konseling Psikologi di Halodoc (UX Case Study),Rdzkday,2021-02-16T21:20:19.392Z,"Hallo! Setelah melewati draft UI_Final_Revisi_akhir_VVI.figma errr, ga ding canda, sekarang kansemua udah auto save. Akhirnya prototype kami, saya (UX Designer);gempitaff(UX Researcher); danAndhika S Pratama(UX Writer), selesai dan siap untuk di test kepada pengguna setia layanan Halodoc. Sembari menunggu hasil Usability Testing (UT), simak dulu yuk udah sejauh apa sih perubahan yang kami buat.
Sebelumnya, kami menggunakan Design Thinking dalam proses pembuatannya, udah paham kan ya? Bagus. Lanjut! Alasan kenapa kami memilih Halodoc sendiri simpel. Kami merasa aplikasi ini bisa di tingkatkan lebih baik lagi dari beberapa hal minor yang kita temukan serta kita validasi ke user. Thanks buatgempitaffyang telah memandu proses researchnya.
Oke jadi problemnya adalah …
“Bagaimana membuat user merasa nyaman saat berkonsultasi dengan psikolog?”
Sebuah pertanyaan yang bersifat general sebenarnya. Kami tidak berafiliasi dengan Halodoc, atau mungkin belum? Jadi kami belum memiliki data aktual dan/atau permasalahan terkait app Halodoc dari sudut pandang internal. Kami hanya bisa menerka sambil memvalidasi terkaan kami dengan banyak bertanya kepada user.
Oke selamat datang di malam-malam dan akhir minggu penuh diskusi dalam rangka menerapkan apa yang kami pelajari sembari berupaya menambah nilai jual alias portfolio. Namun pada postingan kali ini saya tidak akan banyak menjabarkan keseluruhan prosesnya karena kadang melakukan lebih mudah dibanding menjelaskan, betul? yaa.
Sedikit gambaran, kami melalui proses research, affinity diagram, meramu design mandate, ideate, dan jargon-jargon lainnya yang ada di design sprint ala aplikasi Duco serta beberapa langkah lain terkait UX Writing yang akan dijelaskan olehAndhika S Pratamadangempitaffdalam tahapan Researchnya.
Sedikit memberikan pandangan, dari serangkaian proses empati yang kami lakukan, beberapa problematika yang kami temukan terutama adalah lama waktu konsultasi, metode konsultasi, jadwal konsultasi psikolog, dan metode pembayaran.
Oke baik, saya akan langsung menjelaskan data/input yang kami dapat dan implementasinya dalam desain yang kami buat. Jika penasaran dengan proses lengkapnya, bisa langsung hubungi saya atau kasih komentar dibawah yaa, hehe.
Tidak banyak perubahan di bagian ini, hanya peletakan posisi yang kami ubah dengan berpedoman padathumb rule.Banner hanyalah banner, user hanya perlu mengambil instisarinya saja sehingga bagian dengan aksesibilitas tinggi bagi ibu jari dapat digunakan untuk menu utama.
Kami mengeliminasi shortcut yang ada di navbar dari 5 menjadi 4. Alasannya adalah bila navigasi bisa dibuat lebih mudah dengan fungsi yang lebih maksimal, kenapa tidak? Dengan pengurangan ini kami juga berharap fungsi shortcut pesan yang berisikan iklan atau informasi tambahan akan lebih banyak diakses oleh pengguna. Kami cukup terinspirasi dari tulisaneconomist. Untuk alasan penyederhanaan lainnya mari kita bedah halaman profil.
Ini alasan utama navbar dapat dirampingkan, keberfungsian halaman profil dapat ditingkatkan dengan langsung memperlihatkan riwayat penggunaan Halodoc sehingga shortcut riwayat transaksi (yang isinya bukan seperti transaksi😅) di navbar tidak lagi dibutuhkan.
Saldo halodoc serta paket konsultasi yang sudah dibeli oleh user juga diperlihatkan. Sebelumnya saya berupaya mencari dan tidak menemukan cara untuk mengetahui saldo Halodoc saya. Jadi saya coba menaruhnya disini.
Isi dari menu Pesan dan Lainnya (pengaturan) tidak mengalami perubahan karena bukan menjadi fokus pengerjaan kali ini. Namun ada beberapa hal yang menjadi catatan dibagian pengaturan, seperti pengaturan aplikasi diganti saja jadi pengaturan bahasa yang dapat langsung diubah dengan sekali klik. Kemudian untuk paket sehat dan metode pembayaran sudah bisa diakses di card paling atas di laman Profil.
Mari masuk ke menu utama, kesehatan mental. Sebelumnya terima kasih untuk design rant yang telah menyorot halaman ini di postinganIG, kami jadi semakin terpacu untuk melakukan pembenahan disini.
Halaman ini kami jadikan sebagai gerbang utama sekaligus cara mempromosikan kesehatan mental yang baik. Kami menemukan bila user sebelum memutuskan berkonsultasi membutuhkan pertimbangan sekaligus kekhawatiran sendiri. Alih-alih memperbesar kekhawatiran itu, kami berupaya menenangkan user.
Ide dari halaman ini adalah untuk memberi sambutan yang menenangkan kepada pengguna yang sedang terpikir untuk bercerita dengan psikolog atau psikiater. Dengan rancangan desain baru ini kami juga berupaya menjadikan halodoc sebagai aplikasi harian dengan fitur mood tracker dan artikel harian.
Pertimbangan user dalam memilih psikolog yang menjadi temuan kami diantaranya adalah gender, pengalaman praktik, wajah psikolog dan berapa banyak pengguna yang telah menggunakan layanan psikolog tersebut. Namun sayangnya disini kami hanya menggunakan unsplash untuk mengisi foto psikolognya, jadi apresiasi untuk Halodoc yang telah memiliki standar dalam menampilkan foto partnernya!
Salah satu temuan besar kami adalah user ingin menggunakan metode lain dalam berkonsultasi, seperti call atau video call. Selidik punya selidik, di Halodoc sebenarnya kita memang sudah bisa menggunakan fitur video call loh! adakah yang pernah mencobanya? Coba tulis di komentar yaa!
Selain itu akses untuk melihat jadwal konsultasi yang dimiliki psikolog pun kami tampilkan langsung. Salah satunya karena temuan kami mengatakan user cenderung ingin bercerita ke satu psikolog yang sama saja ketimbang berganti-ganti psikolog. Sehingga jadwal psikolog perlu dapat diketahui lebih mudah untuk membantu user berkonsultasi dengan psikolog favoritnya.
Selanjutnya adalah pembayaran, bagian yang cukup tricky karena ada beberapa penyesuaian terkait dengan fitur baru yang kami ajukan untuk diimplementasikan.
Bagaimana jika user berencana bercerita banyak hal kepada psikolog? Tentu ia membutuhkan waktu lebih banyak. Kami berusaha mewadahi hal tersebut dengan kesempatan untuk memilih durasi konsultasi sesuai kebutuhan user. Sementara frasa Estimasi Biaya dipilih karena user nantinya bisa memperpanjang durasi konsultasinya yang akan dijelaskan kemudian.
Selanjutnya akan masuk sesi konsultasi dengan di dahului ruang tunggu.
Ruang chat ada dengan pertimbangan kemungkinan user perlu melampirkan foto dalam sesi konsultasi atau mengetik suatu hal semisal koneksi tidak stabil atau hendak menginformasikan teks tertentu.
Sementara apabila koneksi bermasalah hingga menyebabkan sambungan terputus maka akan berlanjut ke halaman berikut.
Bagaimana bila user masih di tengah sesi konsultasinya sementara waktu sudah hampir habis? Di sini kami memberikan kesempatan user untuk memperpanjang waktu sesi konsultasinya dengan metode pembayaran sesuai dengan pilihan yang diambil di halaman pembayaran sebelumnya.
Sistem rating psikolog kami ubah menjadi hanya dua, yakni terbantu (jempol keatas) atau tidak (jempol kebawah) hal ini untuk memudahkan informasi seberapa banyak user yang terbantu oleh psikolog tersebut di tampilan card psikolog di halaman pencarian. Sementara rating untuk layanan psikolog berbentuk lima bintang untuk lebih menggali seberapa terbantunya user oleh layanan yang disediakan oleh Halodoc sebagai bahan evaluasi internal aplikasi.
Rancangan ini tentu masih jauh dari kata ideal, salah satunya terutama karena belum di ujicobakan langsung kepada user halodoc itu sendiri. Namun sejauh ini kami mendapat banyak pelajaran berharga seperti bagaimana tim yang solid dan memiliki satu visi bersama dapat menciptakan inovasi dalam memecahkan masalah yang ada. Saya sendiri ditengah prosesnya setelah berguru dan bertanding di innovation battlefield jadi lebih memahami hierarki antar konten, menerapkan panduan dari material.io dan mendapat banyak masukan mengenai desain saya dari grup designjam danAssyifa Narulitasaya jadi merasa selangkah lebih dekat dalam memahami UI Design meski di pengerjaan kali ini masih banyak yang terlewatkan.
Jika kalian ingin melihat prototypenya, silahkan mampir ke laman berikut ya!
Sekian dan terima kasih sudah membaca hingga selesai. Untuk artikel lain bisa dilihat di lamanmedium, jika ingin menyapa bisa mampir keIGataulinkedinsaya. Sampai bertemu di artikel berikutnya!
UX Designer enthusiast",https://miro.medium.com/v2/resize:fit:1000/1*YYMMCYoonDzGI6t3wBgIBg.png,"UX Design, Ux Case Study, UI Design, Medical Apps, Counseling Psychology"
https://medium.com/nusanet/flutter-ui-instagram-desktop-c0ed262b9278,Flutter UI: Instagram Desktop,Yudi Setiawan,2021-05-06T02:00:08.889Z,"Stories and insights from the developers in Nusanet
Menjelang akhir tahun 2020 ini saya ingin membahas mengenai Flutter Desktop. Saya lebih tertarik ke desktop karena dulunya saya pernah develop aplikasi desktop pakai J2SE. Dan ini membuat saya penasaran apakah Flutter Desktop mampu memberikan saya kesan pertama yang luar biasa atau biasa-biasa saja. Untuk menjawab pertanyaan tersebut mari kita bahas ditulisan ini ya. Jadi, pada tanggal 20 Desember 2020 tepat tulisan ini saya publikasikan saya lihat Flutter telah mampu untuk melakukan pengembangan untuk aplikasi desktop MacOS, Linux, dan Windows hanya saja belum ready to use di production ya. Info tersebut bisa kamu baca di link berikut ya.
Di tulisan ini nantinya saya akan berusaha menduplikat design UI Instagram Desktop dari Dribble.
Dan mau tahu hasilnya sepertinya apa? Berikut adalah perbandingan antara design UI dan output di Flutter Desktop.
Saya rasa hasilnya benar-benar cukup memuaskan ya. 😃
Sebelum memulai pembuatan projeknya saya ingin memberitahukan diawal bahwa pada tulisan ini yang saya bahas adalah sebatas menduplikasi design UI-nya. Jadi, UI-nya itu belum ada interaksi ataupun event yang bisa diklik. Dan untuk masalah responsive pun tidak akan saya bahas pada tulisan ini. Oleh karena itu, mohon dimaklumi jika ternyata ketika teman-teman menjalankan aplikasinya dan hasilnya tidak sesuai dengan digambar pada tulisan ini mengingat mungkin resolusi
Adapun beberapa persiapan yang perlu kita persiapkan adalah sebagai berikut:
Silakan buat projek di IDE favoritmu dan buat nama projeknyainstagram_desktop.
Selanjutnya kita perlu setup filepubspec.yaml-nya menjadi seperti berikut.
Untuk aset gambar dan font-nya silakan kamu unduh disini. Lalu, kamu ekstrak file tersebut dan masukkan ke dalam projek seperti gambar berikut.
Selanjutnya, jangan lupa jalankan perintah berikut.
Jadi, pada tulisan ini kita hanya menggunakan 1 file saja yaitu filemain.dart. Untuk pertama-tama silakan ubah kode di dalam filemain.dartmenjadi seperti berikut.
Dan berikut adalah outputnya.
Sekarang kita akan buat layout dasarnya. Silakan buka filemain.dartdan ubah kode didalamnya menjadi seperti berikut.
Selanjutnya kita ubah kode di dalamWidgetNavigationDrawermenjadi seperti berikut.
Langkah berikutnya kita akan membuat widget yang menampilkan logo Instagram. Untuk membuatnya kita perlu buat satu fungsi yang bernama_buildWidgetInstagramLogodan isi dengan kode berikut.
Selanjutnya, kita ubah kode todo berikut.
Menjadi seperti berikut.
Berikut adalah perubahan lengkapnya.
Dan outputnya akan menjadi seperti berikut.
Langkah selanjutnya adalah kita akan membuat fungsi yang menampilkan widget foto profil si pengguna. Silakan buat fungsi dengan nama_buildWidgetPhotoProfiledan isi dengan kode berikut.
Dan ubah kode todo berikut.
Menjadi seperti berikut.
Outputnya akan menjadi seperti berikut.
Kemudian kita akan membuat satu widget yang akan menampilkan teks ‘Hello Ditta’. Untuk membuatnya, silakan ubah kode todo berikut.
Menjadi seperti berikut.
Kemudian, kita akan menampilkan teks username-nya si pengguna. Pada kali ini kita contohkan nama username-nya helloditta. Untuk membuatnya silakan kita ubah kode todo berikut.
Menjadi seperti berikut.
Selanjutnya kita akan menampilkan info jumlah post, followers, dan following si pengguna. Silakan buat fungsi dengan nama_buildWidgetInfoPostsFollowersFollowinglalu, isi dengan kode berikut.
Selanjutnya, kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan menampilkan list menu Feed, Explore, Notifications, Direct, IG TV, Stats, dan Settings. Untuk membuatnya kita perlu buat fungsi dengan nama_buildWidgetItemMenudan isikan dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Sampai langkah ini akan saya tunjukkan kode lengkapnya sekarang seperti berikut.
Dan berikut ialah outputnya.
Langkah berikutnya ialah kita akan menampilkan garis horizontal. Untuk membuatnya silakan ubah kode todo berikut.
Menjadi seperti berikut.
Dan widget terakhir yang akan kita buat didalamWidgetNavigationDraweradalah kita akan menampilkan menu Logout. Untuk membuatnya silakan ubah kode todo berikut.
Menjadi seperti berikut.
Sampai di sini akan saya tunjukkan kode lengkapnya agar teman-teman bisa mengikutinya dengan baik.
Sekarang kita akan menuju ke dalamWidgetContent. Silakan kita ubah kode didalam widget tersebut menjadi seperti berikut.
Sekarang kita akan menampilkan widget TextField pencarian. Cara membuatnya adalah kita buat satu fungsi dengan nama_buildWidgetTextFieldSearchdan isikan dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah selanjutnya adalah kita akan membuat fungsi widget yang akan menampilkan icon notification dibagian header. Untuk membuatnya silakan buat fungsi baru dengan nama_buildWidgetIconHeaderNotificationdan isi dengan kode berikut.
Lalu, ubah kode todo berikut.
Menjadi seperti berikut.
Langkah selanjutnya adalah kita akan menampilkan icon direct dibagian header-nya. Caranya adalah kita buat fungsi baru dengan nama_buildWidgetIconHeaderDirectdan isi dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Sekarang kita akan menampilkan satu button dengan teks Add Photo didalamnya. Untuk membuatnya silakan buat fungsi baru dengan nama_buildWidgetButtonAddPhotodan isi dengan kode berikut.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Selanjutnya adalah kita akan menampilkan teks Stories beserta dengan tombol watch all. Untuk membuatnya kita perlu membuat fungsi baru dengan nama_buildWidgetTitleStoriesdan isi dengan kode berikut.
Lalu kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan menampilkan list story pengguna lain secara horizontal. Untuk membuatnya kita perlu membuat class baru dengan namaWidgetStoriesdan isi dengan kode berikut.
Lalu, kita ubah kembali kode todo berikut.
Menjadi seperti berikut.
Langkah berikutnya adalah kita akan membuat fungsi yang bernama_buildWidgetTitleFeedLatestPopulardimana, output dari fungsi tersebut adalah akan menampilkan teks Feed dan filter Latest dengan Popular.
Selanjutnya kita ubah kode todo berikut.
Menjadi seperti berikut.
Langkah terakhir adalah kita akan menampilkan list posts dari pengguna lain menggunakan StaggeredView. Untuk membuatnya silakan buat class baru dengan namaWidgetFeeddan isikan dengan kode berikut.
Jika ada error jangan lupa diimport class-nya ya. Berikut adalah kode yang perlu kita import.
Lalu, kita ubah kode todo berikut.
Menjadi seperti berikut.
Dan berikut adalah output terakhirnya.
Akhirnya selesai juga ya. Jadi, setelah saya lihat hasilnya ternyata Flutter Desktop mampu memberikan kesan pertama yang luar biasa bagi saya. Saya ingat benar bahwa dulu di Java Desktop untuk membuat tampilan seperti UI diatas benar-benar susah loh. Namun, sekarang semakin berkembangnya bahasa pemrograman dan framework maka, untuk membuat UI yang keren pun juga semakin lebih mudah. Untuk source code lengkapnya bisa kamu lihat di Github berikut ya.
Stories and insights from the developers in Nusanet
Product Engineer @nusawork.com| #AndroidDev | #FlutterDev |",https://miro.medium.com/v2/resize:fit:1200/1*9H2NjK6vPIovG19sx76Jow.png,"Flutter, Flutter Widget, UI Design, Flutter Ui, Flutter Desktop"
https://medium.com/@technicalvkv/copyright-free-images-%E0%A4%95%E0%A5%88%E0%A4%B8%E0%A5%87-%E0%A4%A1%E0%A4%BE%E0%A4%89%E0%A4%A8%E0%A4%B2%E0%A5%8B%E0%A4%A1-%E0%A4%95%E0%A4%B0%E0%A5%87-2020-4e2a050cb76d,Copyright Free Images कैसे डाउनलोड करे 2021,Kunvip,2021-04-07T08:51:32.280Z,"halo doston aapka bahut bahut swagat hai doston is post mein main aapko yah bataunga ki aap apne blog ya website ke liye copyright free images Kahan se download kar sakte hain aapko UN website on ke bare mein main is post mein main batayega doston post ko Pura padhen jisse ki aapko is baat ka naraj ho jaega ki aapko free copyright images Kahan se download karni hai to ja shuru karte Hain aaj ki post
doston is website se aap copyrights free images download kar sakte hain ya bahut hi acchi website hai yahan se aap high resolution wali stock images download kar sakte hain aapko jisse images ki jarurat hai aap is per search karke usi mein jisko download kar sakte hain copy acchi aur popular website hai
Is website mein roj lakho copyright free images upload kiye jaate Hain is website main aap images ke sath sath free copyright videos ko bhi download kar sakte hain is website main aap custom size choose karke bhi images aasani se download kar sakte hain
iske alava or dusri website ke bare me jane jinse aap free copy right image download kar payenge
top 10 websites for free copyright images
I am blogger my site ishttps://www.technicalvkv.com",https://miro.medium.com/v2/resize:fit:1200/1*wk2WIXdxiSYrVW1jvZkTLA.png,"Blogging, Technology, Blog, Hindi Blog, Images"
https://medium.com/@adzzie/laravel-8-membuat-restful-api-crud-sederhana-101c8c869d40,Laravel 8 : Membuat RESTful API CRUD Sederhana,Aji Gojali,2021-02-18T14:49:09.502Z,"Assalamu’alaikum Warahmatullahi Wabarakaatuh.
Kali ini kita akan membuat API menggunakan laravel dengan versi yang terbaru (pada saat tulisan ini dibuat), yaitu versi 8. yups ga usah kelamaan intro langsung aja kita kerjakan.
Sebelumnya kalian harus menginstalphp,Databasedancomposerlalu setting environment terlebih dahulu.Saya sendiri menggunakan php versi 7.3 dan menggunakan database mysql. Bagi yang belum menginstal composer kalian bisa liat dilink ini.
Membuat Project— Jika sudah menginstal yang di atas, kita buka command prompt atau cmd atau terminal atau sejenis teman-temannya. Lalu tuliskan script ini.
jika sudah sampai seperti gambar di bawah ini berarti penginstalan atau membuat project laravel sudah selesai.
lalu kita pindah kedirektoriapi_crudyang kita buat tadi. dan hasilnya seperti gambar di bawah ini.
Setting Database— Setelah itu kita setting database yang ada di file.env-nya. sesuaikan settingan database kalian. contohnya seperti di bawah ini.
Membuat Database— Sebelum mulai coding, mari kita buat databasenya terlebih dahulu. Kita balik lagi ke terminal atau cmd. jika kalian menggunakan mysql seperti yang saya guanakan kalian tinggal mengikuti pertintah berikut atau menyesuaikan dengan database kalian.
setelah masuk dalam client database-nya. kita buat databasenya.
Membuat Migrasi— Sekarang saatnya coding. pertama kita buat migration databasenya terlebih dahulu. saya di sini akan membuat table companies.
Setelah tergenerate filenya, kita ubah atau kita tambahkan field atau column pada methodup-nya sesuai kebutuhan, jika kalian lupa atau belum tau type apa yang akan dibuat pada fieldnya, kalian bisa liat dilink ini.
contohnya saya buat seperti ini.
Jika sudah selesai menambahkan fieldnya, mari kita eksekusi dengan perintah ini.
Jika kalian mendapatkanerrorketika migrasi database. Kalian bisa lihat yang ada dilink ini.
Kalau hasilnya success akan seperti ini.
Membuat Model— Sekarang kita buat modelnya. kalian ketikan seperti script dibawah ini di cli atau terminal kalian.
Setelah tergenerate modelnya kita tambahkan code seperti di bawah ini.
Membuat Controller— Untuk membuat controllernya kalian bisa ikuti sctrip di bawah ini.
Untuk penjelasannya di bawah ini.
Di dalam controller sudah terbuat method-method-nya sepertiindex,store,show,update,destroy. sekarang kita tinggal isi script sesuai kebutuhan kita.
Method index— Method ini digunakan untuk menampilkan data yang berbentuk list. contohnya seperti dibawah ini.
Method store— Method ini digunakan untuk menyimpan data. dan ini contoh codenya.
Method show— Method ini digunakan untuk menampilkan data dalam bentuk satu data. contoh codenya.
Method update— Method ini digunakan untuk mengupdate data. bisa kalian ikuti codenya di bawah ini.
Method destroy— Method ini digunakan untuk menghapus salah satu data. dan ini contoh codenya.
Setelah selesai membuat controller kita akan membuat routes.
Membuat Route— Route digunakan untuk mendaftarkan atau mendefinisikan suatu alamat yang biasa kita sebut URI (Uniform Resource Identifier) ke dalam file route. Di dalam direktori routes terdapat beberapa file route, salah satunya file api.php yang akan kita gunakan untuk mendefinisikan uri apinya. kalian tambahkan code berikut kedalam file api route tersebut.
Kalian bisa lihat list route yang sudah terdaftar di dalam aplikasi kita dengan menggunakan script dibawah ini pada cli atau terminal.
Gambar di atas adalah hasil dari route yang kita definisikan, karena kita mendefinisikan routeapiResource, maka route akan otomastis membuat route seperti pada gambar di atas. selain menggunakan routeapiResourcekalian juga bisa mendefinisikan route secara manual. contohnya seperti di bawah ini.
Code route tersebut akan menghasilkan route yang sama seperti pada gambar route list di atas.
Lihat hasil— Untuk melihat hasil yang kita buat, kita membutuhkan tools seperti postman, rested atau yang lainnya. di sini kita akan mencoba memakai postman. dan di bawah ini adalah hasilnya.
Terima kasih, salam Programmer.
Wasalamu’alaikum Warahmatullahi Wabarakaatuh.",https://miro.medium.com/v2/resize:fit:1200/1*5uBsg8qj0UkdojEOpOntNQ.png,"Restful, Postman, Laravel, API, Crud"
https://medium.com/@teddykoerniadi/basic-crud-api-on-laravel-8-3092b7a79c29,Basic CRUD API di Laravel 8,Teddy,2020-12-25T03:35:21.456Z,"Dalam dunia software development mungkin sudah tidak asing lagi di telinga kalian sebuah istilah yang satu ini yaitu API atau Application Programming Interface. Kali ini kita akan coba membuat API Create, Read, Update, dan Delete di salah satu framework php yaitu Laravel. Yang perlu kita siapkan adalah:
Oke kita langsung coding
Code di atas adalah contoh dasar routing api pada 1 modul, disini kita coba buat module product. ada 5 route:
disebelahnya di dalam kurung siku adalah controller dan function yang akan di jalankan ketika routenya terpanggil.
3. Buat modul (model, controller, migration), disini kita buat memakai perintah di terminal ya, coba jalankan di terminal dan pastikan posisinya di folder project kalian.
perintah di atas akan mengenerate model, factory, migration, seeder, dan controller. Karena diakhir perintahnya kita tambahkan -a yang artinya all jadi ter-generate-lah semuanya tadi dalam satu perintah. Ini akan membantu kita dalam naming modul nya karena seperti di dalam migrationnya sudah tergenerate nama table ‘products’, model sudah menggunakan table ‘products’, dan controller sudah menggunakan model ‘Product’. EMEIZING-kan!!!
4. Silahkan edit product migration sesuai dengan kebutuhan kita. karenakita disini coba-coba. Jadi kita akan tambahkan saja kolom name, price, dan stock. lalu jalankan di terminal “php artisan migrate”
5. Edit app/models/Product.php, tambahkan variable fillable yang diisi dengan semua nama kolom yang kita ingin bisa masukan datanya lewat aplikasi kita, contoh code lihat di bawah.
6. Edit app/Http/Controllers/ProductController.php. Ketika kita buka ProductController pasti sudah ada template function yang sudah dibuat. kita hanya akan edit function index, show, store, update, dan delete. Karena dasarnya function itu yang sering dipakai untuk api. Lihat code di bawah.
untuk function lainnya dibiarkan saja atau juga boleh di hapus karena kita tidak pakai.
Oke sekarang sudah selesai kita coba di postman hit ke 5 endpointnya. Jangan lupa jalankan dulu servernya ya, jalankan di terminal “php artisan serve”, jika server berjalan dengan lancar. READY FOR TESTING
Ketika semua test case telah berhasil, maka percobaan api kita berhasil.Sekia Basic CRUD API on Laravel 8, terima kasih semoga bermanfaat.
Full-Stack developer. Currently is javascript instructor.",https://miro.medium.com/v2/da:true/resize:fit:887/0*En1OqgjqVhLwOGTU,"Laravel, API, Laravel Framework, Crud, Eloquent"
https://medium.com/@azharogi/cara-handle-notifikasi-midtrans-pada-api-coreapi-ep3-50861ad4e7a6,Cara Handle Notifikasi Midtrans pada API —CoreAPI #ep3,Azhar Ogi,2020-12-10T13:27:48.855Z,"Hallo brosis, sesuai yang di janjikan nih kita lanjut ke episode 3 untuk Handle notification. Sebenarnya ini simple saja ya brosis.
Masih Bingung ? Perkenalan dulu, baca Disini
Jika brosis lebih tertarik dengan metode video, brosis bisa langsung akses video dibawah ini yah.
Btw karna disini kita masih menggunakan localhost, maka kita menggunakan simulasi menggunakan webhook.
Tahap 1 : Request Checkout
Untuk brosis yang belum paham Checkout menggunakan Core API Midtrans brosis bisa baca dulu diEp1&Ep2
Tahap 2 : Handle Notification
Kali ini kita menyiapkan table terlebih dahulu yah guys. Siapkan dulu saja schemanya sesuai dengan database yang kalian gunakan, disini kita menggunakan MySQL. Buat 1 migration pada lumen kalian.
Masukan syntax ini ke dalam file migration kalian pada function up()
Sehingga menghasilkan struktur table seperti dibawah ini
Jika sudah jangan lupa buat 1 class model untuk table orders ya
Kita buat 1 class controller baru dengan nama NotificationController dan buat 1 function di dalamnya yaitu post() , brosis bisa lihat syntaxnya dibawah ini :
Penting :Notifikasi yang dikirim oleh midtrans memiliki format JSON, sehingga brosis harus melakukan decode pada json yang diterima oleh API ini.
Jangan lupa untuk routingnya
Lanjut, brosis modifikasi class PaymentController agar menyimpan data checkoutnya ke dalam sebuah table orders
(tambah) function buyProduct
(modifikasi) function bankTransfer
(modifikasi) function chargeCreditCard
Jika sudah maka jangan lupa di serve dulu
Kita buka webhook :https://webhook.site/
Kita copy unique url
Oke sekarang kita buka akun midtrans > settings > configuration, dan paste unique url ke kolom Payment Notification URL.
Kita uji coba, lakukan checkout terlebih dahulu menggunakan bank transfer
Brosis bisa cek data pada table orders
Brosis bisa cek notification history / log ke :https://dashboard.sandbox.midtrans.com/settings/vtweb_configuration/history
Jika statusnya complete, kalian bisa cek di webhook kalian
Brosis bisa copy va_number pada json di column raw content
Untuk simulasi pembayarannya kalian bisa buka :https://simulator.sandbox.midtrans.com/bca/va/index
Jika berhasil maka brosis akan mendapatkan payment confirmation seperti dibawah ini.
Dan jangan lupa klik pay.
Brosis buka kembali webhook kalian dan wala… ada notifikasi success
Sekarang, brosis copy json pada raw content.
Kita sekarang test json request menggunakan Postman,
Jangan lupa send, jika status 200 OK maka kalian bisa cek data kalia pada table orders. Jika status berubah sukses maka kalian sudah berhasil sampai sini
Oke, sekian dulu dan semoga bermanfaat. Jika brosis merasa bingung atau mengalami error, bisa chat aja langsung ke socmed ya.
Jika kalian suka silahkan di clap ya :D
PaymentGateway — CoreAPI Series
Social Media & Channel
Youtube :CLASSIC IMPLEMENTS
facebook: zharlet30 ,linkedin:https://www.linkedin.com/in/azhar-ogi-2bb408112/,gitlab: @zharlet ,line: zharlet .
Engineer with 5+ years of experience in web development, backend, APIs, and cloud infrastructure. Tech enthusiast with a strong Soft Skills & Hard Skills",https://miro.medium.com/v2/resize:fit:1002/1*LVOK9XskY5tNW-dFhDr1cg.png,"Payment Gateway, Rest Api, Laravel, Lumen, Backend"
https://medium.com/@gusjul/react-native-firebase-authentication-b28aa301e3af,[React Native] Firebase Authentication,Gus Jul,2020-12-03T08:14:46.592Z,"Firebase merupakkan salah satu realtime database yang berbasis cloud yang dikembangkan oleh google, kali ini kita akan menghubungkan react native dengan firebase
Langsung kita mulai saja untuk mengauthentifikasi firebase dengan react native, kali in kita akan menggunakan package react-native firebase, yang merupakan package yang sudah dikostum dan disesuaikan untuk native app
Buat sebuah projek baru dengan perintah berikut :
Pergi ke lamanfirebasekemudian klik konsole yang berada di bagian pojok kanan atas kemudian akan tiba di halaman konsole firebase
Buat projek pada konsole firebase dengan klik ikon “add projek” sehingga nantinya akan diberikan form untuk melanjutkan projek, kemudian isi nama projek, setelah itu untuk google analytic bisa di turn off atau on, selanjutnya mengkonfigurasi google analytic (untuk yang turn on google analytic)
Tambahkan packcage agar lebih mudah menggunakan firebase untuk dokumentasi lengkap dapat dilihatdisini, atau dapat diinstal pada projek kita menggunakan perintah berikut :
Setelah projek selesai dibuat, tambahkan aplikasi ke firebase, dengan memilih antara aplikasi IOS, Android, ataupun Web. Khusus saat ini kita akan menambahkan aplikasi android.
Setelah itu isikan nama aplikasi anda, nama ini terdapat pada bundle projek yang sudah dibuat. Dapat dicari pada folder projek yang sudah dibuat/android/app/src/main/AndroidManifest.xml
kemudian untuk app nickname anda bisa membuatnya secara bebas, dan untuk SHA-1 bersifat optional jika punya dapat ditambahkan, namun jika tidak bisa dikosongkan kemudian klik “register app”
Download file yang diberikan kemudian pindahkan tersebut ke folder projek anda tepatnya pada /android/app/google-services.json.
Tambahkan Firebase SDK sesuai dengan petunjuk yang tersedia. SDK yang ditambahkan berada pada /android/build.gradle, kemudian untuk langkah dibawahnya tidak perlu dan lanjut ke langkah berikutnya
Seperti yang terdapat pada dokumentasi react-native-firebase tambahkan line berikut pada /android/app/build.gradle
Install auth pada react-native-firebase menggunakan perintah sebagai berikut :
Selamat anda sudah menghubungakan projek react-native dengan firebase
Pertama kita buat stack screen dimana akan menjadi navigasi antar halaman kita, selain itu penting diketahui bahwa kita harus membuat komponen seperti gambar projek diatas. Berikut merupakan file App.js yang kita jadikan sebagai stackscreen
Ikuti langkah-langkah dibawah ini
Setelah itu kita lanjutkan untuk develop signup.js dimana kita akan membuat tampilan sebagai berikut, dengan menginputkan program seperti dibawah ini
Untuk signup sendiri kita memerlukan fungsi untuk dapat terdaftar sebagai pengguna pada firebase dengan menambahkan function berikut :
Hal tersebut akan dapat membuat kita mendaftar dan terautentifikasi pada aplikasi yang dibuat
Setelah itu kita akan mendevelop login pada login.js, setelah membuat akun kita arahkan kembali ke login screen setelah itu user harus login menggunakan akun yang sudah didaftarkan tadi berikut ini perintah login.js
Selanjutnya kita harus menambahkan fungsi login agar dapat masuk pada aplikasi yang kita gunakan, dengan perintah berikut :
Dengan itu kita dapat login pada aplikasi kita menggunakan akun yang sudah dibuat tadi pada halaman signup
Dan yang terakhir adalah log out, logout disini adalah fungsi untuk user keluar dari sistem. fungsi logout lebih simpel daripada fungsi lainnya. Berikut merupakan perintah pada homescreen.js
Tambahkan function untuk logout, agar user dapat keluar dengan perintah berikut :
Horee, sekarang anda sudah membuat semua komponen dan dapat dites untuk firebase.
Sekarang saatnya kita melakukan testing apakah sudah semua fungsi berjalan baik atau tidak. Silahkan buka firebase konsole pada projek yang telah dibuat tadi
Yey kita sudah berhasil menghubungkan firebase dengan react-native dan juga membuat firebase authentication menggunakan email pada halaman signup, login dan juga homescreen untuk logoutnya.
Untuk keseluruhan code dapat dilihat seperti di bawah ini :
tapi jangan di copy paste aja ya, pastikan ikuti langkah dari awal, karena kalau eror lama untuk deteksinya :D
Untuk yang gemar UX, ada sedikit hal yang dapat ditambahkan yaitu pada Textinput baik pada saat memasukkan email di login dan juga signup. Dimana keyboard akan berisi tanda “@” dan juga tidak otomatis huruf besar pada awal pengetikkan. dengan menambahkan code berikut
Mungkin sekian untuk toturial menghubungkan react native dengan firebase menggunakan package react-native-firebase. sampai jumpa di toturial selanjutnya",https://miro.medium.com/v2/resize:fit:1200/1*LPJL9qcf4SVNFbZg1vi2WQ.png,"React Native Firebase, Mobile App Development, Firebase, React, React Native"
https://medium.com/@utpalblog/youtube-se-paise-kaise-kamaye-uski-puri-jankari-97e6b2198d40,Youtube Se Paise Kaise Kamaye Uski Puri Jankari,Utpal konwar,2020-12-26T09:31:16.025Z,"Hi friends aaj me aapko batauga kiyoutube se paise kaise kamaye. Dekha jaye toh india me bahut sare log hai jo sirf youtube ki kamai par apne ghar chala raha hai. Youtube ek aisa platform hai jaha par hum apne video dalke apne ghar me hi paise kama sakte hai, jab se jio network aaya hai tab se youtube me itne sare youtuber aagaye hai jo jyadatar tech video hi apne YouTube channel me upload karte hai.
Agar aap bhi youtube se paise kamana chahte hai to aap is post ko jarur padhe, kiyoki is post me aapko youtube ke bare me puri jankari denge. Bahut sare log hai jo youtube ko full time career ki hisab se le rahe hai. Agar aap apne mehnat yotube me kuch mahine lagate hai toh aap bhi ek successful youtuber bon sakte hai. Me bhi youtube me video dalta hu lekin me apne blog ke upar jyada focus karta hu.
Youtbe se paise kamane keliye aapke channel par 1000 subscriber aur 4000 watch time hone chahiye. Jiske channel par 1000 subscriber aur 4000 watch time ho jate hai, unke channel ko monetize karke paise kama sakte hai. Aapke video ko monetize karne keliye aapke paas ek google adsense account hona chahiye. Agar aapke pass google adsense account nahi hai toh aap bahut he easy apply kar sakte hai.
Aapke videos ko monetize karne ke baad aapke videos par ads show hoga. uske baad aapke videos par jitney views milenge aapko utnahi paise income kar sakenge. Aur aapke adsense account mei 100$ hone ke baad woh paise aapke bank account me withdraw kar sakte hai.
Agr app bhi youtube se paise kamana chahte hai to pahle aapko youtube par ek channel create karna hoga. uske baad apne channel keliye video shoot karke publish kare. Apne youtube channel keliye aap koi bhi categories ke video bana sakte hai example- funy, tech, sport, blogging, internet etc.. matlab aapko jis categories ke knowledge hai aap usi categories ke video banaye. Pahle aapke channel ke videos par views aane par kuch time log sakte hai kiyoki aapke youtube channel par subscriber nahi honge. Lekin jab aapke channel par subscriber badhte jayenge aapke views bhi jyada honge.
Ek baat aapko mei clear kar dena chahta hu ki aap jo bhi content aapke channel par publish karnege woh video unique hona chahiye. Bahut sare log hai jo youtube se video download karke usko thora sa edit karke firse apne channel par publish karte hai. Aisa videos youtube policy ke bahor hai aise karne par aapke channel aaj nahi to kaal band ho sakta hai. Isiliye aap sirf aapne original content he publish kare.
1.Computer Knowladge :Agr aapke pass computer hai to aapke videos ko editing karne me easy hoga. waise dekha jaye toh india me hajaro log apne mobile par hi apne videos ko edit karte hai.
2.Basic internet knowladge :Dekha jaye to bahut kom logo ke pass internet ka knowledge hote hai. maine bahut sare logo ko dekha hai ki woh sirf facebook, whatsapp, youtube he dekhte hai. Usse jyada unlogo ke pass internet ka jyada knowledge nahi hota. Agar aap youtube par kaam start karna chahte hai to aapko internet ka knowledge hona jaruri hai.
3.Video Editing :Ek successful youtuber bane keliye aapko thora kuch video editing ka knowledge hona chahiye. Taki aap jo video apne youtube channel keliye bana rahe hai woh video dekhne me moja aaye. Internet par aapko bahut sare free aur paid video editing software milega aap free software ko thora din keliye use kar sakte hai.
4.Quality Video :Aap apne youtube channel keliye jo bhi video banante hai us video ka quality achhi honi chahiye tabhi aapke viewer aapke videos ko dekhenge. Agr aapke video quality acchi nahi hai to aapke viewers aapke videos ko ignore kar denge. Aur duwara aapke channel par visit nahi karenge.
5.Your Voice :Agar aap apne video me aapke voice use karte hai to aapke voice clear hona chahiye, taki logoko aap kiya bol rahe hai woh easy samajah me aaye. Apne youtube me dekha hoga ki bahut sare youtuber ke voice clear nahi hota aise video me views bhi jyada nahi aate.
To dosto youtube se paise kaise kamaye aapko puri jankari maine diya hai. I hope ki mera ye post aapko achha lga aur bhi jyada janakri keliye aap hamare comment box par comment kijiye.
My name is Utpal and I am a blogger. I love writing articles in this media platform, if you liked my article then follow me. Thank you",,Make Money Online
https://medium.com/@ryansombreros/contoh-kalimat-menulis-e-mail-dengan-bahasa-inggris-dalam-berbagai-situasi-4d78b6eb51b1,Contoh kalimat menulis E-Mail dengan bahasa Inggris dalam berbagai situasi,Rian,2020-12-17T03:36:46.222Z,"Buat kalian yang bekerja memakai Email dengan menggunakan bahasa Inggris namun masih merasa bahasa Inggrisnya belum bagus atau belum PD. Berikut berbagai contoh kalimat dalam bahasa Inggris dengan berbagai situasi:
Dalam situasi Mengekspresikan terima kasih:
Dalam situasi menunggu tanggapan:
Dalam situasi membutuhkan bantuan:
Saran waktu dan tempat:
1.Could we meet on Monday in the afternoon at 13:00 pm?
Balasan tidak bisa ikut:
Balasan mengubah jadwal meeting/rapat:
Balasan bisa ikut meeting/rapat:
Balasan menolak meeting/rapat:
Memberitahu darimana anda mendapat kontak:
We met last Thursday on your stand at the Jakarta Trade Fair. I am emailing you off your website, which I found through Goggle.
Alasan anda mengirim email:
We are a manufacturer/supplier/provider of…… We are interested in… We are a Japanese Company exporting to the EU and we need…
Permintaan yang akan diminta:
We would be grateful for some information about… Please send us information about your product range and prices.
Permintaan yang lebih spesifik:
In particular, please send full details of your prices, discounts, terms of payment, and delivery times. Could you also say whether there is any minimum order?
I look forward to an early reply and I’m sure that there is a market for your products here in Jakarta.
Ucapan terima kasih terlebih dahulu
Thank you for your email of 4 December inquiring about…..
Memberikan informasi yang dibutuhkan
We can quote a price of… CIF/FOB Instanbul. We can deliver by…(date)/within…. (period of time). The goods will be shipped 3 days from receipt of a firm order. We can offer a discount of… om orders over…
We require payment by bank transfer/letter of credit. Our normal procedure is to… Our normal terms for first-time customers are… We can supply the items you require directly from stock.
May I suggest that I call you at your convenience to discuss the matter further? If you need any further information, please do not hesitate to contact me. My direct line is….
I am writing to complain about the quality of a product I bought from your website/company.
Our order date 16 December clearly stated that we wanted 1,000 items, however, the goods were faulty/damaged/in poor condition.
Permintaan tanggung jawab:
Please replace the faulty goods as soon as possible. We must insist on an immediate replacement/full refund. Unless I receive the goods by the end of this week, I will have no choice but to cancel my order.
I hope that you will deal with this matter promptly as it is causing me considerable incovenience.
Sebenarnya masih banyak lagi situasi urusan kerjaan menggunakan email dalam bahasa Inggris. Saya akan tambahkan apabila ada waktu luang kembali. Terima kasih
Just Do It - Impossible Is Nothing",,"Email, Kalimat, Bahasa Inggris, Contoh, Pekerjaan"
https://medium.com/@bobbyzarkasih/understanding-universe-in-your-free-time-6c82bdebdb48,Understanding Universe in your free time,Bobby Zarkasih,2020-12-27T11:29:29.000Z,"My thoughts on “Astrophysics for People In a Hurry” by Neil Degrasse Tyson
Okay, where to start?
Gw akan mulai dengan bilang, waktu kecil cita-cita gw adalah menjadi seorang Astronom. Ya sedari kecil gw udah seneng banget baca buku-buku tentang antariksa, gw punya puluhan koleksi buku dan ensiklopedia yang membahas mulai dari bumi, bulan, bintang, dan tata surya. Alesannya saat itu simple,it’s cool
Salah satu konsep favorit gw adalah soal betapa jauhnya bintang-bintang yg kita lihat di langit. saking jauhnya sampai cahaya dari mereka butuh waktu jutaan tahun untuk sampai ke bumi(dalam 1 detik, cahaya bisa bergerak sejauh 300.000 km. itung dah tuh kalo jutaan tahun). yang berarti saat kita melihat bintang di langit, kita sedang melihat masa lalu, kita sedang melihat sisa-sisa cahaya yang dikirimkan oleh bintang itu jutaan tahun yang lalu, dan bintang yang kita sedang kita lihat mungkin saat ini sudah tidak ada lagi. isn’t that cool?
Hal-hal tentang betapa kerennya alam semesta ini yang bikin gw jatuh cinta dengan astronomi. But in highschool i give up on my dream to be an astronomer, because in high school i hate physics so much haha. Setelah itu gw memutuskan untuk kuliah DKV dan ujung-ujungnya sekarang meniti karir di Film. agak random emang wkwk.
Bertahun-tahun tidak tersentuh dengan dunia astronomi lagi, di tahun 2019 gw lihat ada buku “Astrofisika untuk Orang Sibuk” di gramedia. Dari judulnya yang terdengar norak (karena ini terjemahan) gw kira ini buku abal-abal tulisan penulis yang tidak jelas asal-usul dan kredibilitasnya, sejenis buku-buku cara cepat menjadi kaya yang banyak banget di obral di gramedia. Tapi gw lihat penulisnya Neil deGrasse Tyson,well, i’ve heard this guy, this guy is cool.Setelah gw googling ternyata buku ini adalah terjemahan dari buku “Astrophysics for People in a Hurry” yang kalau kata review membahas alam semesta secara singkat dan mudah dipahami. Dan akhirnya gw pun beli bukunya… di 2020, karena waktu itu ga bawa uang dan besoknya lupa.
Impresi pertama saat membaca buku ini, gw dibawa kembali ke masa kecil gw. Persis kaya Anton Ego di film Ratatouille waktu makan… well.. ratatouille. gw dibawa kembali mengingat kenapa gw dulu bisa suka banget tentang alam semesta. Dan gw langsung men-chargekembalicuriositygw tentang dunia astronomi.
Oke ini sangat amat personal, tapi secara objektif gw akan bilang
Buku ini terdiri dari 12 bab, yang ngebahas mulai dari Big Bang,konsep-konsep rumit kayakDark Matter &Dark Energy,planet dan galaksi kita, bahkan kita diajakrevisittabel periodik, untuk kenal unsur-unsur yang ada di alam semesta, dan ditutup dengan perenungan akancosmic perspective
Di halaman pertama buku jelas tertulis
For all those who are too busy to read fat books — Yet nonetheless seek a conduit to the cosmos.
ya buku ini sangat tipis, versi bahasa indonesianya cuman 146 halaman. tapi apakah buku ini cukup untuk memahami semua tentang alam semesta kita?
Jawabannya, gak juga.
Gini, buku ini bukan buku “Cara cepat memahami Astrofisika, 2 minggu dijamin langsung paham”.Astrofisika adalah bidang yang sangat rumit, butuh puluhan tahun mungkin untuk bisa memahami bagaimana alam semesta bekerja. Buku ini gak akan menjelaskan itu semua, dia gak akan menjelaskan secara detail rumus-rumus, argumen-argumen yang digunakan oleh para astrofisikawan. Tapi buku ini akan membantu me-simplifiedkonsep-konsep tersebut agar lebih mudah kita cerna. Buat gw pribadi, buku ini setidaknya akan membuat kita paham, apasih yang bikin para astrofisikawan ini tertarik dengan alam semesta. Kita dibawa masuk ke bagian kulit dari ilmu Astrofisika, pertanyaannya kemudian apakah cukup? kalau kamu ingin jadi seorang astrofisikawan, jelas enggak. tapi kalau kamu mau setidaknya memahami bagaimana alam semesta bekerja dengan cara yang mudah dicerna, maka buku ini cukup.
Gw tetep merekomendasikan kalian untuk baca buku “Origins” yang ditulis juga sama NDT sebagai lanjutannya. Disana bakal ada penjelasan yang lebih panjang soal alam semesta, tapi tetep cukup mudah untuk dipahami.
Menurut gw Neil DeGrasse Tyson mungkin bukanAstrophysicistpaling jenius di dunia saat ini, yang mungkin akan menemukan teori-teori baru soal alam semesta. Tapi dia adalahAstrophysicistyang cukup jenius untuk bisa menjelaskan konsep yang sangat rumit, menjadi sesuatu yang bisa dicerna oleh orang awam seperti kita, yang mungkin ga lebih cerdas dari anak SD.Dan ini yang sangat dibutuhkan oleh ilmuwan di masa sekarang,sciencebukan lah benda yang eksklusif dan hanya akan dibicarakan oleh orang-orang cerdas.Scienceadalah cara bagaimana kita memahami apa yang terjadi di sekitar kita, dan ini harus juga bisa diakses oleh orang-orang yang tidak cukup cerdas.
Sekarang gini, kenapa orang-orang masih banyak yang ga patuh dengan protokol COVID-19, masih banyak yang gamau di vaksin, dll sebagainya. Mungkin sebagian ada yg memang egois atau tidak ada pilihan lain. Tapi sebagian lainnya mungkin bahkan gak paham apa yang para ilmuwan dan pemerintah udah jelasin. Ilmuwan kita yang pinter banyak, banyak banget, tapi ilmuwan yang bisa menjelaskan itu ke orang awam? gak banyak.
Prof. Herawati Sudoyo pun mengamini hal ini dalam interviewnya di Endgame
Sekali lagi, menurut gwScientistga bisa jadi kayak broker yang seneng banget pake bahasa dan istilah ribet buat jelasin gimana main saham cuman supaya mereka kelihatan eksklusif dan mereka kelihatan paling paham soal saham. Karena gak ada ruginya sama sekali jikasciencebisa diakses oleh semua orang.
Oke, gw udah mulai ngalor ngidul nih. balik ke bukunya Neil DeGrasse Tyson
Buku ini ditutup dengan bab yg membahas soalcosmic perspective, sebagai manusia kita punya ego yang sangat besar, kita selalu merasa kita adalah spesies paling spesial di alam semesta.cosmic perspective will lowering your ego by explaining how insignificant we are in this vast universe. Gimana caranya? gw ga akan masukin semua yang ada di buku kesini, tapi ini yang paling menarik buat gw.
Kita punya hidup yang sangat amat penting untuk kita di bumi,
Bumi yang kita tinggali ini sangat amat luas,
Planet kita tercinta ini mengitari sebuah bintang yg kita sebut matahari, yang merupakan bintang yang biasa-biasa aja,
Matahari kita ada di sebuah galaksi bernama bimasakti yang didalamnya ada ratusan milyar bintang,
Dan galaksi kita ini adalah bagian dari 50 sampai 100 milyar galaksi lain di alam semesta,
Alam semesta yang sangat amat luas ini sudah ada sejak milyaran tahun lalu dan diprediksi akan tetap ada milyaran tahun kedepan,
Umat manusia hidup hanya sekitar 200ribu tahun yang lalu
We are really really really tiny and insignificant, the universe doesn’t give a damn about us.
Gw mau menutup tulisan gw dengan salah satu “puisi” favorit gw, judulnya “Pale Blue Dot”. Tahun 1990, wahana Voyager 1 akan meninggalkan tata surya. NASA memerintahkan untuk memutar kamera Voyager 1 dan mengambil gambar bumi ditengah luasnya alam semesta.
Look again at that dot. That’s here. That’s home. That’s us. On it, everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every “superstar,” every “supreme leader,” every saint and sinner in the history of our species lived there — on a mote of dust suspended in a sunbeam.
The Earth is a very small stage in a vast cosmic arena. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds.
Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.
The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.
It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we’ve ever known.",https://miro.medium.com/v2/resize:fit:1200/1*4tdN7kdiZWGAqWjyWM5Glw.jpeg,"Book Review, Neil DeGrasse Tyson, Indonesia, Astrophysics"
https://medium.com/@nasriadzlani/rest-api-dengan-fastapi-dan-elasticsearch-93f035b5f05,Rest API dengan FastAPI dan Elasticsearch,Nasri Adzlani,2020-12-21T07:19:27.714Z,"FastAPI adalah sebuah framework python yang digunakan untuk membangun API(). FastAPI merupakan salah satu framework python yang memiliki performa bagus dalam membangun API. Bahkan lebih dari itu, dalam documentasinya yang bisa kita lihat disini, FastAPI memiliki banyak kelebihan lain.
Elasticsearch adalah salah satudatabaseyang masuk ke dunia NoSQL dengan fokus disearch engine database. Elasticsearch ditenagai oleh Apache Lucene yang juga merupakansearch engine databaseyang memilikiquery low level. Elasticsearch memilikiqueryyang lebih mudah untuk digunakan karena berbasis RESTful.
bahan yang dibutuhkan antara lain
🔹 pip install fastapi uvicorn
🔹 pip install fastapi
🔹 pip install elasticsearch
Jika kedua library di atas sudah terinstall dengan benar, saatnya kita mencoba membuat API. Kita buat sebuah file dengan nama main.py,
import yang dibutuhkan
Deklarasi yang dilakukan
Macam fungsi di FastAPI
tukang ketik kode. //https://nasriadz.tech/",https://miro.medium.com/v2/resize:fit:1023/1*du7p50wS_fIsaC_lR18qsg.png,"Elasticsearch, Fastapi, Rest Api, Rest, Python"
https://medium.com/@mnaufala/menggabungkan-beberapa-database-dengan-menggunakan-psql-logical-replication-b99db0ee3d3,Menggabungkan Beberapa Database dengan Menggunakan PSQL Logical Replication,Muhammad Naufal Abiyyu,2021-04-26T16:10:55.447Z,"Saat kita migrasi dari arsitekturmonolithkemicro services, kita akan mengubah cara kita menyimpan data. Saatmonolithkita hanya memiliki satudatabase, maka dimicro serviceskita akan punya banyakdatabasekarena setiapserviceakan memilikidatabasesendiri-sendiri. Dari sini muncul masalah baru yaitu saat kita ingin analisis data dan memerlukan operasijoinketablelain, namuntabletersebut berada didatabaselain.
Dari permasalahan diatas, berbagai solusi muncul untuk mengatasi permasalahan tersebut. Salah satu solusinya dengan cara memanfaatkan fiturlogical replicationdari postgresql untuk menggabukandatabase. Fitur tersebut memungkinkan kita untuk mereplika data pada sebuahdatabasesatu kedatabaselain.
Ok, langsung saja kita simulasikan fiturlogical replicationuntuk menggabungkandatabase. Untuk mempermudah simulasi kita akan menggunakan 3postgres(2db_masterdan1db_replicasebagai aggregator) yang kitainstalldidalamdocker container.
2. Langkah selanjutnya yaitu jalankanfiledocker-compose berikut:
3. Setelah db berhasil hidup, buattabledidb_memberdandb_merchantlalu masukkan beberapa datasample, misalkan kita buattable users(db_member) dantable merchant(db_merchant) denganschemaseperti berikut:
4. Langkah berikutnya yaitu membuat publication didb_memberdandb_merchant.Publicationadalah mekanismepostgresagar dapat mereplikasitable. Untuk membuatpublication, jalankanquerydibawah ini:
5. Terakhir yaitu kita buattable usersdanmerchantsdidb_replica.Setelahtableberhasil dibuat lalu buatsubscriptionsuntuk menghubungkanpublicationyang telah kita buat ke db_replica. Berikutqueryyang dijalankan:
Jika berhasil maka akan mendapatkanoutputsebagai berikut:
6. Lakukan testing dengan memasukkan data daridb_memberdandb_merchant. Nantinya secara otomatis data yang kita masukkan akan muncul juga didb_replica.",https://miro.medium.com/v2/resize:fit:451/1*Y5m694V1i1omr1l_B4RSWA.jpeg,"Database, Data Engineering, Microservices, Data Warehouse, Postgres"
https://medium.com/@sijjin/fix-wifi-ubuntu-20-04-rtl8188eu-4058ee7fb096,Fix wifi Ubuntu 20.04 ‘rtl8188eu’,ジブリル,2021-08-16T10:15:25.399Z,"Assalamualaikum warahmatullahi wabarakatuSalam sejahtera bagi kita semuaShamlomOm SwastyastuNamo BuddhayaSalam Kebajikan
Hello, lama kali ngga buat story di medium. Jadi intinya wifi pada ubuntu 20.04 yang awalnya normal, namun seteleah didist-upgradejadi tidak bisa konek dan selaludisconnect. Sebelum di upgrade lancar-lancar saja, saya akan membagikan “cara saya” mengatasi hal tsb.
Output file, tidak ada error:
Output file, tidak ada error:
Dan hasilnya seperti gambar dibawah
Sekian dari saya, semoga bermanfaat. Salam rahayu.
Eat - Sleep - Profit - Repeat",https://miro.medium.com/v2/resize:fit:1200/1*3P2tGLvCZwNlzuMHVsriuA.png,"Wifi, Linux, Ubuntu 20 04, Linux Tutorial, Ubuntu"
https://medium.com/@imnitishverma/bitcoin-mining-guide-bitcoin-mining-kya-hai-6a9006b6d429,Bitcoin Mining Guide : Bitcoin Mining Kya Hai,Nitish Verma,2020-04-22T11:20:05.022Z,"Aaj Bitcoin ka chalan bahut hi teji se world me hi nai india me bhi popular ho raha hai. Bitcoin ke bare me aaj bhi logon ke paas utni jaankari nai hai India me. Lekin ye ek Bahut popular Virtulal Currency hai. Main ish Post me Bitcoin Mining Guide dene jaa raha hun. Ki Bitcoin Mining Kya Hai
Bitcoin Minning ko janne se pehle aapko ye janna jaruri hai aakhirBitcoin Kya Hai?Aap Ynaha Click karke padh sakte hain. To aaiye Jante hain Bitcoin Mining ke bare me.
Bitcoin Mining Ko Profitable dhang se karna Thoda kathin hai. Lekin ish post ko padh kar aap ek acche bitcoin Minner ban sakte hain. Sabse Pehle ki aap Bitcoin Mining Suru karein samjh lena jaruri hai ki bitcoin Mining ka matlab kya hai. Bitcoin Mining legal hai aur SHA256 double round hash verification process ke sampann ki jati hai. Taaki Bitcoin Transactions ko proof kiya jaa sake aur bitcoin network ke Sarvjanik ledger ko aavshayak security pradaan ki jaa sake. Jis Raftaar se aap Bitcoin Mine karte hain usey Hash/PerSecond (Hash Per Second ) me mapa jaata hai.
Bitcoin Network Bitcoin Miners dwara aavshyak Computational Power ka yogdaan karne ke Pryaas ke badle Bitcoins ko release karta hai. Yeh Naye Jaari Kiye gaye bitcoins aur bitcoins ki mining karte samay satyapit kiye gaye transactions me samil transaction fee dono ke rup me aata hai. Aap Jitni adhik Computational Power ka Yogdaan karte hain aapko utna hi adhik Profit hota hai.
[caption id=”attachment_940"" align=”aligncenter” width=”300""]
Bitcoin Mining Guide : Bitcoin Mining Kya Hai[/caption]
Kuch Mamlon me aapko Bitcoins Mining wala Hardware Kharidne Ki jarurat pad sakti hai. Aaj Ke Samay me aap adhiktar HardwareAmazonpar Kharid Sakte hain. Ya aap Chahein toBitcoin Chartsme bhi iski talaash kar sakte hain.
Bitcoin Mining Start Karne Ke liye, Aapko Bitcoin Mining Harware lena hoga. Starting ke dino me aapke Computer ke CPU ya High Speed wale video Processor Card Se Mining Karna Possible tha. Aaj aisa karna sambhav nahi hai. Custom Bitcoin ASIC CHIP purane Systems ke Power se 100 guna adhik tak aur teji ke saath kaam kar sakte hain.
Issey low Speed ki kisi bhi any Chij se Bitcoin ki mining karne me ussey adhik Electricity Consume hoti hai. To Electricity Bill ki wajah se aapki profit bahut hi kam reh jayegi. Yeh Jaruri hai ki Bitcoin Mining ke liye khaastaur par banaye gaye Best Bitcoin Mining Hardware se hi Mine Kiya jaye. Avalon Jaisi Kai Companies Bitcoin Mining ke liye khas taur par bane Best systems ko provide karwati hain.
Aap Ke liye Ek aur Option hai Bitcoin Mining ke liye Cloud Contracts Ko Purchase Karna. Ye Process to bahut easy hai lekin isme Jokhim badh jaata hai, Kyunki aap Real way me Physical Hardware ko control nai kar rahe hote hain.
Yanha Hum Bitcoin Mining ke Best Cloud Mining Services ki list bata rahe hain. Lekin Hum inke Services lene ke liye aapko aagrah nai karte hain. Kyunki Bitcoin ke cloud mining me pehle bhi kai Ghotale ho chuke hain.
Genesis Mining: Genesis Mining Sabse Badi aur Bitcoin aur Scrypt Cloud mining Provider hai. Genesis Mining 3 Cloud Mining Plans offers karta hai jiske price bahut hi reasonable hain. Saath hi ZCash Mining Contracts bhi available hain.
Hashing 24 Mining: Hasing 24 varsh 2012 se Bitcoin Mining kar rahi hai. Inka Center Georgia aur Iceland me hai. Ye BitFury dwara Manufacture Modern ASIC Chips ka use karte hain Jinki wajah se maximum Performance aur efficiency milti hai.
Hashflare Mining: Hashflare SHA-256 Mining Contracts ko offer karta hai, Aur BTC me automatic Payouts hote rehne ke dauran hi adhik Profitable SHA-256 Coins ko mine kia jaa sakta hai. Coustomers ko kam se kam 10 GH/S purchase karna Chahiye.
Minex Mining: Minex Blockchain Project ka ek naya rup hai. Jisey ek Kifayti Simulation Game Version me present kiya gaya hai. Users Cloudpacks purchase kar sakte hain Jinhe fir Cloud mining farms, lotteries, casinos, real-world markets aadi ke purv Chayanit sets se ek index banane me use kiya jaata hai.
Hahnet Mining: Hashnet ka sanchalan Bitmain Dwara liya jaata hai, jo bitcoins minors ki Antmier Chain ka nirmata hai. Hashnet Me Filhal 600 se adhik Antminer S7 kiraye par uplabdh hain. Aap Hashnet Ki website par sabse latest price aur availability dekh sakte hain. Ish post ke likhe jaane ke samay tak Antiminer S7 ki hashrate ko 1200 Dollar me rent par liya jaa sakta hai.
Minergate Mining: Minergate Bitcoin ke liye Pool, Merged Mining aur Cloud Mining Offer karta hai.
Bitcoin Cloud Mining: Currently Bitcoin Cloud Mining ke sabhi Contract Sell Ho chuke hain.
MineOnCloud: Present time me MineOnCloud ne cloud me rent par dene ke liye Mining Equipment ke lagbhag 35 TH/S ublabdh karaye hain. Rent par uplabdh kuch miners me AntiMiner S4 aur S5 availble hain.
NiceHash: NiceHash ki viseshta yeh hai ki ye Mining Contracts Buyers aur Sellers ka milan karne ke liye ek orderbook ka upyog karta hai. Iske Latest Price ke liye site par visit karein.
Eobot: Yanha aap 10 Dollar jitne low amount se bhi cloud mining suru kar sakte hain. Eobot ka daawa hai ki grahak 14 Mahino me laabh kamana suru kar sakte hain.
Aapko Apna Bitcoin Mining Software Mil jaane ke baad, Aapko Bitcoin mining ke liye ek visesh program Download karna hoga. Bitcoin Mining ke liye prayog kiye jane ke liye kai programs available hain, Lekin Do Sabse Lokpriye Programs hain CGminer aur BFGminer, jo comand Line Program hain.
Yadi aap GUI dwara Pradaan ki jane wali upyog ki saralta pana chahte hain, to aap Easyminer ko aajma sakte hain jo ek click se suru hone wala Window/Linux/Android Program hai.
fully decentralized pool ke liye , hum highly recommend karte hainp2pool. ko.
Yanha jo pool diye gaye hain un pools me aajkal Bitcoin core aur uske baad ke versions me ye currently fully validating blocks hain.(0.10.2 or later recommended due to DoS vulnerabilities):
Aaj Ke Post Ko hum yanhi End karte hain. Next Post me Bitcoin Wallet setup karne ke baare me batayenge. Kynki Bitcoin Wallet me hi aapke coins save hotey hain. To Thoda wait kijiye next post ke liye. Waise aage abhi bahut kuch batana hai aapko Bitcoin aur Bitcoins Mining ke baare me.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",https://miro.medium.com/v2/resize:fit:300/0*pIiNtF62Gu37zOC2.png,"Bitcoincash, Bitcoin Wallet, Hindi, Bitcoin, Bitcoin Mining"
https://medium.com/@imnitishverma/koinex-indian-cryptocurrency-exchange-3dd17411c81f,Koinex Indian Cryptocurrency Exchange,Nitish Verma,2020-04-22T11:28:38.109Z,"India me cryptocurrency ka chalan bahut teji ke saath badh raha hai. Isi kram me Koinex Indian Cryptocurrency Exchange aa gai hai. Kisi Bhi Cryptocurrency ki selling ko takkar uske nikat ke competitors dete hain. Haal me bahut se exchanges ne pop up kiya hai aur bhi sayad cryptocurrency exchanges indian market me aane ko taiyaar hongi. Koinex India me New Platforms me ek hai.
India ke logon ke Koinex ek naya uphaar hai Jo crypto currency ka use karta hai. Kaafi lambe samay aur kadi mehnat ke baad safaltapurvak “Koinex Multi currency Exchange” Banaya gaya jo 25 August 2017 ko bharat me launch kiya gaya tha.
Aaiye Milte hain Koinex Exchange ke Founders se.
Koinex exchange ki sthapna Rahul Raj (IIT Khargpur), Rakesh Yadav (IIT Delhi) aur Aaditya Nayak (BITS Pilani) ne ki hai. Sabse Pehle 2016 me inhone Blockchain Technology ki suruaat ki, uske baad sansthpakon ne 2017 me Cryptocurrency me trading suru ki. Jaisa ki aap sabhi jante hain desh me abhi bhi in chijo ke naa ke brabar vikalp hain, we sbhi Global Exchange (Coinbase, Poloniex, Liqui etc) par business business karne ke liye majbur they. Global Exchange aur Present Indian Exchange (Zebpay, Unocoin, EthexIndia etc) dwara diye jane wale quality ke mamle aur kafi research ke baad, Unhone Indian Currency aur kai cryptocurrency ko support karne waali ek indian Exchange banane ka Nirnay liya. Jo Koinex ke Rup me aapke saamne hai.— Koinex Indian Cryptocurency Exchange
Sabhi teeno sansthapko ne Technology, Product aur business se pahuchne waale kaushal ka shandaar aur asadhaarn mixture pesh kiya hai. Koinex Team ke saath we daawa karte hain ki Koinex ek centralized, Live Open Order Book,Pear to pear exchange hai, Jo Maujuda khiladiyon ke viprit nahi hai.
Koinex ko ek highly secure, scalable, trading friendly aur bahut adhik digital assets exchange pradaan karne ke liye banaya gaya hai. Grade A Security architecture ke saath milakar New aur latest technology, Koinex ke uddesya India ko International Manko ko par laaya jaana hai. Yeh Super Fast, Super Safe, Super Interactive hai, aur crypto traders ke liye visesh rup se taiyaar kia gaya hai. Koinex High frequency ke liye atyadhik competitor traded Business Fee anusuchi pradaan karta hai. Crypto Financial Instruments aur anya cryptocurrency products ko pahle se avdharna aur 1 year ke roadmap me shaamil kiya gaya hai.
CoinFunda ko diye interview me batate huye Company ke Co Founder Rahul Raj ne kaha ki Koinex suruaat me 4 Coins ka samarthan karega: Bitcoin, Ethereum, Litecoin aur Ripple (other altcoin ko baad me joda jayega). Present me, Koinex fiat-to-crypto ka samarthan karta hai, INR/BTC aur INR/ETH etc. (Crypto-to-crypto Koinex launching ke baad 2–4 month ke bhitar live kiya jayega). Halanki wey ish process ka experience karne ki planning kar rahe hain jissey wo users ko striking launch offer de sakein.
Koinex me bhugtaan aap india me Jo banking system hai uske hisaab se kar sakte hain Jaise NEFT, RTGS, IMPS Debit card aadi. Waise khabar ye bhi hai Koinex jaldi hi Bitcoin Cash me kaam shuru karne waala hai. Halanki india me abhi bitcoin exchange ke niyam effectively laagu nahi hain.
Koinex attacks, chori aur privacy leak se users ko safety provide karega. Yeh ek quickly accessible wallets me se ek hai, Jo order, Booking ki jaanch karne aur sthaan ke aadeshon ki jaanch me madad karega, aur ye bahut teji se hoga.
Payment Mode me Card ya bank transaction aur cryptocurrency shaamil honge. Dusri aur transaction fee prakaar ke business ke liye kam hoga, Web se alag Android aur IOS platform par ek app ke rup me availble hoga, Jo number of users ko badhayega.
Yeh user ke wallet me Cryptocurrency aur INR ka prabandhan karega. Users dwaara aavshyak kisi bhi help ke lie On Demand help available hogi.
Koinex Pros and Cons:
Koinex accha hai aur other cryptocurrency ke mukable iske kai advantages hain.
Yeh Fst aur adhik secure hai.
Apne KYC ko pura karne me kam samaya lagata hai aur teji se business karne me saksham banata hai.
Yeh Bitcoin,BHC aur Ethereum sahit 4–5 altcoins ka samrthan karta hai.
Iska Interface bahut hi aasan hai Jisey aasani se use kiya jaa sakta hai.
Other Wallets ki tulna me tulna me iski fee thodi adhik hai.
Heavy traffic hone ki wajah se kabhi kabhi server down ho jaata hai jiske liye wey aksar trading band kar dete hain.
Yadi aap dektop versio use kar rahe hain to har baar account me sign karne ke liye aapke registered e mail par OTP verification aata hai.
Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai
Buy Bitcoin — Unocoin Ki Help Se India Me Bitcoin Buy Karein [Full Guide]
Bitcoin Mining Guide : Bitcoin Mining Kya Hai
Halanki abhi suruaat ki wajah se ye thodi bahut samsya hai aur nisandeh ye ek secure aur accha wallet hai. Jish trah se ish par users grow ho rahe hain nischit hi ye ek Cryptocurrency ki duniya me accha mukaam hasil kar lega. Meri maane to Koinex jaldi hi indian market me acchi pakad bana lega.
- Bitcoin Gold, Golem, IOTA, Omisego and few more standard coins coming soon
Join now and get 50 Rs Free on first trade with this referral code:
Koinex Indian Cryptocurency Exchange ke baare me apne vichaar Hume comments me jarur dein. Hum aage aur bhiBitcoin, Bitcoin Minning aur Cryptocurrencyke baare me information dete rahenge.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",https://miro.medium.com/v2/resize:fit:500/0*NkirQKbfW4fuKris.png,"Cryptocurrency, Koinex, Bitcoin Wallet, Bitcoin, Bitcoin Mining"
https://medium.com/@imnitishverma/ethereum-vs-bitcoin-ethereum-aur-bitcoin-me-kya-difference-hai-b85854bad326,Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai,Nitish Verma,2020-04-22T11:26:36.165Z,"Vitalik Buterin dwara Varsh 2014 ki suruaat me North America me Bitcoin Conference me Ethereum ki ghoshna ke baad Ethereum par bahut adhik logo ka dhyaan gaya. Iski Badhti Popularity ka parinaam ye hai ki Bitcoin Ke saath lagataar iski tulna ho rahi hai. Aaj Ke Post me Ethereum Vs Bitcoin : Ethereum aur Bitcoin Me Kya Difference Hai? Iske Baare batayenge.
2. Ethereum Me Blocks ka samay Bitcoins ki tulna me kam hai. Bitcoins Blocks ko clear karne me 10 minute ka samay lete hain wanhi Ethereum 14 se 15 Seconds me ye kaam karta hai. Yeh Fast transaction ki anumati deta hai. Ethereum aisa Ghost Protocol ka use karke karta hai.
3. Ethereum Ki tulna me Bitcoin ka Economic Model bhi Thoda Different hai. Bitcoin Block Rewards har 4 saal me aate hain, Jabki Ethereum Pratyek Varsh ke antraal par saman raashi jama karta hai.
4. Ethereum me unki Computational Complexity, Bandwidth Use, aur Storage ki Jaruraton ke aadhar par Transaction ka ek alag tarika hai. Bitcoin Transaction ek dusre ke saath saman rup se pratispardha (Compete) karte hain. Isey Gas in Ethereum kaha jaata hai aur Bitcoin me Prati Block limit hota hai, Yeh Block Size ke dwara limit hota hai.
5. Ethereum Ka apna Complete Turing Internal Code hai. Ek Complete Turing Code ka matlab hai enough computing power and enough time.. Janha Kuch Bhi calculated kiya jaa sakta hai. Jabki Bitcoin ke saath aapko ye Flexibility nahi Milti hai.
6. Ethereum Me Bitcoin ko jaari kiye jane ke dauraan Crowd Funding se dhan Muhayaa karaya gaya tha aur suruaat me adhikansh coins Khud hi mine kiye jaa chuke hain. Ethereum ke saath 50% Coins ki mining 5 saal me hogi.
7. Ethereum Centralized Pool Mining ko apne Ghost Protocol ke madhyam se Rewarded stale blocks ko discourage karta hai. Jiska saaf matlab hai aapko Block Prasar ke mamle me pool me hone ka koi fayda nahi hai.
8. Ethereum ASIC ke pryog ke khilaf Mitigate karne wale ETHASH namak ek memory hard hashing Algorithm ka upyog karta hai aur logon dwaara apne GPU ke upyog se decentralized mining ko protsahit karta hai.
Bitcoin Wallets Ke Types
Indian Users Ke Liye Top Bitcoin Wallets
Bitcoin Ki aur adhik Jankari ke Liye Yanha Click Karein.
Nitish Verma is Hindi Blogger, Digital Marketer, Podcaster and Author. Visit My Hindi Blog:https://www.nitishverma.com/https://technicalmitra.com/",,"Bitcoin Wallet, Bitcoin, Bitcoincash, Bitcoin Mining"
https://medium.com/@ryanrezafadillah/tripadvisor-data-review-bdc908c0f9a4,Scrapping Data Using R,Ryan Reza Fadillah,2020-07-17T13:48:24.275Z,"Hi Data Enthusiast !!!
Materi kali ini kita akan menganalisis data review Marina Bay Sands Hotel, Singapore. Jadi kita ingin mengetahui bagaimana sih secara garis besaranya review pengjung terhadap hotel tersebut.
Review hotel bisa kalian dapatkan di websiteTripAdvisordan kalian juga perlu mengunduh SelectorGadget untuk melihat posisi review yang akan diambil.
Pada materi ini package yang kalian butuhkan sebagai berikut,
Lalu copy-kan link halaman yang memuat review hotel yang akan kita scraping dan pindahkan link tersebut kedalam R
nampak hasilnya seperti gambar dibawah,
Selanjutnya sorot salah satu komentar yang ada dengan menggunakan SelectorGadget,
Lalu masukkan script letak review ke dalam R,
nampak hasilnya seperti gambar di bawah,
Selanjutnya, kita akan membersihkan huruf yang tidak diperlukan misal “\n” dan lainnya dan simpan data yang telah dibersihkan dalam csv
Selanjutnya, buka file csv yang telah tersimpan dan lihat jika ada baris yang tidak memuat data maka bisa kita hapuskan. Contohnya dibawah ini pada baris pertama tidak memuat data yang berarti jadi bisa kalian hapus
Selanjutnya, panggil kembali data csv yang telah dibersihkan kedalam R,
Lalu buatlah Corpus dan str untuk dokumen di atas,
Kemudian, cobalah dari kelima dokumen yang ada,
maka akan nampak seperti gambar di bawah,
Setelah itu, buatlah matriks kata- kata dalam dokumen dengan menggunakan script di bawah ini,
Bisa dilihat jika dari 5 dokumen ada 262 kata yang berbeda, maka kita gunakan
maka akan menghasilkan,
Pada gambar di atas bisa kita ketahui jika, pada dokumen ke-1 kata “citi” muncul 2 kali, “hotel” 8 kali, dst. Kemudian kita dapat menampilkan semua kata- kata yang ada,
maka akan menghasilkan,
Selanjutnya run script dibawah ini,
maka akan menghasilkan,
(dokumenDTM,3) artinya kata — kata yang mempunyai frekuensi 3 kali muncul atau lebih, sedangkan (dokumenDTM,4) rtinya kata — kata yang mempunyai frekuensi 4 kali muncul atau lebih.
maka akan menghasilkan,
Selanjutkan, buatlah matriks nya,
Setelah muncul matriksnya, kita akan melihat 6 data terbesar,
maka akan menghasilkan,
Setalh itu, kita akan membuat wordcloud dari kata- kata di atas menggunakan script di bawah ini,
maka akan menghasilkan,
Dari hasil wordcloud diatas, terlihat masih ada kata “the”, “was”, dll yang artinya kurang bagus. Jika kita ingin mengetahui asosiasi kata- kata yang sering muncul, misal kata “good”, “great”, “nice” maka gunakanlah perintah berikut,
dan akan menghasilkan,",https://miro.medium.com/v2/resize:fit:452/1*w5oyrWsl7O9UfXfvDc5ncw.png,"R, Data Science, Hotel Reviews, Sentiment Analysis, Tripadvisor"
https://medium.com/@yogiisari/why-use-selenium-grid-5c2ab3f13a5a,Why use Selenium Grid?,Yogi is ariyanto,2021-06-04T02:55:38.936Z,"Selenium Grid adalah alat yang digunakan untuk menjalankan tes paralel di berbagai mesin dan browser yang berbeda secara bersamaan sehingga proses eksekusi lebih cepat.
Dengan Selenium Grid, satu server bertindak sebagai hub yang merutekan perintah dalam format JSON ke satu atau lebih node yang terdaftar.
Memungkinkan kita untuk menjalankan tes secara paralel pada beberapa mesin, dan untuk mengelola berbagai versi browser dan konfigurasi browser secara terpusat.
Mengapa Selenium Grid?
How to use Selenium Grid version in Mac
*sesuaikan path location selenium server standanlone , hub, node & chrome driver
maka selenium grid sudah jalan di localhttp://localhost:4444/grid/console
Untuk setup automation testing untuk parallel execution menggunakan beda browser pada selenium Remotewebdriver dengan selenium grid. untuk menjalankan paralel test yang di perlu di perhatikan ada di:
Pada codingan di atas, menggunakan classDesiredCapabilitiesuntuk mengatur properti pada Selenium WebDriver. Properti pada Capability digunakan untuk konfigurasi browser seperti BrowserName BrowserVersion, lokasi script yang akan di Execute dan lainnya.
Catatan:script hanya akan berjalan di versi browser yang diinstal pada laptop. Misalkan, jika kamu menjalankan automation di Chrome versi 72 dan Chrome di laptop kamu versi 74, maka scipt automation kamu akan fail karna browser tidak sesuai. itu terjadi juga jika settingan platform tidak sesuai dengan laptop kamu.
Full source code diGitHub:https://github.com/yogiis/autobot-selenium-grid-ui
Test Engineer atFlip.id",https://miro.medium.com/v2/resize:fit:800/1*bwQQ6gsi256xirno2x0u4g.png,"Web Automation Testing, Selenium Webdriver, Selenium Test Automation, Selenium, Selenium Grid"
https://medium.com/@akbarfarraz/yang-maha-kecil-c22c7b4143cd,Yang Maha Kecil,Farraz Akbar,2021-04-26T12:29:28.856Z,"Dua tahun sebelum saya lahir, Carl Sagan pernah menulis sebuah renungan indah yang terilhami oleh sebuah foto kabur yang diambil dari jarak 6.4 milyar kilometer dari tempat kita tinggal. Satu Titik Biru Pucat judulnya. Sesuai namanya, ia memaknai sebuah foto yang menampakkan mungilnya planet bumi di tengah semesta. Bumi hampir tidak terlihat, sekalipun terlihat ia hanya satu titik biru pucat yang samar. Berikut cuplikan renungan Carl Sagan yang berjudulA Pale Blue Dot.
Look again at that dot. That’s here. That’s home. That’s us. On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every ""superstar,"" every ""supreme leader,"" every saint and sinner in the history of our species lived there--on a mote of dust suspended in a sunbeam.
The Earth is a very small stage in a vast cosmic arena. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds.
Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.
The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.
It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we’ve ever known.
Sagan menggarisbawahi keserakahan manusia yang berakibat pertumpahan darah yang mubazir, yang hanya memperebutkansebagiandari satu titik biru pucat yang umurnya barusekedip matadibandingkan semesta. Andai saja kita semua menyadari kesendirian komunitas kita di tengah lautan kosmos yang maha luas ini, mungkin kita akan saling jaga, alih-alih saling jagal.
Mengakui posisi kita sebagai Yang Maha Kecil haruslah didampingi dengan pengakuan adanya Yang Maha Besar. Dalam Islam, pengakuan ini disebuttakbir. Makna kalimat takbir adalah pengingat bahwa tiada hal yang lebih besar dari-Nya. Namun, takbir juga bisa dimaknai untuk menyadarkan kita betapa kecilnya manusia di hadapan-Nya. Maka dari itu saya seringkali heran dengan kelompok orang yang meneriakkan kalimat takbir untuk mengusung egonya, untuk menindas kaum minoritas. Bukankah yang dibesarkan dalam kalimat takbir adalah Tuhannya, mengapa jadi egonya yang tambah besar, jangan-jangan ego adalah Tuhan-Nya(?).
Seorang muslim mengucap takbir minimal 85 kali dalam sehari ketika mendirikan shalat. Kebanyakan dari kita (termasuk saya) 85 takbir tidaklah cukup untuk mengerdilkan diri kita di hadapan semesta dan pencipta-Nya. Jika demikian, tambahlah dengan ibadah sunnah, maka bertambah pula takbir harian kita. Bila perlu bertakbirlah dalam hati setiap saat, tidak mesti ketika shalat. Apapun caranya, takbir kita tidak boleh salah sasaran, jangan sampai ego kita yang dibesarkan takbir, alih-alih Tuhan kita.
Wallahualam bishawab
Writes about science, energy, data center, film, photography, and personal reflections.",https://miro.medium.com/v2/resize:fit:1035/1*C-2qIeBVO6R9DSd7N_yQ-w.jpeg,"Islam, Cosmos"
